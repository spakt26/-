{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarization_methods.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "207px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7ababbb917e8427991b9af81dffa1680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bdb37c7571614b5bb90d90e2bd9947e0",
              "IPY_MODEL_2b2cd25d7bf34c898b4fd3888e52bc67"
            ],
            "layout": "IPY_MODEL_b459594541d64a2e9c5b2f4754b2ee5a"
          }
        },
        "bdb37c7571614b5bb90d90e2bd9947e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_528a2d12f68a49ae87821a550dac15be",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5f84b72ed9146578f58b82b2b9e2074",
            "value": 571
          }
        },
        "2b2cd25d7bf34c898b4fd3888e52bc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ef9e282a0b64c08a7606a8547530648",
            "placeholder": "​",
            "style": "IPY_MODEL_8cd4fc093a7e41fabc37ade69146ac99",
            "value": " 571/571 [00:32&lt;00:00, 17.5B/s]"
          }
        },
        "b459594541d64a2e9c5b2f4754b2ee5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "528a2d12f68a49ae87821a550dac15be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5f84b72ed9146578f58b82b2b9e2074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "8ef9e282a0b64c08a7606a8547530648": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cd4fc093a7e41fabc37ade69146ac99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c02ef3d1c2f44a3b490362ae8c391ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bddfeecbe1e447199e05a0228812f0e",
              "IPY_MODEL_c5ccf3da42d44959b9d8e6327d1e76e5"
            ],
            "layout": "IPY_MODEL_819b5786799e4498b02fd818bbcddf02"
          }
        },
        "3bddfeecbe1e447199e05a0228812f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e3d8659993a4f1ca0cb2e617e8d8ee4",
            "max": 1344997306,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e96e60e741447ac8b80cd970e2bbf49",
            "value": 1344997306
          }
        },
        "c5ccf3da42d44959b9d8e6327d1e76e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cc9a57634164569a9f057489db8d598",
            "placeholder": "​",
            "style": "IPY_MODEL_4cb1889baf36418d977ab79d30e8d7e1",
            "value": " 1.34G/1.34G [00:31&lt;00:00, 42.1MB/s]"
          }
        },
        "819b5786799e4498b02fd818bbcddf02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e3d8659993a4f1ca0cb2e617e8d8ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e96e60e741447ac8b80cd970e2bbf49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "8cc9a57634164569a9f057489db8d598": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb1889baf36418d977ab79d30e8d7e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "683f37d80a1c40a49b52e204a35c7bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ba9e2f760024470a1b756d5119ccfa9",
              "IPY_MODEL_7f9fdf30c287421fb900b6be8956da70"
            ],
            "layout": "IPY_MODEL_18b2e64f536b4c4d9cf05bfc47e50fd2"
          }
        },
        "7ba9e2f760024470a1b756d5119ccfa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0f30aa0ff12487aa69018892e631847",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2375bb2c124143dc8fd2c98d95675bb0",
            "value": 231508
          }
        },
        "7f9fdf30c287421fb900b6be8956da70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b212133eb4e442b6b78ad0c449ba880a",
            "placeholder": "​",
            "style": "IPY_MODEL_c6310450d4f54b75a91c8eddfe0f9401",
            "value": " 232k/232k [00:00&lt;00:00, 354kB/s]"
          }
        },
        "18b2e64f536b4c4d9cf05bfc47e50fd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0f30aa0ff12487aa69018892e631847": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2375bb2c124143dc8fd2c98d95675bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "b212133eb4e442b6b78ad0c449ba880a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6310450d4f54b75a91c8eddfe0f9401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1cc9ed33536458691eea4033079ddf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_abad6dbbd46141c39b2343990274fe41",
              "IPY_MODEL_660c11570197475bbedcc6526b540a22"
            ],
            "layout": "IPY_MODEL_e6fa4e39e3b941fca09b4484af78adc3"
          }
        },
        "abad6dbbd46141c39b2343990274fe41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_130c71fa4d5845139dc7a357a6479f4f",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff0cea90dc074ef5a6a3c9f387e4d47c",
            "value": 28
          }
        },
        "660c11570197475bbedcc6526b540a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17ec69dbcb8a409a85641e28517b28fa",
            "placeholder": "​",
            "style": "IPY_MODEL_cb143baeecb24303ab29162eb67b511d",
            "value": " 28.0/28.0 [00:00&lt;00:00, 116B/s]"
          }
        },
        "e6fa4e39e3b941fca09b4484af78adc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "130c71fa4d5845139dc7a357a6479f4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff0cea90dc074ef5a6a3c9f387e4d47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "17ec69dbcb8a409a85641e28517b28fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb143baeecb24303ab29162eb67b511d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71bbc4ce1b674615b57103107ffd6f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_915641fb8e70495baf3bd55f1db3d16c",
              "IPY_MODEL_6304db960a9848d3b8eb9fb3f805e78c"
            ],
            "layout": "IPY_MODEL_c93bd3e43a2549afb42028de34d83337"
          }
        },
        "915641fb8e70495baf3bd55f1db3d16c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f234129661a149d2806bb46365834b6c",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a5cee1b46a54cacb389c2197fa6cd35",
            "value": 466062
          }
        },
        "6304db960a9848d3b8eb9fb3f805e78c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bde55504e09459587c6ebe1b94d5156",
            "placeholder": "​",
            "style": "IPY_MODEL_8a222052ed73406a8758540677f19f86",
            "value": " 466k/466k [00:00&lt;00:00, 2.80MB/s]"
          }
        },
        "c93bd3e43a2549afb42028de34d83337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f234129661a149d2806bb46365834b6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a5cee1b46a54cacb389c2197fa6cd35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "0bde55504e09459587c6ebe1b94d5156": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a222052ed73406a8758540677f19f86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "570b7eb2ff844d50889c8ebf986e29c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9bc031fb5e104ba6910ac0778ab4f559",
              "IPY_MODEL_36b8b2dc6d534848bec66aab34fcfe97"
            ],
            "layout": "IPY_MODEL_9a8b3857c8a14c64a44d0f306d3ff76e"
          }
        },
        "9bc031fb5e104ba6910ac0778ab4f559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b09d55a8d5f4831b84b6744d5ce70be",
            "max": 718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f0de94287cf4de183d4e7eb368f9f90",
            "value": 718
          }
        },
        "36b8b2dc6d534848bec66aab34fcfe97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edfc6581d197425ab5ab4c7028a72514",
            "placeholder": "​",
            "style": "IPY_MODEL_2b78a7189e514ed791ee79ae8a3c403d",
            "value": " 718/718 [00:00&lt;00:00, 2.88kB/s]"
          }
        },
        "9a8b3857c8a14c64a44d0f306d3ff76e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b09d55a8d5f4831b84b6744d5ce70be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f0de94287cf4de183d4e7eb368f9f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "edfc6581d197425ab5ab4c7028a72514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b78a7189e514ed791ee79ae8a3c403d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45669e37fcc94111af15efbd5ef3cd78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbdf8970481a43c4ac32934502f821b1",
              "IPY_MODEL_43d11d5661644332b5c618fca9584e95"
            ],
            "layout": "IPY_MODEL_0ba8687590f249b9b7e355e3ed2ff534"
          }
        },
        "dbdf8970481a43c4ac32934502f821b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8309409102d44bf8b75d2ae600ddd27d",
            "max": 1520013706,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d951bc69b34436ea24f4da805b62082",
            "value": 1520013706
          }
        },
        "43d11d5661644332b5c618fca9584e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b1d856e3f0a4572a09e3077436f130c",
            "placeholder": "​",
            "style": "IPY_MODEL_6f05655b617d4aa385640aafdcb309ea",
            "value": " 1.52G/1.52G [00:37&lt;00:00, 40.9MB/s]"
          }
        },
        "0ba8687590f249b9b7e355e3ed2ff534": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8309409102d44bf8b75d2ae600ddd27d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d951bc69b34436ea24f4da805b62082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "9b1d856e3f0a4572a09e3077436f130c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f05655b617d4aa385640aafdcb309ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a314ffae4334c6d81beebbeaca9c1d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79d7397c69914203adae864bce8f0a24",
              "IPY_MODEL_391b1a112a8f414985099727332d27de"
            ],
            "layout": "IPY_MODEL_5e77d3ed98a04a1d91c12d7e3949d2c4"
          }
        },
        "79d7397c69914203adae864bce8f0a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46a76cd135974dee834531a7e13a3ca2",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5f3228a279b45c68b1f2383e2103c9f",
            "value": 1042301
          }
        },
        "391b1a112a8f414985099727332d27de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b054df56acec413cbc1687979f47a29a",
            "placeholder": "​",
            "style": "IPY_MODEL_5b0ad052d02d4257befdd74d0cd36fbc",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 1.23MB/s]"
          }
        },
        "5e77d3ed98a04a1d91c12d7e3949d2c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46a76cd135974dee834531a7e13a3ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f3228a279b45c68b1f2383e2103c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "b054df56acec413cbc1687979f47a29a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b0ad052d02d4257befdd74d0cd36fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2eee9f698271469f83f18602b70fc17c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_808cf20c6a0f4430a666d1093a76376d",
              "IPY_MODEL_adb2e0b0e81243a39a79bda182935bca"
            ],
            "layout": "IPY_MODEL_4c64746478984036b10a2d825ce03220"
          }
        },
        "808cf20c6a0f4430a666d1093a76376d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4317ec5dcf64865b4227d5bab52f9fb",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4655a13f3d194da4aaa5dfedc387fa9f",
            "value": 456318
          }
        },
        "adb2e0b0e81243a39a79bda182935bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6064d0d2d8a4465b2e04f1d1c865dd4",
            "placeholder": "​",
            "style": "IPY_MODEL_7c3cbfac64c54a19aca7861bf4f95653",
            "value": " 456k/456k [00:00&lt;00:00, 860kB/s]"
          }
        },
        "4c64746478984036b10a2d825ce03220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4317ec5dcf64865b4227d5bab52f9fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4655a13f3d194da4aaa5dfedc387fa9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "b6064d0d2d8a4465b2e04f1d1c865dd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c3cbfac64c54a19aca7861bf4f95653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98ac83e820c044118e0be4ae56abd88a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea5332e277934b8cbfffdba493a2ca2a",
              "IPY_MODEL_83927b5d0add4de183e2eb38cb9f0d51"
            ],
            "layout": "IPY_MODEL_76bfe9baa49a47b1b6e04a5bfa97c5f5"
          }
        },
        "ea5332e277934b8cbfffdba493a2ca2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9310ff12804418d9b6e013ecf10b951",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6efb7444ef34e62b68dfed308d1a1f8",
            "value": 1355256
          }
        },
        "83927b5d0add4de183e2eb38cb9f0d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4737848bf4124c82b76405ef7a7bfda5",
            "placeholder": "​",
            "style": "IPY_MODEL_bb8813b9cf4a4056b90bb8871e6c41c2",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 5.74MB/s]"
          }
        },
        "76bfe9baa49a47b1b6e04a5bfa97c5f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9310ff12804418d9b6e013ecf10b951": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6efb7444ef34e62b68dfed308d1a1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "4737848bf4124c82b76405ef7a7bfda5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb8813b9cf4a4056b90bb8871e6c41c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d45559ceb3e74e49b9442c1eb01797ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52363879d9794b8e9ba9b95a6b3468b4",
              "IPY_MODEL_ad8f2b32b7fa4a1aa8c9bce4dea6c134"
            ],
            "layout": "IPY_MODEL_340cbc5e876e440c8a528162c5cbc5cf"
          }
        },
        "52363879d9794b8e9ba9b95a6b3468b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eabba7b047748268582f5651311e34f",
            "max": 760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9c7ec4003cd4956be6658e6c56fb0b2",
            "value": 760
          }
        },
        "ad8f2b32b7fa4a1aa8c9bce4dea6c134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28a3520817c54b92a733ec991a5042ca",
            "placeholder": "​",
            "style": "IPY_MODEL_a8a38317f5124b38a7c33fef115a6aac",
            "value": " 760/760 [00:11&lt;00:00, 66.2B/s]"
          }
        },
        "340cbc5e876e440c8a528162c5cbc5cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eabba7b047748268582f5651311e34f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9c7ec4003cd4956be6658e6c56fb0b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "28a3520817c54b92a733ec991a5042ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8a38317f5124b38a7c33fef115a6aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70b0a3ba98c54b8ab10d3aab0a9f16e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbdf02a8aab04bf990bebc19b5cfa6f1",
              "IPY_MODEL_9dc676a99e024d6ca3a6f77992c412b9"
            ],
            "layout": "IPY_MODEL_27df0bcbd6014447b02de5ea2b04786a"
          }
        },
        "dbdf02a8aab04bf990bebc19b5cfa6f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8d4307ab0744acaaca28e830e6fd161",
            "max": 467042463,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94e7bddd43fa4257aa19e43a15039fa6",
            "value": 467042463
          }
        },
        "9dc676a99e024d6ca3a6f77992c412b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d029b45f5b24f75b1555687fe5cdb50",
            "placeholder": "​",
            "style": "IPY_MODEL_2eb5696871d04ae28b3332d08dc9b2fa",
            "value": " 467M/467M [00:11&lt;00:00, 41.5MB/s]"
          }
        },
        "27df0bcbd6014447b02de5ea2b04786a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8d4307ab0744acaaca28e830e6fd161": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94e7bddd43fa4257aa19e43a15039fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "2d029b45f5b24f75b1555687fe5cdb50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eb5696871d04ae28b3332d08dc9b2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ca4aa332f6d400f8d080337aad1315d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e32f7a7aa0f84f40b169c7997ee04938",
              "IPY_MODEL_7320043017a346728a426312d4d7e072"
            ],
            "layout": "IPY_MODEL_ee0bcbd64e40479e86d6ef9e1bfd6d7b"
          }
        },
        "e32f7a7aa0f84f40b169c7997ee04938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd52456be2fc4c8c89745a21ea610162",
            "max": 798011,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7825592757f4962aecb98759405bf3e",
            "value": 798011
          }
        },
        "7320043017a346728a426312d4d7e072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_700ec00122f14dc09b27894191b07415",
            "placeholder": "​",
            "style": "IPY_MODEL_f7072b3cd7a247e381c605edbe93b3c5",
            "value": " 798k/798k [00:09&lt;00:00, 80.8kB/s]"
          }
        },
        "ee0bcbd64e40479e86d6ef9e1bfd6d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd52456be2fc4c8c89745a21ea610162": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7825592757f4962aecb98759405bf3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "700ec00122f14dc09b27894191b07415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7072b3cd7a247e381c605edbe93b3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97eeb9791d7a407a9cd19f7a79ad5047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2bce7d1bc7b41beb7de980c6d74e173",
              "IPY_MODEL_a074ffc49417443db38c320b76adc347"
            ],
            "layout": "IPY_MODEL_de6e46b42ac34111a71739559eb80816"
          }
        },
        "c2bce7d1bc7b41beb7de980c6d74e173": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdb3fdf6b8a946bda53beb2888dda625",
            "max": 1382015,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72b294e6db804e39b7209be50d8481c2",
            "value": 1382015
          }
        },
        "a074ffc49417443db38c320b76adc347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f0caf30d46341c6b2fda47901ed9966",
            "placeholder": "​",
            "style": "IPY_MODEL_3c11856d85524e7dbb015ab8ca089873",
            "value": " 1.38M/1.38M [00:09&lt;00:00, 148kB/s]"
          }
        },
        "de6e46b42ac34111a71739559eb80816": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdb3fdf6b8a946bda53beb2888dda625": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72b294e6db804e39b7209be50d8481c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "2f0caf30d46341c6b2fda47901ed9966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c11856d85524e7dbb015ab8ca089873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZtP2lzCanU_"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Text-summarization\" data-toc-modified-id=\"Text-summarization-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Text summarization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Введение\" data-toc-modified-id=\"Введение-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Введение</a></span></li><li><span><a href=\"#Анализ-всевозможных-алгоритмов-саммаризации\" data-toc-modified-id=\"Анализ-всевозможных-алгоритмов-саммаризации-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Анализ всевозможных алгоритмов саммаризации</a></span></li><li><span><a href=\"#Описание-тестового-набора-данных\" data-toc-modified-id=\"Описание-тестового-набора-данных-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Описание тестового набора данных</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Preprocessing</a></span></li><li><span><a href=\"#Метрики\" data-toc-modified-id=\"Метрики-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Метрики</a></span></li><li><span><a href=\"#Extractive-methods\" data-toc-modified-id=\"Extractive-methods-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Extractive methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word-frequency-method\" data-toc-modified-id=\"Word-frequency-method-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Word frequency method</a></span></li><li><span><a href=\"#TextRank\" data-toc-modified-id=\"TextRank-1.6.2\"><span class=\"toc-item-num\">1.6.2&nbsp;&nbsp;</span>TextRank</a></span></li><li><span><a href=\"#ClusterRank\" data-toc-modified-id=\"ClusterRank-1.6.3\"><span class=\"toc-item-num\">1.6.3&nbsp;&nbsp;</span>ClusterRank</a></span></li><li><span><a href=\"#LSA\" data-toc-modified-id=\"LSA-1.6.4\"><span class=\"toc-item-num\">1.6.4&nbsp;&nbsp;</span>LSA</a></span></li><li><span><a href=\"#KLsum\" data-toc-modified-id=\"KLsum-1.6.5\"><span class=\"toc-item-num\">1.6.5&nbsp;&nbsp;</span>KLsum</a></span></li><li><span><a href=\"#Mead\" data-toc-modified-id=\"Mead-1.6.6\"><span class=\"toc-item-num\">1.6.6&nbsp;&nbsp;</span>Mead</a></span></li></ul></li><li><span><a href=\"#Abstractive-methods\" data-toc-modified-id=\"Abstractive-methods-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Abstractive methods</a></span><ul class=\"toc-item\"><li><span><a href=\"#Bidirectional-Encoder-Representations-from-Transformers-(BERT)---2018\" data-toc-modified-id=\"Bidirectional-Encoder-Representations-from-Transformers-(BERT)---2018-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Bidirectional Encoder Representations from Transformers (BERT) - 2018</a></span></li><li><span><a href=\"#GPT2\" data-toc-modified-id=\"GPT2-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>GPT2</a></span></li><li><span><a href=\"#XLNet\" data-toc-modified-id=\"XLNet-1.7.3\"><span class=\"toc-item-num\">1.7.3&nbsp;&nbsp;</span>XLNet</a></span></li><li><span><a href=\"#BART-Bidirectional-and-Auto-Regressive-Transformers-(bidirectional-BERT-encoder-+-GPT-decoder)\" data-toc-modified-id=\"BART-Bidirectional-and-Auto-Regressive-Transformers-(bidirectional-BERT-encoder-+-GPT-decoder)-1.7.4\"><span class=\"toc-item-num\">1.7.4&nbsp;&nbsp;</span>BART Bidirectional and Auto-Regressive Transformers (bidirectional BERT-encoder + GPT-decoder)</a></span></li></ul></li></ul></li><li><span><a href=\"#Speech-Recognition\" data-toc-modified-id=\"Speech-Recognition-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Speech Recognition</a></span><ul class=\"toc-item\"><li><span><a href=\"#speech_recognition-модуль\" data-toc-modified-id=\"speech_recognition-модуль-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>speech_recognition модуль</a></span></li><li><span><a href=\"#Google-speech-to-text\" data-toc-modified-id=\"Google-speech-to-text-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Google speech to text</a></span></li><li><span><a href=\"#IBM-Watson\" data-toc-modified-id=\"IBM-Watson-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>IBM Watson</a></span></li><li><span><a href=\"#Метрики-для-распознавания-речи\" data-toc-modified-id=\"Метрики-для-распознавания-речи-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Метрики для распознавания речи</a></span></li><li><span><a href=\"#Список-литературы\" data-toc-modified-id=\"Список-литературы-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Список литературы</a></span></li><li><span><a href=\"#Идеи-для-чего-то-нового\" data-toc-modified-id=\"Идеи-для-чего-то-нового-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Идеи для чего-то нового</a></span></li></ul></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sONpqynRanVD"
      },
      "source": [
        "# Text summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ_aIrB6anVF"
      },
      "source": [
        "## Введение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLVMTKlTanVF"
      },
      "source": [
        "*+* общая формулировка темы;\n",
        "\n",
        "*+* актуальность выбранной темы, её теоретическое и/или практическое значение;\n",
        "\n",
        "*+* степень научной проработанности темы исследования;\n",
        "\n",
        "*+* цель и задачи исследования;\n",
        "\n",
        "*+* объяснение того, как и в каком порядке, автор намеревается решать поставленные задачи;\n",
        "\n",
        "*+* анонс структуры работы (названия глав работы и их краткая характеристика);\n",
        "\n",
        "*+* характеристика основных источников информации.\n",
        "\n",
        "Введение должно быть кратким (1-3 страницы) и четким. Из введения должно быть понятно, чему посвящена работа (цель работы), какие задачи и с помощью каких методов в ней решаются, какие результаты должны быть получены в ходе выполнения курсовой работы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB_fXwWXanVG"
      },
      "source": [
        "**Исследование саммаризации текстов в применении к реферированию переговоров**\n",
        "\n",
        "**Общая формулировка темы**:  \n",
        "Задача саммаризации в применении к реферированию деловых встреч заключается в выделении конкретного подмножества  информации, которое отражает основную суть исходного документа, в нашем случае аудиозаписи. \n",
        "\n",
        "**Актуальность выбранной темы:**   \n",
        "Устойчивые тенденции последнего десятилетия показывают высокую интегрированность NLP в различных сферах деятельности. Список задач, решаемых NLP, постоянно расширяется, на данный момент он включает в себя: анализ текста, генерирование текста, машинный перевод, определение частей речи слов, категоризация текста, автоматическое создание субтитров и другое.  В крупнейших компаниях, таких, как Google, Microsoft, Amazon, IBM NLP-подразделения включают в себя все больше  и больше сотрудников. С каждым годом появляются все новые модификации алгоритмов работы с естественным языком, например, GPT-3 и все больше они внедряются в повседневную жизнь, например, активно начали использоваться голосовые помощники. \n",
        "Задача саммаризации, также называемая задачей реферирования, становится наиболее популярной в настоящее время особенно с ростом количества информации. Со временем анализ текстовой информации вручную стал рутинной работой, которую возможно автоматизировать. Каждый день мы работаем с текстовыми данными, оставляя отзыв в различных организациях, описывая свои проблемы или отправляя запрос в Интернет. Но чем больше данных в пространстве Интернета, тем меньше времени на ознакомление с ними, все сложнее найти релевантную информацию за оптимальное количество времени.  Таким образом, появляется необходимость в отбрасывании несущественной и в выделении уникальной информации из прочитанного, увиденного или услышанного. До этого момента задача саммаризации применялась в основном в новостном канале или для категоризации отзывов или статей на сайтах с использованием англоязычных наборов данных, в данной работе мы попробуем применить модели к англоязычным наборам данных для достижения бизнес-целей. \n",
        "\n",
        "**Теоретическое значение**:  \n",
        "В данной работе комбинируются алгоритмы двух различных направлений: распознавания речи и естественного языка, тем самым раскрывается все больший потенциал развития NLP в других сферах деятельности.  Результаты послужат толчком к развитию обработки естественного языка при помощи англоязычных наборов данных и откроется новый взгляд на вопрос о том, какие метрики и алгоритмы наиболее подходящие для данных о деловых встречах. \n",
        "\n",
        "**Практическое значение**:   \n",
        "За крупными компаниями прослеживаются тенденции в горизонтальном расширении своих продуктов в различных сферах. С ростом количества направлений, многим отделам все сложнее интегрироваться в работу других отделов и чувствовать себя единой системой. За день в компаниях проводится достаточно большое количество встреч, генерируются все больше новых предложений и идей, в результате чего достаточно трудозатратно успевать за полетом мысли руководителей разных отделений. К тому же, у сотрудников недостаточно времени, чтобы присутствовать на всех мероприятиях, организованных компанией, поэтому в данной сфере практическое применение находит саммаризация огромного количества информации для общей осведомленности сотрудников об актуальных новостях и идеях. \n",
        "\n",
        "**Степень научной проработанности темы исследования**:  \n",
        "Саммаризация в NLP является достаточно новым, постоянно развивающемся явлением.  Только в  1958 году появились алгоритмы, извлекающие самое важное из текстовых данных, такие как Luhn. Они были основаны на частотном подходе, который не учитывал ни контекст предложений, ни  связь предложения с другими. Первая метрика,  способная оценить качество данных моделей была придумана советским математиком в 1965 году, она называлась “Расстояние Левенштейна”, но она рассчитывалась по-символьно: сколько замен в символах нужно произвести, чтобы одна последовательность была максимально схожа с другой. Наиболее релевантная метрика BLEU (BiLingual Evaluation Understudy), учитывающая не символы последовательности, а различных комбинации слов (n-граммы) была придумана в 2002 году одной из команд  IBM. Основные открытия в языковых моделях была сделаны в XXI веке: Google разработала алгоритм BERT в 2019 году, основная идея которого заключается в способности анализировать запрос, как цельное предложение, основываясь на контексте; GPT-3 от OpenAI - самая продвинутая языковая модель в мире, умеющая решать любые задачи, сформулированные на английском языке, разработана в 2020 году. \n",
        "Первая модель распознавания речи была нацелена на числа, не слова и была разработана в 1952 году в лаборатории Bell. Спустя 10 лет появяилась модель \"Shoebox\", способная распознавать 16 англоязычных слов. С 2011 года и по сей день различные компании разрабатывают голосовых помощников Siri, Alexa, Cortana, способных распознавать человечекую речь в real-time, но их точность составляет при идеальных условиях (без шума, разной дикции, разных тонов, эмоциональных окрасок) 95%. Таким образом, область языковых моделей является постоянно развивающейся, но симбиоз распознавания речи и алгоритмов суммаризации является актуальной темой среди современных алгоритмов. \n",
        "\n",
        "**Цель исследования:**  \n",
        "- Разработать модели, применимые для англоязычного набора данных  \n",
        "- Выявить связь между распознаванием речи и реферированием текста  \n",
        "- Подобрать метрики, которые наиболее точно оценивают эффективность применяемых моделей   \n",
        "- Выявить преимущества и недостатки рассмотренных моделей  \n",
        "\n",
        "**Поставленные задачи в порядке их выполнения:**  \n",
        "- Исследовать данную сферу деятельности на степень научной проработанности  \n",
        "- Определиться с тестовым набором данных  \n",
        "- Изучить классификацию алгоритмов саммаризации  \n",
        "- Изучить способы оценки качества алгоритмов реферирования    \n",
        "- Реализовать работу алгоритмов и оценить качество их работы    \n",
        "- Провести сравнительный анализ, используя различные метрики    \n",
        "- Изучить алгоритмы распознавания речи  \n",
        "- Подумать над модификациями алгоритмов саммаризации   \n",
        "- Скомбинировать алгоритмы распознавания речи и саммаризации   \n",
        "\n",
        "Для начала нужно аккуратно подойти к выбору данных, с которыми придется работать. Данные должны иметь максимальную тематическую схожесть с рассматриваемой областью. Имея данные с саммари для каждого из текстовых документов можно приступать к изучению классификации алгоритомов саммаризации. Проведя анализ преимуществ и недостатков алгоритмов, можно перейти к способам оценивания качества, после чего уже применить алгоритмы на данных и провести сравнительный анализ, сопоставляя различные метрики и алгоритмы. На данном этапе можно приступать к исследованию алгоритмов распознования речи в применении к задаче саммаризации или независимо от нее, если таковых источников не имеется. Аналогично провести сравнительный анализ, анализируя преимущества и недостатки различных библиотек для распознавания речи. Перед тем как комбинировать алгоритмы распознавания речи и реферирования текстов, нужно снова определиться с данными, с которыми мы будем работать. В этот раз в наличии должы быть аудиофайлы и их краткое содержание. Определившись с датасетом, снова смотрим на метрики и по ним выбираем итоговую модель, которая показывает наилучшие результаты по сравнению с другими. \n",
        "\n",
        "**Обзор литературы (характеристика основных источников информации):** \n",
        "\n",
        "Первая статья на тему частотного метода саммаризации Luhn была опубликована в 1958 году. С 1960-х годов было разработано огромное количество метододов, которые в последтсвии уже стали классифицировать. \n",
        "Можем выделить основных два направления входных данных: [2]\n",
        "1. Реферирование одиночных документов (на вход подается один текстовый документ)   \n",
        "2. Реферирование нескольких документов (на вход подается несколько документов, например, связанные одной тематикой и нужно извлечь основную информацию по данной тематике) \n",
        "\n",
        "А также можно было выделить три способа саммаризации: \n",
        "1. Extractive summarization. [6]  \n",
        "Данный алгоритм можно сравнить с маркером для выявления основной информации. Исследуем текст, находим  значимые по весу предложения с помощью различных алгоритмов и извлекаем наиболее весомые предложения. У  такого подхода есть недостатки, основной из них - отсутствие баланса и плавности в переходе от одного предложения к другому, но основное преимущество заключается в простоте использования данного подхода, в процессе анализа текста не приходится задумываться о грамматике, семантической связи между словами и предложениями.  \n",
        "2. Abstractive summarization [7]  \n",
        "Данный подход уже больше похож на то, как работает интеллект людей при виде огромного набора текстовых данных : то есть считываем текст, перефразируем его, извлекая основную суть, с помощью слов, которых нет в основном документе. \n",
        "3. Hybrid  (смесь двух вышеупомянутых подходов) \n",
        "\n",
        "Классификация методов саммаризации: \n",
        "- Вероятностные и частотные подходы [11]  [9] [1]  \n",
        "Вероятностные подходы используют статистические характеристики текста для определение веса предложения или строят распределение саммари такое, что оно схоже с распределением исходного текста, но содержит минимальное количество слов.  \n",
        "- Подходы, основанные на графах [5]  \n",
        "В качестве вершин графа используются предложения, а связи между преддожения - взвешенные ребра. С помощью различных алгоритмов каждой вершине присваиваются определнные веса, а затем отбираются предложения с наибольшими весами.  \n",
        "- Нейронные сети [8]  \n",
        "Нейронные сети для построения реферирования текста основываются на структуре encoder-decoder совместно с LSTM.  \n",
        "- Подходы, использующие машинное обучение  \n",
        "Для выявления наиболее оптимального диверсифицированного содеражния исходного документа применяют алгоритмы кластеризации, которые в качестве саммари берут все центроиды, обнаруженные алгоритмом.  \n",
        "- Тематическое моделирование [12].  \n",
        "Тематическое моделирование, например, LDA или LSA, заключается в выделении основных топиков текста, а затем вокруг для каждого топика выделяется 1 - 2 предложения, которые впоследтсвии войдут в саммари.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huUULOfoanVP"
      },
      "source": [
        "## Анализ всевозможных алгоритмов саммаризации  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiELQj0DanVR"
      },
      "source": [
        "Итак, начнем с самого начала истории саммаризации. \n",
        "1. ***Positional method in 1958 by P.Baxendale***  [9]   \n",
        "В 85% случаев первое предложение было смысловым, а в 7% случаев последнее предложение было смысловым, то есть определяющее основную суть текста, поэтому решили брать первое и последнее предложение текстового документа в итоговый реферат.  \n",
        "2. ***Luhn's method H.P.Luhn in 1958***  [1]  \n",
        "Анализировал, как часто слова появляются в документе. Убираем самые частые слова и редкие слова. Затем делаем дата-клининг посредством стемминга и извлечением стоп-слов. Дальше уже считаем частоту для каждого предложения. Например, у нас есть предложение из 10 слов, там 4 основных слова, как мы определяем, что предложение главное? Мы берем 4^2 и делим на колчисетво слов между первым важным и последним важным, допустим у нас их будет 6, поэтому считаем, что вес нашего предложения: 16/6  Потом в реферат мы берем предложения с наибольшим весом, количество таких предложений задается в качестве параметра. \n",
        "3. ***H.P.EDMUNSON 1968***  [3]  \n",
        "Edmunson также обращал внимание на позицию предложения (P) в тексте и на частоту слов (F), как в предыдущих двух способах, но он добавил три типа ключевых слов (C): bonus words - показывают важность предложения, stigma words - негативно влияют на важность предложения (алгоритм считает их важными, но самом деле они не важные), null words (нейтральные для важности). Смотрел на структуру документа (S), находится ли это предложение в названии, под названием, в главном параграфе и тд. А затем в качестве оценки для каждого предложения он предложил использовать линейную комбинацию: a1 * P + a2 * F + a3 * C + a4 * S   \n",
        "4. ***FRUMP (Fast reading understanding and memorable program) 1979 introduced by G.deJong***  [4]  \n",
        "Совсем другой подход к данной проблеме, G.deJong хотел применить саммаризацию к новостям и создал 50 sketchy scripts, которые описывают типичные ситуация в новостях, а потом когда мы получаем новый текст для реферирования, то мы заполняем пропуски в этих скриптах наиболее подходящими словами из текста, но это оказался нерабочий подход, 50 сценариев было недостаточно для всех мировых проблем.\n",
        "5. ***Classification Kupiec 1995***  \n",
        "Суть подхода заключалась в классификации. Kupiec вручную выбирал документы и вручную составлял краткие описания по ним для своей тренировочной выборки. Для задачи он использовал наивный байесовский классификатор. Вероятность, что предложение s принадлежит краткому описанию, при том что есть какой-то набор признаков F1, F2, F3... это то же самое, что и вероятность, что эти признаки есть у предложения s в его описании * вероятность, что это предложение в описании / вероятность, что эти признаки появляются вместе.  \n",
        "$ P(s \\in S|F_1, F_2...F_k) = \\frac{P(F_1, F_2...F_k|s \\in S) * P(s \\in S)}{P(F_1, F_2...F_k)}$  \n",
        "Предполагая статистическую независимость признаков, можем записать так:  \n",
        "$ P(s \\in S|F_1, F_2...F_k) = \\frac{\\prod\\limits_{j = 1}^k P(F_j|s \\in S) * P(s \\in S)}{\\prod\\limits_{j = 1}^k P(F_j)}$   \n",
        "Precision этой модели была достаточно высокой: 84%. Но в 2002 году Miles Osborne доказал, что метод энтропии рыботает эффективнее, потому что признаки скорее всего зависимы друг от друга. \n",
        "6. ***Miles Osborne 2002 Maximum entropy for setntence extraction***  \n",
        "Аналогичный подход, что и в прошлом пункте, но вместе наивного байесовского классификатора, мы используем формулу максимальной энтропии.  \n",
        "7. ***Частотный подход (TDIDF)***  \n",
        "TF (term frequency — частота слова) — отношение числа вхождений некоторого слова к общему числу слов документа. Таким образом, оценивается важность слова ${t_i}$ в пределах отдельного документа.  \n",
        "$tf(t, d) =\\frac{n_t}{\\sum_{i=1} n_k} $,  \n",
        "где ${n_t}$ есть число вхождений слова в t документ, а в знаменателе — общее число слов в данном документе.  \n",
        "idf(t, D) = $\\log \\frac{|D|}{|{(d_i \\in D|t \\in d_i)}|}$  \n",
        "tf - idf = tf * idf  \n",
        "Выбор основания логарифма в формуле не имеет значения, поскольку изменение основания приводит к изменению веса каждого слова на постоянный множитель, что не влияет на соотношение весов.  \n",
        "|D| - число документов в коллекции;  \n",
        "${|(d_i \\in D|t \\in d_i)|}$ - число документов из коллекции D, в которых встречается t когда ${n_i}$ !=0  \n",
        "Применительно к нашему случаю, скорее всего документ - предложение. То есть, для понимания приведем пример: \n",
        "Если наше предложение содержит 25 слов, и слово «заяц» встречается в нём 3 раза, то частота слова (TF) для слова «заяц» в документе будет 0,12 (3/25). Вычислим IDF как десятичный логарифм отношения количества всех документов к количеству предложений содержащих слово «заяц». Таким образом, если «заяц» содержится в 1000 предложениях из 10 000 000 предложений, то IDF будет равной: log(10 000 000/1000) = 4. Для расчета окончательного значения веса слова необходимо TF умножить на IDF. В данном примере, TF-IDF вес для слова «заяц» в выбранном документе будет равен: 0,12 × 4 = 0,48.  А дальше мы считаем score для каждого предложения и выбираем предложения с высоким score.  \n",
        "8. ***Maximum marginal relevance 1998, Carbonell and Goldstein***  \n",
        "MMR = $argmax_{s_i \\in \\mathcal{R-S}} (\\lambda * Sim_1(s_i, Q) - (1-\\lambda) * \\max\\limits_{s_j \\in S}Sim_2(s_i, s_j))$  \n",
        "S - уже извлеченные предложения из текста  \n",
        "Q - user query (описание категории документа) ?не очень понятно, где его брать в случае single-document classification   \n",
        "Sim - similarity metrics  \n",
        "${\\lambda}$ - некий порог, который мы сами устанавливаем  \n",
        "Идея заключается в том, что мы выбираем предложение, которое не похоже на все остальные в нашем итоговом саммари.   \n",
        "9. ***MEAD (centric based method) Radev 2000***.  \n",
        "Для начала определяем наши предложения, как точки в векторном пространстве. Первое, что мы хотим это применить к ним кластеризацию, то есть разделить их на топики. Затем мы ищем центроиды кластеров. И выбираем предложения, которые наиболее близки к центроидам, то есть фактически применяем k-means. Но тут также может возникнуть, что у нас появятся предложения, одинаковые по смыслу, но близкие к центроиду, поэтому нужно будет затем посмотреть на similarity matrix (их виды указаны ниже) и выбрать предложения, у которых коэффицент схожести низкий.  \n",
        "Имеем предложения в векторном пространстве,делаем клестеризацию по топикам. \n",
        "10.  ***LSA (Latent Semantic Analysis)*** [10]  \n",
        "В основе лежит построение матрицы (слова x предложения), а затем применение сингулярного разложения для этой матрицы. Так, для каждого слова можно получить необходимый вес, а в суммаризацию включать те предложения, чья суммарная \"важность\" слов наибольшая. \n",
        "Итак, опишем подробнее алгоритм.  \n",
        "Составляем матрицу размерностью(n×m) - term-sentence matrix. В ней каждый ряд соответсвует слову из текстового документа (n слов), а каждая колонка соответсвует предложению из текстового документа (m предложений). Каждый элемент такой матрицы a_ij это вес слова i в предложение j (веса считются по метрике TDIDF, описанной выше). Если в предложении нет слова, то вес этого слова для этого предложения равен 0.  \n",
        "С помощью алгоритма SVD мы можем определить основные темы нашего документа.  \n",
        "Применяем SVD для полученной матрицы и получаем три матрицы (U, S, V).  \n",
        "$A = {U * S * V^T}$  \n",
        "Матрица U (n×m) представляет ортонормированную матрицу с весами слов  \n",
        "Матрица S представляет диагональную матрицу (m×m), где каждый ряд i соответсвует весу темы i  \n",
        "Матрица ${V^T}$ размерностью (n×n) представляет матрицу, где ряд это тема, а колонка это предложение, причем темы расположены в порядке важности. \n",
        "Произведение S*V описывает то, насколько предложение соответсвует теме  \n",
        "Имея такое разложение, как нам понять, какие предложения извлекать?  \n",
        "     - Gong and Liu (2001). Они использовали матрицу ${V^T}$ для извлечения предложений. Количество предложений, которую нужно выбрать, дано в виде параметра n. Каждый элемент этой матрицы показывает корреляцию между темой и предложением. Итак, в каждой строке мы выбираем наибольший показатель, а потом извлекам в наше краткое описание (реферат) n предложений с высокими показателями.  У данного алгоритма есть недостаток: мы не можем для одного топика выбрать несколько предложений, поэтому спустя несколько лет придумали модификацию к данному способу.  \n",
        "    - Рзаберемся еще раз подробнее. 𝐴 — матрица терм-предложение, полученная по исходному документу. Её размер равен 𝑛 × 𝑚, где 𝑛 — количество термов в документе, 𝑚 — количество предложений. Элемент 𝑎𝑖𝑗 этой матрицы равен частоте встречаемости терма 𝑖 в тексте, если этот терм встречается в предложении 𝑗, и 0 в противном случае. К полученной матрице применяется сингулярное разложение. \n",
        "$A = {U * S * V^T}$  \n",
        "𝑈 = — ортонормированная матрица размера 𝑛 × 𝑚, \n",
        "S = 𝑑𝑖𝑎𝑔(𝜎1, 𝜎2, . . . , 𝜎𝑚) — диагональная матрица,  \n",
        "𝑉 =  — ортонормированная матрица размера 𝑚 × 𝑚.\n",
        "Если 𝑟𝑎𝑛𝑘(𝐴) = 𝑟, то выполняется неравенство:  \n",
        "${\\sigma _1 >= \\sigma _2>= \\sigma _3...>= \\sigma _r>= \\sigma _r+1 = \\sigma _m = ... = 0}$  \n",
        "С точки зрения семантики сингулярное разложение матрицы 𝐴 интерпретируется как разбиение исходного документа на 𝑟 концепций (тем).\n",
        "Каждый элемент 𝑣𝑖𝑗 матрицы 𝑉 отражает степень информативности предложения 𝑗 по теме 𝑖. При этом значение 𝜎𝑖 матрицы Σ отражает\n",
        "степень важности темы 𝑖 в исходном документе. Каждому предложению ${s_k}$ исходного документа присваивается вес по формуле:  \n",
        "${s_k = \\sqrt{\\sum\\limits_{i=1}^m (v_{ik}) ^2 * \\sigma_i^2}}$  \n",
        "Т.о. больший вес получают предложения, наиболее информативные по одной из тем документа, при этом учитывается и степень важности концепции в документе. Значения весов предложений упорядочиваются по убыванию, и в реферат включаются предложения, соответствующие первым 𝑙 значениям,где 𝑙 — желаемое количество предложений в реферате.  \n",
        "11. ***LexRank 2004***  \n",
        "Для начала мы строим similarity matrix, затем представляем каждое предложение в виде вершины графа и смотрим, какие из предложений связаны и насколько их связь сильная по матрице схожести. На графе, если у одной вершины присуствует много ребер, значит6 она схожа со многими другими предложениями, следовательно, ее нужно взять, но отбросить предложения, с которыми она связана. Далее алгоритм PageRank.  \n",
        "12. ***TextRank***  \n",
        "TextRank это аналогия PageRank, но для предложений.  \n",
        "Почти такой же алгоритм, как и LexRank, был изобретен в то же время, но есть маленькие отличия, например, LexRank используется больше для multi-document classification, а TextRank наоборот, для single-document classification + различаются в similarity matrix.  \n",
        "Он заключается в следующем:  \n",
        "По тексту строится взвешенный неориентированный граф, вершины в котором обозначают предложения текста. Весом ребра между двумя\n",
        "вершинами является степень схожести двух предложений, соответствующих вершинам. Она вычисляется, как количество совпадающих слов\n",
        "в предложениях, нормированное суммарной длиной этих предложений.  \n",
        "Исходя из весов ребер, каждой вершине присваивается вес по следующей формуле:  \n",
        "$𝑊({𝑉_i}) = (1 − 𝑑) + 𝑑 * {\\sum\\limits_{V_j \\in Inc(V_i)}\\frac{w_{ij}}{\\sum\\limits_{V_k \\in Inc(V_i)}w_{jk}} * W({V_j})}$  \n",
        "где 𝑉𝑖, 𝑉𝑗 — вершины графа  \n",
        "𝐼𝑛𝑐(𝑉𝑖) — множество вершин, смежных с вершиной 𝑉𝑖  \n",
        "𝑤𝑖𝑗 — вес ребра между вершинами 𝑉𝑖, 𝑉𝑗  \n",
        "𝑑 — коэфициент затухания, который в данном алгоритме равен 0.85  \n",
        "Итерационный процесс завершается, как только веса вершин перестают меняться более, чем на 0.0001.\n",
        "После вычисления весов вершин они упорядочиваются по убыванию значения веса и в реферат включаются предложения, соответствующие первым 𝑛 вершинам, где 𝑛 — желаемое количество предложений в кратком описании.  \n",
        "13. ***ClusterRank***  \n",
        " Алгоритм:  \n",
        "     - Пусть изначально каждое предложение это один отдельный кластер. \n",
        "     - Объединим некоторые кластеры, если они схожи друг с другом, схожесть определяем по косинусовой сходимости.  \n",
        "     - Определяем вес каждого слова в кластере по заданой формуле:  \n",
        "     W = freq*IDF, где freq - количество предложений в кластере X, в которых присуствует слово w, IDF = $\\log \\frac{number of sentences}{number of sentences with the word w}$   \n",
        "      Как мы поймем, что нужно остановиться объединять классы? Изначально мы задаем similarity threshold, на основе которого мы и создаем из двух классов один класс.   \n",
        "      Основаня идея: выделяются кластеры предложений, описывающих одну идею. Далее из каждого кластера выбирается центроид (самое близкое к центру кластера предложение), которое попадает в финальную суммаризацию. Объединение этих идей - сначала кластеризация предложений и построение графа на основе полученной кластеризации и запуск PageRank для расчета весов.   \n",
        "https://github.com/AdiChat/senpai  \n",
        "14. ***Sequencetosequence***   \n",
        "Это рекуррентная нейронная сеть.\n",
        "Под капотом у модели находятся энкодер и декодер.  \n",
        "Энкодер обрабатывает каждый элемент входной последовательности, переводит полученную информацию в вектор, называемый контекстом (context). После обработки всей входной последовательности энкодер пересылает контекст декодеру, который затем начинает генерировать выходную последовательность элемент за элементом.  Но этот подход достаточно медленный, к каждому слову нам нужно применить операцию вычисления. Еще один недостаток заключается в том, что в конец вычисления скрытого состояния должна поместиться вся информация. Поэтому и придумали attention механзм: чем он отличается?  \n",
        "Во-первых, энкодер передает значительно больше данных декодеру: вместо передачи лишь последнего скрытого состояния после этапа кодирования, энкодер отправляет ему все свои скрытые состояния. Во-вторых, декодер проходит через дополнительный этап, прежде чем сгенерировать выход. Для того, чтобы сфокусироваться на тех частях входной последовательности, которые релевантны для соответствующего временного отрезка.   \n",
        "В 2018 году появился алгоритм BERT, который показал лучшие результаты к языковым моделям, чем реккурентные нейронные сети и seq2seq модели. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_cX_NY5anVT"
      },
      "source": [
        "## Описание тестового набора данных  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmiIWZZnanVT"
      },
      "source": [
        "Одна из главных и первых задач - это разобраться с данными, на которых мы будем обучать наши модели.  \n",
        "Какие возможные источники данных мы можем взять? \n",
        "1. Субтитры к фильмам и краткие содержания к ним  \n",
        "Думая о предстоящей второй части работы, алгоритмах распознования речи, датасет с субтитрами и аудиозаписями был бы одним из наиболее подходящих.  \n",
        "https://github.com/Desklop/Russian_subtitles_dataset\n",
        "https://yts-subs.com/subtitles/forrest-gump-1994-english-yify-125025\n",
        "2. CNN/Daily Mail\n",
        "3. Cornell Newsroom\n",
        "4. Google Dataset\n",
        "5. DUC\n",
        "6. Webis-TLDR-17 Corpus\n",
        "7. Датасет с совещаниями и их кратким саммари.   \n",
        "https://github.com/gcunhase/AMICorpusXML/tree/master/data/ami-summary/extractive  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQZcEwJefJwV"
      },
      "source": [
        "Наиболее подходящим датастетом является последний.  \n",
        "Этот датасет включает в себя транскрипты совещаний, их аудиозаписи (100 часов записей)  и краткое содержание совещания. Главным преимуществом данного датасета является то, что в нем есть разделение на extractive и abstractive. Пример текстового файла: ES2002a.transcript.txt, первые две буквы это страна, в котором записывалось данное совещание, одно совещание состоит из 4 частей: a, b, c и d.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28dEkz1nfJwW"
      },
      "source": [
        "Всего транскриптов совещаний для extractive методов: 137, для abstractive методов: 142. \n",
        "Посмотрим, что из себя представляют эти фалы на примере ES2002a.transcript.txt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXUWW70yanVU"
      },
      "source": [
        "test = open('ami-transcripts/ES2002a.transcript.txt', \"r\", errors = 'ignore')\n",
        "transcript = test.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anBpVjQWanVV",
        "outputId": "4b00d33e-3cc0-4d60-9cea-7fff9ca3b3e0"
      },
      "source": [
        "transcript"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Um I'm Craig and I'm User Interface. Yeah. Well, my favourite animal would be a monkey. Then they're small cute and furry, and uh when planet of the apes becomes real, I'm gonna be up there with them. Yeah. I know um My parents went out and bought um remote controls because um they got fed up of having four or five different remote controls for each things the house. So um for them it was just how many devices control. Uh.\\nMm-hmm. Great. And I'm Andrew and I'm uh our marketing expert. Mm-hmm. Mm-hmm. Yeah, that's that's it. Yeah. I will go. That's fine. Alright. So This one here, right? Okay. Very nice. Alright. My favourite animal is like A beagle. Um charac favourite characteristics of it? Is that right? Uh, right, well basically um high priority for any animal for me is that they be willing to take a lot of physical affection from their family. And, yeah that they have lots of personality and uh be fit and in robust good health. So this is blue. Blue beagle. My family's beagle. I coulda told you a whole lot more about beagles. Boy, let me tell you. Impressionist. Alright. Mm. Superb sketch, by the way. Yep. I see a dog in there. Yep. Now I see a rooster. What kind is it? Is he aware that th it's his own cha tail he's chasing? Hmm. Probably when he was little he got lots of attention for doing it and has forever been conditioned. 'Kay. Um, can we just go over that again? Uh, so bas at twel Alright, yeah. Okay. So cost like production cost is twelve fifty, but selling price is is that wholesale or retail? Like on the shelf. Our sale our sale anyway. Yeah, okay okay. Okay. Mm-hmm. Alright. Yes. Mm-hmm. Mm-hmm. Well right away I'm wondering if there's um th th uh, like with D_V_D_ players, if there are zones. Um f frequencies or something um as well as uh characters, um different uh keypad styles and s symbols. Um. I don't know. Yeah. Yeah. Yeah. And then a and then al the other thing international is on top of the price. I'm thinking the price might might appeal to a certain market in one region, whereas in another it'll be different, so Just a chara just a characteristic of the Just Or just like, basic product podi positioning, the twenty five Euro remote control might be a big hit in London, might not be such a big hit in Greece, who knows, something like that, yeah. Yep. Right away I'm making some kind of assumptions about what what information we're given here, thinking, 'kay trendy probably means something other than just basic, something other than just standard. Um so I'm wondering right away, is selling twenty five Euros, is that sort of the thi is this gonna to be like the premium product kinda thing or Uh-huh. Mm-hmm. Yep. Yeah, I'd say so, yeah. No. Yeah, yeah. Mm-hmm. Do we have any other background information on like how that compares to other other Yeah. Mm-hmm. Yeah, interesting thing about discussing um production of a remote control for me is that l as you point out, I just don't think of remote controls as somethin something people consciously assess in their purchasing habits. It's just like getting shoelaces with shoes or something. It just comes along. Do you know what I mean? Like so sort of like how do you I I mean one one way of looking at it would be, well the people producing television sets, maybe they have to buy remote controls. Or another way is maybe people who have T_V_ sets are really fed up with their remote control and they really want a better one or something. But Right. Right. Okay so Right, so in function one of the priorities might be to combine as many uses I think so. Yeah, yeah. Yeah. Well like um, maybe what we could use is a sort of like a example of a successful other piece technology is palm palm pilots. They're gone from being just like little sort of scribble boards to cameras, M_P_ three players, telephones, everything, agenda. So, like, I wonder if we might add something new to the to the remote control market, such as the lighting in your house, or um Yeah, yeah. An Yeah. Like, p personally for me, at home I've I've combined the um the audio video of my television set and my D_V_D_ player and my C_D_ player. So they w all work actually function together but I have different remote controls for each of them. So it's sort of ironic that that then they're in there um you know, the sound and everything it's just one system. But each one's got its own little part. Mm. Mm. Mm. Mm-hmm. Mm-hmm. Yeah. Yeah. That's just really good id Yep. Uh, sure. I remember when the first remote control my my family had was on a cable. Actually had a cable between it and the T_V_ and big like buttons that sort of like, like on a blender or something. And um, you know, when I think about what they are now, it's better, but actually it's still kind of, I dunno, like a massive junky thing on the table. Maybe we could think about how, could be more, you know, streamlined. S Something like that, yeah. Or whatever would be technologically reasonable. 'Cause it could b it could it could be that f it could be that functionally that doesn't make it any better, but that just the appeal of of not having You know, these days there's a r pe things in people's homes are becoming more and more like chic, you know. Um, nicer materials and might be be worth exploring anyway. Okay. Um. Before we wrap up, just to make sure we're all on the same page here, um, do we We were given sort of an example of a coffee machine or something, right? Well, um are we at ma right now on the assumption that our television remote control may have features which go beyond the television? Or are we keeping sort of like a a design commitment to television features? I I don't know. Yep. Yeah, sure. Okay. Okay, yeah. Okay. Okay. Okay. Alright.\\nOkay. Right. Um well this is the kick-off meeting for our our project. Um and um this is just what we're gonna be doing over the next twenty five minutes. Um so first of all, just to kind of make sure that we all know each other, I'm Laura and I'm the project manager. Do you want to introduce yourself again? Okay. Great. Okay. Um so we're designing a new remote control and um Oh I have to record who's here actually. So that's David, Andrew and Craig, isn't it? And you all arrived on time. Um yeah so des uh design a new remote control. Um, as you can see it's supposed to be original, trendy and user friendly. Um so that's kind of our our brief, as it were. Um and so there are three different stages to the design. Um I'm not really sure what what you guys have already received um in your emails. What did you get? Mm-hmm. Is that what everybody got? Okay. Um. So we're gonna have like individual work and then a meeting about it. And repeat that process three times. Um and at this point we get try out the whiteboard over there. Um. So uh you get to draw your favourite animal and sum up your favourite characteristics of it. So who would like to go first? Very good. Mm-hmm. Yeah. Yeah. Right. Lovely. Right. You can take as long over this as you like, because we haven't got an awful lot to discuss. Ok oh we do we do. Don't feel like you're in a rush, anyway. Ach why not We might have to get you up again then. I don't know what mine is. I'm gonna have to think on the spot now. Is that a whale? Ah. Okay. God, I still don't know what I'm gonna write about. Um. I was gonna choose a dog as well. But I'll just draw a different kind of dog. M my favourite animal is my own dog at home. Um That doesn't really look like him, actually. He looks more like a pig, actually. Ah well. Do you? Oh that's very good of you. Uh. Um he's a mixture of uh various things. Um and what do I like about him, um That's just to suggest that his tail wags. Um he's very friendly and cheery and always pleased to see you, and very kind of affectionate and um uh and he's quite quite wee as well so you know he can doesn't take up too much space. Um and uh And he does a funny thing where he chases his tail as well, which is quite amusing, so It is. I think it is. He only does it after he's had his dinner and um he'll just all of a sudden just get up and start chasing his tail 'round the living room. Yeah, so uh Yeah, maybe. Maybe. Right, um where did you find this? Just down here? Yeah. Okay. Um what are we doing next? Uh um. Okay, uh we now need to discuss the project finance. Um so according to the brief um we're gonna be selling this remote control for twenty five Euro, um and we're aiming to make fifty million Euro. Um so we're gonna be selling this on an international scale. And uh we don't want it to cost any more than uh twelve fifty Euros, so fifty percent of the selling price. Sure. All together. Um I dunno. I imagine That's a good question. I imagine it probably is our sale actually because it's probably up to the the um the retailer to uh sell it for whatever price they want. Um. But I I don't know, I mean do you think the fact that it's going to be sold internationally will have a bearing on how we design it at all? Think it will? Um. Hmm. Oh yeah, regions and stuff, yeah. Yeah. Okay. Yeah. Well for a remote control, do you think that will be I suppose it's depends on how complicated our remote control is. Yeah, yeah. Okay. What, just like in terms of like the wealth of the country? Like how much money people have to spend on things like? Aye, I see what you mean, yeah. Marketing. Good marketing thoughts. Oh gosh, I should be writing all this down. Um. Mm. Yeah. Yeah, yeah. Like how much does, you know, a remote control cost. Well twenty five Euro, I mean that's um that's about like eighteen pounds or something, isn't it? Or no, is it as much as that? Sixteen seventeen eighteen pounds. Um, I dunno, I've never bought a remote control, so I don't know how how good a remote control that would get you. Um. But yeah, I suppose it has to look kind of cool and gimmicky. Um right, okay. Let me just scoot on ahead here. Okay. Um well d Does anybody have anything to add to uh to the finance issue at all? Thin No, actually. That would be useful, though, wouldn't it, if you knew like what your money would get you now. Mm-hmm. Yeah, yeah. Oh. Five minutes to end of meeting. Oh, okay. We're a bit behind. Yeah. Right, so do you think that should be like a main design aim of our remote control d you know, do your your satellite and your regular telly and your V_C_R_ and everything? Mm-hmm. Yeah. Or even like, you know, notes about um what you wanna watch. Like you might put in there oh I want to watch such and such and look a Oh that's a good idea. So extra functionalities. Mm-hmm. Hmm. Um okay, uh I'd wel we're gonna have to wrap up pretty quickly in the next couple of minutes. Um I'll just check we've nothing else. Okay. Um so anything else anybody wants to add about what they don't like about remote controls they've used, what they would really like to be part of this new one at all? You keep losing them. Okay. Yeah. W You get those ones where you can, if you like, whistle or make a really high pitched noise they beep. There I mean is that something we'd want to include, do you think? Dunno. Okay maybe. My goodness. Still feels quite primitive. Maybe like a touch screen or something? Okay. Uh-huh, okay. Well I guess that's up to our industrial designer. It looks better. Yeah. Okay. Okay. Right, well um so just to wrap up, the next meeting's gonna be in thirty minutes. So that's about um about ten to twelve by my watch. Um so inbetween now and then, um as the industrial designer, you're gonna be working on you know the actual working design of it so y you know what you're doing there. Um for user interface, technical functions, I guess that's you know like what we've been talking about, what it'll actually do. Um and uh marketing executive, you'll be just thinking about what it actually what, you know, what requirements it has to has to fulfil and you'll all get instructions emailed to you, I guess. Um. Yeah, so it's th the functional design stage is next, I guess. And uh and that's the end of the meeting. So I got that little message a lot sooner than I thought I would, so Mm-hmm. Uh-huh, yeah. Th Okay, well just very quickly 'cause this we're supposed to finish now. Um I guess that's up to us, I mean you probably want some kind of unique selling point of it, so um, you know Yeah. Mm-hmm. Yeah. Okay. Right, okay, we'll that's that's the end of the meeting, then. Um. So, uh thank you all for coming.\\nHi, I'm David and I'm supposed to be an industrial designer. Um, I just got the project announcement about what the project is. Designing a remote control. That's about it, didn't get anything else. Did you get the same thing? Cool. There's too much gear. Okay. Can't draw. Um. Yeah. Um, well anyway, I don't know, it's just the first animal I can think off the top of my head. Um. Yes. Big reason is 'cause I'm allergic to most animals. Allergic to animal fur, so um fish was a natural choice. Um, yeah, and I kind of like whales. They come in and go eat everything in sight. And they're quite harmless and mild and interesting. Tail's a bit big, I think. It's an after dinner dog then. Hmm. It does make sense from maybe the design point of view 'cause you have more complicated characters like European languages, then you need more buttons. So, possibly. Hmm. Yeah. And you keep losing them. Finding them is really a pain, you know. I mean it's usually quite small, or when you want it right, it slipped behind the couch or it's kicked under the table. You know. Yep. Mm-hmm. I think one factor would be production cost. Because there's a cap there, so um depends on how much you can cram into that price. Um. I think that that's the main factor. Cool.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6mLS2IpanVV"
      },
      "source": [
        "test2 = open('extractive/ES2002a.extsumm.txt', \"r\", errors = 'ignore')\n",
        "transcript_sum = test2.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHWkb4TGanVW",
        "outputId": "ee099615-03f7-420f-fb7e-1505e0316ebc"
      },
      "source": [
        "transcript_sum"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Um well this is the kick-off meeting for our our project . so we're designing a new remote control and um Um , as you can see it's supposed to be original , trendy and user friendly . Um and so there are three different stages to the design . So we're gonna have like individual work and then a meeting about it . And repeat that process three times . So uh you get to draw your favourite animal and sum up your favourite characteristics of it . My favourite animal is like A beagle . Uh , right , well basically um high priority for any animal for me is that they be willing to take a lot of physical affection from their family . And , yeah that they have lots of personality Well Then they're small cute and furry , and I kind of like whales . They come in and go eat everything in sight . M my favourite animal is my own dog at home . Um he's very friendly and cheery and always pleased to see you , Um so according to the brief um we're gonna be selling this remote control for twenty five Euro , And uh we don't want it to cost any more than uh twelve fifty Euros , so fifty percent of the selling price . but selling price is is that wholesale or retail ? I imagine it probably is our sale actually because it's probably up to the the um the retailer to uh sell it for whatever price they want . Um . I mean do you think the fact that it's going to be sold internationally will have a bearing on how we design it at all ? Well right away I'm wondering if there's um th th uh , like with D_V_D_ players , if there are zones . um as well as uh characters , um different uh keypad styles and s symbols . 'cause you have more complicated characters like European languages , then you need more buttons . I'm thinking the price might might appeal to a certain market in one region , whereas in another it'll be different , so thinking , 'kay trendy probably means something other than just basic , Like how much does , you know , a remote control cost . Well twenty five Euro , I mean that's um that's about like eighteen pounds or something , thi is this gonna to be like the premium product kinda thing or so I don't know how how good a remote control that would get you . Um . But yeah , I suppose it has to look kind of cool and gimmicky . I just don't think of remote controls as somethin something people consciously assess in their purchasing habits . I I mean one one way of looking at it would be , well the people producing television sets , maybe they have to buy remote controls . Or another way is maybe people who have T_V_ sets are really fed up with their remote control and they really want a better one or something . My parents went out and bought um remote controls because um they got fed up of having four or five different remote controls for each things the house . So um for them it was just how many devices control . So extra functionalities . So , like , I wonder if we might add something new to the to the remote control market , so in function one of the priorities might be to combine as many uses Right , so do you think that should be like a main design aim of our remote control d you know , do your your satellite and your regular telly and your V_C_R_ and everything ? maybe what we could use is a sort of like a example of a successful other piece technology is palm palm pilots . You keep losing them . W You get those ones where you can , if you like , whistle or make a really high pitched noise they beep . Maybe we could think about how , could be more , you know , streamlined . S Maybe like a touch screen or something ? Or whatever would be technologically reasonable . Um so inbetween now and then , um as the industrial designer , you're gonna be working on you know the actual working design of it Um for user interface , technical functions , Um and uh marketing executive , you'll be just thinking about what it actually what , you know , what requirements it has to has to fulfil Yeah , so it's th the functional design stage is next , I guess . are we at ma right now on the assumption that our television remote control may have features which go beyond the television ? Or are we keeping sort of like a a design commitment to television features ? I think one factor would be production cost . I mean you probably want some kind of unique selling point of it , \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLiUMackanVW",
        "outputId": "9d56c071-f953-4a7c-eca8-7b231feba5fb"
      },
      "source": [
        "print('The length of the transription: {}'.format(len(transcript)))\n",
        "print('The length of the summary of the transcription: {}'.format(len(transcript_sum)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of the transription: 13676\n",
            "The length of the summary of the transcription: 4318\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFzqQK0ffJwa"
      },
      "source": [
        "Создадим 2 отдельных датасета для extractive методов и abstarctive методов с соотвественными им аудиофайлами. В датасете будет содержаться название файлов. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKVSmBZrfJwa",
        "outputId": "385e03f6-585d-48d7-aa70-4a53dc0a3593"
      },
      "source": [
        "import os\n",
        "folder_ext = 'extractive'\n",
        "ext = [x[2] for x in os.walk(folder_ext)][0]\n",
        "ext_summaries_files = []\n",
        "for i in ext:\n",
        "    ext_summaries_files.append('extractive/' + i)\n",
        "ext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TS3007d.extsumm.txt',\n",
              " 'ES2010d.extsumm.txt',\n",
              " 'IS1008a.extsumm.txt',\n",
              " 'ES2005d.extsumm.txt',\n",
              " 'IS1005b.extsumm.txt',\n",
              " 'ES2002d.extsumm.txt',\n",
              " 'IS1002b.extsumm.txt',\n",
              " 'TS3012d.extsumm.txt',\n",
              " 'ES2004a.extsumm.txt',\n",
              " 'ES2012c.extsumm.txt',\n",
              " 'TS3005c.extsumm.txt',\n",
              " 'ES2003a.extsumm.txt',\n",
              " 'ES2015c.extsumm.txt',\n",
              " 'ES2009b.extsumm.txt',\n",
              " 'IS1009d.extsumm.txt',\n",
              " 'TS3006a.extsumm.txt',\n",
              " 'TS3010c.extsumm.txt',\n",
              " 'ES2016a.extsumm.txt',\n",
              " 'ES2007c.extsumm.txt',\n",
              " 'ES2011a.extsumm.txt',\n",
              " 'IS1009a.extsumm.txt',\n",
              " 'ES2003d.extsumm.txt',\n",
              " 'IS1003b.extsumm.txt',\n",
              " 'ES2004d.extsumm.txt',\n",
              " 'IS1004b.extsumm.txt',\n",
              " 'ES2011d.extsumm.txt',\n",
              " 'ES2016d.extsumm.txt',\n",
              " 'TS3006d.extsumm.txt',\n",
              " 'ES2010a.extsumm.txt',\n",
              " 'ES2006c.extsumm.txt',\n",
              " 'TS3011c.extsumm.txt',\n",
              " 'TS3007a.extsumm.txt',\n",
              " 'TS3012a.extsumm.txt',\n",
              " 'ES2014c.extsumm.txt',\n",
              " 'ES2002a.extsumm.txt',\n",
              " 'TS3004c.extsumm.txt',\n",
              " 'ES2013c.extsumm.txt',\n",
              " 'ES2005a.extsumm.txt',\n",
              " 'TS3003c.extsumm.txt',\n",
              " 'ES2008b.extsumm.txt',\n",
              " 'IS1008d.extsumm.txt',\n",
              " 'TS3009c.extsumm.txt',\n",
              " 'ES2008a.extsumm.txt',\n",
              " 'IS1002d.extsumm.txt',\n",
              " 'ES2002b.extsumm.txt',\n",
              " 'TS3012b.extsumm.txt',\n",
              " 'ES2005b.extsumm.txt',\n",
              " 'ES2010b.extsumm.txt',\n",
              " 'TS3007b.extsumm.txt',\n",
              " 'IS1007c.extsumm.txt',\n",
              " 'IS1000c.extsumm.txt',\n",
              " 'IS1003a.extsumm.txt',\n",
              " 'IS1004a.extsumm.txt',\n",
              " 'IS1009b.extsumm.txt',\n",
              " 'ES2009d.extsumm.txt',\n",
              " 'ES2016b.extsumm.txt',\n",
              " 'TS3006b.extsumm.txt',\n",
              " 'ES2011b.extsumm.txt',\n",
              " 'ES2009a.extsumm.txt',\n",
              " 'TS3008c.extsumm.txt',\n",
              " 'IS1004d.extsumm.txt',\n",
              " 'ES2004b.extsumm.txt',\n",
              " 'IS1003d.extsumm.txt',\n",
              " 'ES2003b.extsumm.txt',\n",
              " 'IS1005a.extsumm.txt',\n",
              " 'IS1008b.extsumm.txt',\n",
              " 'ES2008d.extsumm.txt',\n",
              " 'IS1001c.extsumm.txt',\n",
              " 'IS1006c.extsumm.txt',\n",
              " 'TS3003b.extsumm.txt',\n",
              " 'ES2013b.extsumm.txt',\n",
              " 'TS3004b.extsumm.txt',\n",
              " 'ES2014b.extsumm.txt',\n",
              " 'ES2008c.extsumm.txt',\n",
              " 'TS3009a.extsumm.txt',\n",
              " 'IS1001d.extsumm.txt',\n",
              " 'TS3011b.extsumm.txt',\n",
              " 'IS1006d.extsumm.txt',\n",
              " 'ES2006b.extsumm.txt',\n",
              " 'IS1000a.extsumm.txt',\n",
              " 'IS1007a.extsumm.txt',\n",
              " 'TS3008d.extsumm.txt',\n",
              " 'IS1004c.extsumm.txt',\n",
              " 'IS1003c.extsumm.txt',\n",
              " 'IS1007d.extsumm.txt',\n",
              " 'ES2007b.extsumm.txt',\n",
              " 'TS3010b.extsumm.txt',\n",
              " 'IS1000d.extsumm.txt',\n",
              " 'ES2015b.extsumm.txt',\n",
              " 'TS3005b.extsumm.txt',\n",
              " 'ES2012b.extsumm.txt',\n",
              " 'TS3008a.extsumm.txt',\n",
              " 'ES2009c.extsumm.txt',\n",
              " 'TS3009d.extsumm.txt',\n",
              " 'IS1002c.extsumm.txt',\n",
              " 'IS1005c.extsumm.txt',\n",
              " 'IS1006a.extsumm.txt',\n",
              " 'IS1001a.extsumm.txt',\n",
              " 'ES2006d.extsumm.txt',\n",
              " 'IS1006b.extsumm.txt',\n",
              " 'IS1001b.extsumm.txt',\n",
              " 'TS3011d.extsumm.txt',\n",
              " 'TS3004d.extsumm.txt',\n",
              " 'ES2014d.extsumm.txt',\n",
              " 'TS3003d.extsumm.txt',\n",
              " 'ES2013d.extsumm.txt',\n",
              " 'IS1008c.extsumm.txt',\n",
              " 'TS3008b.extsumm.txt',\n",
              " 'ES2003c.extsumm.txt',\n",
              " 'TS3005a.extsumm.txt',\n",
              " 'ES2015a.extsumm.txt',\n",
              " 'ES2004c.extsumm.txt',\n",
              " 'ES2012a.extsumm.txt',\n",
              " 'ES2007a.extsumm.txt',\n",
              " 'ES2011c.extsumm.txt',\n",
              " 'TS3006c.extsumm.txt',\n",
              " 'ES2016c.extsumm.txt',\n",
              " 'TS3010a.extsumm.txt',\n",
              " 'ES2012d.extsumm.txt',\n",
              " 'ES2015d.extsumm.txt',\n",
              " 'TS3005d.extsumm.txt',\n",
              " 'IS1009c.extsumm.txt',\n",
              " 'TS3010d.extsumm.txt',\n",
              " 'IS1000b.extsumm.txt',\n",
              " 'ES2007d.extsumm.txt',\n",
              " 'IS1007b.extsumm.txt',\n",
              " 'TS3011a.extsumm.txt',\n",
              " 'TS3007c.extsumm.txt',\n",
              " 'ES2010c.extsumm.txt',\n",
              " 'ES2006a.extsumm.txt',\n",
              " 'TS3009b.extsumm.txt',\n",
              " 'ES2013a.extsumm.txt',\n",
              " 'TS3003a.extsumm.txt',\n",
              " 'ES2005c.extsumm.txt',\n",
              " 'ES2014a.extsumm.txt',\n",
              " 'TS3004a.extsumm.txt',\n",
              " 'ES2002c.extsumm.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLPQoNqWfJwb",
        "outputId": "b4102cf4-b911-4dc1-dc52-3fd4335a8791"
      },
      "source": [
        "folder_transc = 'ami-transcripts'\n",
        "fol_trans = [x[2] for x in os.walk(folder_transc)]\n",
        "ext_sum_trans = []\n",
        "for i in ext:\n",
        "    ext_sum_trans.append('ami-transcripts/' + i[ : i.find(\".\")] + '.transcript.txt' )\n",
        "ext_sum_trans"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ami-transcripts/TS3007d.transcript.txt',\n",
              " 'ami-transcripts/ES2010d.transcript.txt',\n",
              " 'ami-transcripts/IS1008a.transcript.txt',\n",
              " 'ami-transcripts/ES2005d.transcript.txt',\n",
              " 'ami-transcripts/IS1005b.transcript.txt',\n",
              " 'ami-transcripts/ES2002d.transcript.txt',\n",
              " 'ami-transcripts/IS1002b.transcript.txt',\n",
              " 'ami-transcripts/TS3012d.transcript.txt',\n",
              " 'ami-transcripts/ES2004a.transcript.txt',\n",
              " 'ami-transcripts/ES2012c.transcript.txt',\n",
              " 'ami-transcripts/TS3005c.transcript.txt',\n",
              " 'ami-transcripts/ES2003a.transcript.txt',\n",
              " 'ami-transcripts/ES2015c.transcript.txt',\n",
              " 'ami-transcripts/ES2009b.transcript.txt',\n",
              " 'ami-transcripts/IS1009d.transcript.txt',\n",
              " 'ami-transcripts/TS3006a.transcript.txt',\n",
              " 'ami-transcripts/TS3010c.transcript.txt',\n",
              " 'ami-transcripts/ES2016a.transcript.txt',\n",
              " 'ami-transcripts/ES2007c.transcript.txt',\n",
              " 'ami-transcripts/ES2011a.transcript.txt',\n",
              " 'ami-transcripts/IS1009a.transcript.txt',\n",
              " 'ami-transcripts/ES2003d.transcript.txt',\n",
              " 'ami-transcripts/IS1003b.transcript.txt',\n",
              " 'ami-transcripts/ES2004d.transcript.txt',\n",
              " 'ami-transcripts/IS1004b.transcript.txt',\n",
              " 'ami-transcripts/ES2011d.transcript.txt',\n",
              " 'ami-transcripts/ES2016d.transcript.txt',\n",
              " 'ami-transcripts/TS3006d.transcript.txt',\n",
              " 'ami-transcripts/ES2010a.transcript.txt',\n",
              " 'ami-transcripts/ES2006c.transcript.txt',\n",
              " 'ami-transcripts/TS3011c.transcript.txt',\n",
              " 'ami-transcripts/TS3007a.transcript.txt',\n",
              " 'ami-transcripts/TS3012a.transcript.txt',\n",
              " 'ami-transcripts/ES2014c.transcript.txt',\n",
              " 'ami-transcripts/ES2002a.transcript.txt',\n",
              " 'ami-transcripts/TS3004c.transcript.txt',\n",
              " 'ami-transcripts/ES2013c.transcript.txt',\n",
              " 'ami-transcripts/ES2005a.transcript.txt',\n",
              " 'ami-transcripts/TS3003c.transcript.txt',\n",
              " 'ami-transcripts/ES2008b.transcript.txt',\n",
              " 'ami-transcripts/IS1008d.transcript.txt',\n",
              " 'ami-transcripts/TS3009c.transcript.txt',\n",
              " 'ami-transcripts/ES2008a.transcript.txt',\n",
              " 'ami-transcripts/IS1002d.transcript.txt',\n",
              " 'ami-transcripts/ES2002b.transcript.txt',\n",
              " 'ami-transcripts/TS3012b.transcript.txt',\n",
              " 'ami-transcripts/ES2005b.transcript.txt',\n",
              " 'ami-transcripts/ES2010b.transcript.txt',\n",
              " 'ami-transcripts/TS3007b.transcript.txt',\n",
              " 'ami-transcripts/IS1007c.transcript.txt',\n",
              " 'ami-transcripts/IS1000c.transcript.txt',\n",
              " 'ami-transcripts/IS1003a.transcript.txt',\n",
              " 'ami-transcripts/IS1004a.transcript.txt',\n",
              " 'ami-transcripts/IS1009b.transcript.txt',\n",
              " 'ami-transcripts/ES2009d.transcript.txt',\n",
              " 'ami-transcripts/ES2016b.transcript.txt',\n",
              " 'ami-transcripts/TS3006b.transcript.txt',\n",
              " 'ami-transcripts/ES2011b.transcript.txt',\n",
              " 'ami-transcripts/ES2009a.transcript.txt',\n",
              " 'ami-transcripts/TS3008c.transcript.txt',\n",
              " 'ami-transcripts/IS1004d.transcript.txt',\n",
              " 'ami-transcripts/ES2004b.transcript.txt',\n",
              " 'ami-transcripts/IS1003d.transcript.txt',\n",
              " 'ami-transcripts/ES2003b.transcript.txt',\n",
              " 'ami-transcripts/IS1005a.transcript.txt',\n",
              " 'ami-transcripts/IS1008b.transcript.txt',\n",
              " 'ami-transcripts/ES2008d.transcript.txt',\n",
              " 'ami-transcripts/IS1001c.transcript.txt',\n",
              " 'ami-transcripts/IS1006c.transcript.txt',\n",
              " 'ami-transcripts/TS3003b.transcript.txt',\n",
              " 'ami-transcripts/ES2013b.transcript.txt',\n",
              " 'ami-transcripts/TS3004b.transcript.txt',\n",
              " 'ami-transcripts/ES2014b.transcript.txt',\n",
              " 'ami-transcripts/ES2008c.transcript.txt',\n",
              " 'ami-transcripts/TS3009a.transcript.txt',\n",
              " 'ami-transcripts/IS1001d.transcript.txt',\n",
              " 'ami-transcripts/TS3011b.transcript.txt',\n",
              " 'ami-transcripts/IS1006d.transcript.txt',\n",
              " 'ami-transcripts/ES2006b.transcript.txt',\n",
              " 'ami-transcripts/IS1000a.transcript.txt',\n",
              " 'ami-transcripts/IS1007a.transcript.txt',\n",
              " 'ami-transcripts/TS3008d.transcript.txt',\n",
              " 'ami-transcripts/IS1004c.transcript.txt',\n",
              " 'ami-transcripts/IS1003c.transcript.txt',\n",
              " 'ami-transcripts/IS1007d.transcript.txt',\n",
              " 'ami-transcripts/ES2007b.transcript.txt',\n",
              " 'ami-transcripts/TS3010b.transcript.txt',\n",
              " 'ami-transcripts/IS1000d.transcript.txt',\n",
              " 'ami-transcripts/ES2015b.transcript.txt',\n",
              " 'ami-transcripts/TS3005b.transcript.txt',\n",
              " 'ami-transcripts/ES2012b.transcript.txt',\n",
              " 'ami-transcripts/TS3008a.transcript.txt',\n",
              " 'ami-transcripts/ES2009c.transcript.txt',\n",
              " 'ami-transcripts/TS3009d.transcript.txt',\n",
              " 'ami-transcripts/IS1002c.transcript.txt',\n",
              " 'ami-transcripts/IS1005c.transcript.txt',\n",
              " 'ami-transcripts/IS1006a.transcript.txt',\n",
              " 'ami-transcripts/IS1001a.transcript.txt',\n",
              " 'ami-transcripts/ES2006d.transcript.txt',\n",
              " 'ami-transcripts/IS1006b.transcript.txt',\n",
              " 'ami-transcripts/IS1001b.transcript.txt',\n",
              " 'ami-transcripts/TS3011d.transcript.txt',\n",
              " 'ami-transcripts/TS3004d.transcript.txt',\n",
              " 'ami-transcripts/ES2014d.transcript.txt',\n",
              " 'ami-transcripts/TS3003d.transcript.txt',\n",
              " 'ami-transcripts/ES2013d.transcript.txt',\n",
              " 'ami-transcripts/IS1008c.transcript.txt',\n",
              " 'ami-transcripts/TS3008b.transcript.txt',\n",
              " 'ami-transcripts/ES2003c.transcript.txt',\n",
              " 'ami-transcripts/TS3005a.transcript.txt',\n",
              " 'ami-transcripts/ES2015a.transcript.txt',\n",
              " 'ami-transcripts/ES2004c.transcript.txt',\n",
              " 'ami-transcripts/ES2012a.transcript.txt',\n",
              " 'ami-transcripts/ES2007a.transcript.txt',\n",
              " 'ami-transcripts/ES2011c.transcript.txt',\n",
              " 'ami-transcripts/TS3006c.transcript.txt',\n",
              " 'ami-transcripts/ES2016c.transcript.txt',\n",
              " 'ami-transcripts/TS3010a.transcript.txt',\n",
              " 'ami-transcripts/ES2012d.transcript.txt',\n",
              " 'ami-transcripts/ES2015d.transcript.txt',\n",
              " 'ami-transcripts/TS3005d.transcript.txt',\n",
              " 'ami-transcripts/IS1009c.transcript.txt',\n",
              " 'ami-transcripts/TS3010d.transcript.txt',\n",
              " 'ami-transcripts/IS1000b.transcript.txt',\n",
              " 'ami-transcripts/ES2007d.transcript.txt',\n",
              " 'ami-transcripts/IS1007b.transcript.txt',\n",
              " 'ami-transcripts/TS3011a.transcript.txt',\n",
              " 'ami-transcripts/TS3007c.transcript.txt',\n",
              " 'ami-transcripts/ES2010c.transcript.txt',\n",
              " 'ami-transcripts/ES2006a.transcript.txt',\n",
              " 'ami-transcripts/TS3009b.transcript.txt',\n",
              " 'ami-transcripts/ES2013a.transcript.txt',\n",
              " 'ami-transcripts/TS3003a.transcript.txt',\n",
              " 'ami-transcripts/ES2005c.transcript.txt',\n",
              " 'ami-transcripts/ES2014a.transcript.txt',\n",
              " 'ami-transcripts/TS3004a.transcript.txt',\n",
              " 'ami-transcripts/ES2002c.transcript.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opxP5d1yfJwc",
        "outputId": "2d8bb8b6-1b1e-45d5-80c2-df3fdc6e1d88"
      },
      "source": [
        "folder_abs = 'abstractive'\n",
        "abs_ = [x[2] for x in os.walk(folder_abs)]\n",
        "abs_summaries_files = []\n",
        "for i in abs_[0]:\n",
        "    abs_summaries_files.append('abstractive/' + i)\n",
        "abs_summaries_files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abstractive/ES2015a.abssumm.txt',\n",
              " 'abstractive/TS3005a.abssumm.txt',\n",
              " 'abstractive/ES2003c.abssumm.txt',\n",
              " 'abstractive/ES2012a.abssumm.txt',\n",
              " 'abstractive/ES2004c.abssumm.txt',\n",
              " 'abstractive/TS3008b.abssumm.txt',\n",
              " 'abstractive/ES2011c.abssumm.txt',\n",
              " 'abstractive/ES2007a.abssumm.txt',\n",
              " 'abstractive/TS3010a.abssumm.txt',\n",
              " 'abstractive/ES2016c.abssumm.txt',\n",
              " 'abstractive/TS3006c.abssumm.txt',\n",
              " 'abstractive/IB4005.abssumm.txt',\n",
              " 'abstractive/IB4010.abssumm.txt',\n",
              " 'abstractive/ES2006d.abssumm.txt',\n",
              " 'abstractive/IS1006b.abssumm.txt',\n",
              " 'abstractive/TS3011d.abssumm.txt',\n",
              " 'abstractive/IS1001b.abssumm.txt',\n",
              " 'abstractive/IS1008c.abssumm.txt',\n",
              " 'abstractive/ES2014d.abssumm.txt',\n",
              " 'abstractive/TS3004d.abssumm.txt',\n",
              " 'abstractive/ES2013d.abssumm.txt',\n",
              " 'abstractive/TS3003d.abssumm.txt',\n",
              " 'abstractive/TS3007c.abssumm.txt',\n",
              " 'abstractive/TS3011a.abssumm.txt',\n",
              " 'abstractive/ES2006a.abssumm.txt',\n",
              " 'abstractive/ES2010c.abssumm.txt',\n",
              " 'abstractive/ES2005c.abssumm.txt',\n",
              " 'abstractive/TS3003a.abssumm.txt',\n",
              " 'abstractive/ES2013a.abssumm.txt',\n",
              " 'abstractive/ES2002c.abssumm.txt',\n",
              " 'abstractive/TS3004a.abssumm.txt',\n",
              " 'abstractive/TS3012c.abssumm.txt',\n",
              " 'abstractive/ES2014a.abssumm.txt',\n",
              " 'abstractive/TS3009b.abssumm.txt',\n",
              " 'abstractive/IS1009c.abssumm.txt',\n",
              " 'abstractive/ES2012d.abssumm.txt',\n",
              " 'abstractive/TS3005d.abssumm.txt',\n",
              " 'abstractive/ES2015d.abssumm.txt',\n",
              " 'abstractive/IS1000b.abssumm.txt',\n",
              " 'abstractive/TS3010d.abssumm.txt',\n",
              " 'abstractive/ES2007d.abssumm.txt',\n",
              " 'abstractive/IS1007b.abssumm.txt',\n",
              " 'abstractive/IS1000a.abssumm.txt',\n",
              " 'abstractive/IS1007a.abssumm.txt',\n",
              " 'abstractive/IB4003.abssumm.txt',\n",
              " 'abstractive/IS1004c.abssumm.txt',\n",
              " 'abstractive/IS1003c.abssumm.txt',\n",
              " 'abstractive/TS3008d.abssumm.txt',\n",
              " 'abstractive/ES2008c.abssumm.txt',\n",
              " 'abstractive/TS3009a.abssumm.txt',\n",
              " 'abstractive/ES2013b.abssumm.txt',\n",
              " 'abstractive/TS3003b.abssumm.txt',\n",
              " 'abstractive/ES2014b.abssumm.txt',\n",
              " 'abstractive/TS3004b.abssumm.txt',\n",
              " 'abstractive/TS3011b.abssumm.txt',\n",
              " 'abstractive/IS1001d.abssumm.txt',\n",
              " 'abstractive/IS1006d.abssumm.txt',\n",
              " 'abstractive/ES2006b.abssumm.txt',\n",
              " 'abstractive/IS1002c.abssumm.txt',\n",
              " 'abstractive/IS1005c.abssumm.txt',\n",
              " 'abstractive/TS3009d.abssumm.txt',\n",
              " 'abstractive/IS1006a.abssumm.txt',\n",
              " 'abstractive/IS1001a.abssumm.txt',\n",
              " 'abstractive/IS1007d.abssumm.txt',\n",
              " 'abstractive/ES2007b.abssumm.txt',\n",
              " 'abstractive/IS1000d.abssumm.txt',\n",
              " 'abstractive/TS3010b.abssumm.txt',\n",
              " 'abstractive/TS3008a.abssumm.txt',\n",
              " 'abstractive/ES2009c.abssumm.txt',\n",
              " 'abstractive/TS3005b.abssumm.txt',\n",
              " 'abstractive/ES2015b.abssumm.txt',\n",
              " 'abstractive/ES2012b.abssumm.txt',\n",
              " 'abstractive/IS1007c.abssumm.txt',\n",
              " 'abstractive/IS1000c.abssumm.txt',\n",
              " 'abstractive/IS1009b.abssumm.txt',\n",
              " 'abstractive/ES2009d.abssumm.txt',\n",
              " 'abstractive/IS1003a.abssumm.txt',\n",
              " 'abstractive/IS1004a.abssumm.txt',\n",
              " 'abstractive/TS3012b.abssumm.txt',\n",
              " 'abstractive/IS1002d.abssumm.txt',\n",
              " 'abstractive/ES2002b.abssumm.txt',\n",
              " 'abstractive/ES2005b.abssumm.txt',\n",
              " 'abstractive/TS3009c.abssumm.txt',\n",
              " 'abstractive/ES2008a.abssumm.txt',\n",
              " 'abstractive/ES2010b.abssumm.txt',\n",
              " 'abstractive/TS3007b.abssumm.txt',\n",
              " 'abstractive/IS1008b.abssumm.txt',\n",
              " 'abstractive/ES2008d.abssumm.txt',\n",
              " 'abstractive/IS1005a.abssumm.txt',\n",
              " 'abstractive/IB4011.abssumm.txt',\n",
              " 'abstractive/IS1001c.abssumm.txt',\n",
              " 'abstractive/IS1006c.abssumm.txt',\n",
              " 'abstractive/TS3006b.abssumm.txt',\n",
              " 'abstractive/ES2016b.abssumm.txt',\n",
              " 'abstractive/ES2011b.abssumm.txt',\n",
              " 'abstractive/IS1004d.abssumm.txt',\n",
              " 'abstractive/ES2004b.abssumm.txt',\n",
              " 'abstractive/IS1003d.abssumm.txt',\n",
              " 'abstractive/ES2003b.abssumm.txt',\n",
              " 'abstractive/ES2009a.abssumm.txt',\n",
              " 'abstractive/TS3008c.abssumm.txt',\n",
              " 'abstractive/ES2009b.abssumm.txt',\n",
              " 'abstractive/IS1009d.abssumm.txt',\n",
              " 'abstractive/ES2012c.abssumm.txt',\n",
              " 'abstractive/ES2004a.abssumm.txt',\n",
              " 'abstractive/ES2015c.abssumm.txt',\n",
              " 'abstractive/ES2003a.abssumm.txt',\n",
              " 'abstractive/TS3005c.abssumm.txt',\n",
              " 'abstractive/ES2016a.abssumm.txt',\n",
              " 'abstractive/TS3010c.abssumm.txt',\n",
              " 'abstractive/TS3006a.abssumm.txt',\n",
              " 'abstractive/ES2011a.abssumm.txt',\n",
              " 'abstractive/ES2007c.abssumm.txt',\n",
              " 'abstractive/TS3007d.abssumm.txt',\n",
              " 'abstractive/ES2010d.abssumm.txt',\n",
              " 'abstractive/ES2005d.abssumm.txt',\n",
              " 'abstractive/IS1005b.abssumm.txt',\n",
              " 'abstractive/TS3012d.abssumm.txt',\n",
              " 'abstractive/ES2002d.abssumm.txt',\n",
              " 'abstractive/IS1002b.abssumm.txt',\n",
              " 'abstractive/IS1008a.abssumm.txt',\n",
              " 'abstractive/ES2006c.abssumm.txt',\n",
              " 'abstractive/ES2010a.abssumm.txt',\n",
              " 'abstractive/TS3007a.abssumm.txt',\n",
              " 'abstractive/TS3011c.abssumm.txt',\n",
              " 'abstractive/ES2008b.abssumm.txt',\n",
              " 'abstractive/IS1008d.abssumm.txt',\n",
              " 'abstractive/TS3004c.abssumm.txt',\n",
              " 'abstractive/ES2002a.abssumm.txt',\n",
              " 'abstractive/ES2014c.abssumm.txt',\n",
              " 'abstractive/TS3012a.abssumm.txt',\n",
              " 'abstractive/TS3003c.abssumm.txt',\n",
              " 'abstractive/ES2005a.abssumm.txt',\n",
              " 'abstractive/ES2013c.abssumm.txt',\n",
              " 'abstractive/ES2003d.abssumm.txt',\n",
              " 'abstractive/IS1003b.abssumm.txt',\n",
              " 'abstractive/ES2004d.abssumm.txt',\n",
              " 'abstractive/IS1004b.abssumm.txt',\n",
              " 'abstractive/IS1009a.abssumm.txt',\n",
              " 'abstractive/ES2011d.abssumm.txt',\n",
              " 'abstractive/TS3006d.abssumm.txt',\n",
              " 'abstractive/ES2016d.abssumm.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT9Zj_tjfJwd",
        "outputId": "dc674ea3-5d02-48b0-9c7a-1bb9e3c898a6"
      },
      "source": [
        "folder_transc = 'ami-transcripts'\n",
        "fol_trans = [x[2] for x in os.walk(folder_transc)]\n",
        "abs_sum_trans = []\n",
        "for i in abs_[0]:\n",
        "    abs_sum_trans.append('ami-transcripts/' + i[ : i.find(\".\")] + '.transcript.txt' )\n",
        "abs_sum_trans"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ami-transcripts/ES2015a.transcript.txt',\n",
              " 'ami-transcripts/TS3005a.transcript.txt',\n",
              " 'ami-transcripts/ES2003c.transcript.txt',\n",
              " 'ami-transcripts/ES2012a.transcript.txt',\n",
              " 'ami-transcripts/ES2004c.transcript.txt',\n",
              " 'ami-transcripts/TS3008b.transcript.txt',\n",
              " 'ami-transcripts/ES2011c.transcript.txt',\n",
              " 'ami-transcripts/ES2007a.transcript.txt',\n",
              " 'ami-transcripts/TS3010a.transcript.txt',\n",
              " 'ami-transcripts/ES2016c.transcript.txt',\n",
              " 'ami-transcripts/TS3006c.transcript.txt',\n",
              " 'ami-transcripts/IB4005.transcript.txt',\n",
              " 'ami-transcripts/IB4010.transcript.txt',\n",
              " 'ami-transcripts/ES2006d.transcript.txt',\n",
              " 'ami-transcripts/IS1006b.transcript.txt',\n",
              " 'ami-transcripts/TS3011d.transcript.txt',\n",
              " 'ami-transcripts/IS1001b.transcript.txt',\n",
              " 'ami-transcripts/IS1008c.transcript.txt',\n",
              " 'ami-transcripts/ES2014d.transcript.txt',\n",
              " 'ami-transcripts/TS3004d.transcript.txt',\n",
              " 'ami-transcripts/ES2013d.transcript.txt',\n",
              " 'ami-transcripts/TS3003d.transcript.txt',\n",
              " 'ami-transcripts/TS3007c.transcript.txt',\n",
              " 'ami-transcripts/TS3011a.transcript.txt',\n",
              " 'ami-transcripts/ES2006a.transcript.txt',\n",
              " 'ami-transcripts/ES2010c.transcript.txt',\n",
              " 'ami-transcripts/ES2005c.transcript.txt',\n",
              " 'ami-transcripts/TS3003a.transcript.txt',\n",
              " 'ami-transcripts/ES2013a.transcript.txt',\n",
              " 'ami-transcripts/ES2002c.transcript.txt',\n",
              " 'ami-transcripts/TS3004a.transcript.txt',\n",
              " 'ami-transcripts/TS3012c.transcript.txt',\n",
              " 'ami-transcripts/ES2014a.transcript.txt',\n",
              " 'ami-transcripts/TS3009b.transcript.txt',\n",
              " 'ami-transcripts/IS1009c.transcript.txt',\n",
              " 'ami-transcripts/ES2012d.transcript.txt',\n",
              " 'ami-transcripts/TS3005d.transcript.txt',\n",
              " 'ami-transcripts/ES2015d.transcript.txt',\n",
              " 'ami-transcripts/IS1000b.transcript.txt',\n",
              " 'ami-transcripts/TS3010d.transcript.txt',\n",
              " 'ami-transcripts/ES2007d.transcript.txt',\n",
              " 'ami-transcripts/IS1007b.transcript.txt',\n",
              " 'ami-transcripts/IS1000a.transcript.txt',\n",
              " 'ami-transcripts/IS1007a.transcript.txt',\n",
              " 'ami-transcripts/IB4003.transcript.txt',\n",
              " 'ami-transcripts/IS1004c.transcript.txt',\n",
              " 'ami-transcripts/IS1003c.transcript.txt',\n",
              " 'ami-transcripts/TS3008d.transcript.txt',\n",
              " 'ami-transcripts/ES2008c.transcript.txt',\n",
              " 'ami-transcripts/TS3009a.transcript.txt',\n",
              " 'ami-transcripts/ES2013b.transcript.txt',\n",
              " 'ami-transcripts/TS3003b.transcript.txt',\n",
              " 'ami-transcripts/ES2014b.transcript.txt',\n",
              " 'ami-transcripts/TS3004b.transcript.txt',\n",
              " 'ami-transcripts/TS3011b.transcript.txt',\n",
              " 'ami-transcripts/IS1001d.transcript.txt',\n",
              " 'ami-transcripts/IS1006d.transcript.txt',\n",
              " 'ami-transcripts/ES2006b.transcript.txt',\n",
              " 'ami-transcripts/IS1002c.transcript.txt',\n",
              " 'ami-transcripts/IS1005c.transcript.txt',\n",
              " 'ami-transcripts/TS3009d.transcript.txt',\n",
              " 'ami-transcripts/IS1006a.transcript.txt',\n",
              " 'ami-transcripts/IS1001a.transcript.txt',\n",
              " 'ami-transcripts/IS1007d.transcript.txt',\n",
              " 'ami-transcripts/ES2007b.transcript.txt',\n",
              " 'ami-transcripts/IS1000d.transcript.txt',\n",
              " 'ami-transcripts/TS3010b.transcript.txt',\n",
              " 'ami-transcripts/TS3008a.transcript.txt',\n",
              " 'ami-transcripts/ES2009c.transcript.txt',\n",
              " 'ami-transcripts/TS3005b.transcript.txt',\n",
              " 'ami-transcripts/ES2015b.transcript.txt',\n",
              " 'ami-transcripts/ES2012b.transcript.txt',\n",
              " 'ami-transcripts/IS1007c.transcript.txt',\n",
              " 'ami-transcripts/IS1000c.transcript.txt',\n",
              " 'ami-transcripts/IS1009b.transcript.txt',\n",
              " 'ami-transcripts/ES2009d.transcript.txt',\n",
              " 'ami-transcripts/IS1003a.transcript.txt',\n",
              " 'ami-transcripts/IS1004a.transcript.txt',\n",
              " 'ami-transcripts/TS3012b.transcript.txt',\n",
              " 'ami-transcripts/IS1002d.transcript.txt',\n",
              " 'ami-transcripts/ES2002b.transcript.txt',\n",
              " 'ami-transcripts/ES2005b.transcript.txt',\n",
              " 'ami-transcripts/TS3009c.transcript.txt',\n",
              " 'ami-transcripts/ES2008a.transcript.txt',\n",
              " 'ami-transcripts/ES2010b.transcript.txt',\n",
              " 'ami-transcripts/TS3007b.transcript.txt',\n",
              " 'ami-transcripts/IS1008b.transcript.txt',\n",
              " 'ami-transcripts/ES2008d.transcript.txt',\n",
              " 'ami-transcripts/IS1005a.transcript.txt',\n",
              " 'ami-transcripts/IB4011.transcript.txt',\n",
              " 'ami-transcripts/IS1001c.transcript.txt',\n",
              " 'ami-transcripts/IS1006c.transcript.txt',\n",
              " 'ami-transcripts/TS3006b.transcript.txt',\n",
              " 'ami-transcripts/ES2016b.transcript.txt',\n",
              " 'ami-transcripts/ES2011b.transcript.txt',\n",
              " 'ami-transcripts/IS1004d.transcript.txt',\n",
              " 'ami-transcripts/ES2004b.transcript.txt',\n",
              " 'ami-transcripts/IS1003d.transcript.txt',\n",
              " 'ami-transcripts/ES2003b.transcript.txt',\n",
              " 'ami-transcripts/ES2009a.transcript.txt',\n",
              " 'ami-transcripts/TS3008c.transcript.txt',\n",
              " 'ami-transcripts/ES2009b.transcript.txt',\n",
              " 'ami-transcripts/IS1009d.transcript.txt',\n",
              " 'ami-transcripts/ES2012c.transcript.txt',\n",
              " 'ami-transcripts/ES2004a.transcript.txt',\n",
              " 'ami-transcripts/ES2015c.transcript.txt',\n",
              " 'ami-transcripts/ES2003a.transcript.txt',\n",
              " 'ami-transcripts/TS3005c.transcript.txt',\n",
              " 'ami-transcripts/ES2016a.transcript.txt',\n",
              " 'ami-transcripts/TS3010c.transcript.txt',\n",
              " 'ami-transcripts/TS3006a.transcript.txt',\n",
              " 'ami-transcripts/ES2011a.transcript.txt',\n",
              " 'ami-transcripts/ES2007c.transcript.txt',\n",
              " 'ami-transcripts/TS3007d.transcript.txt',\n",
              " 'ami-transcripts/ES2010d.transcript.txt',\n",
              " 'ami-transcripts/ES2005d.transcript.txt',\n",
              " 'ami-transcripts/IS1005b.transcript.txt',\n",
              " 'ami-transcripts/TS3012d.transcript.txt',\n",
              " 'ami-transcripts/ES2002d.transcript.txt',\n",
              " 'ami-transcripts/IS1002b.transcript.txt',\n",
              " 'ami-transcripts/IS1008a.transcript.txt',\n",
              " 'ami-transcripts/ES2006c.transcript.txt',\n",
              " 'ami-transcripts/ES2010a.transcript.txt',\n",
              " 'ami-transcripts/TS3007a.transcript.txt',\n",
              " 'ami-transcripts/TS3011c.transcript.txt',\n",
              " 'ami-transcripts/ES2008b.transcript.txt',\n",
              " 'ami-transcripts/IS1008d.transcript.txt',\n",
              " 'ami-transcripts/TS3004c.transcript.txt',\n",
              " 'ami-transcripts/ES2002a.transcript.txt',\n",
              " 'ami-transcripts/ES2014c.transcript.txt',\n",
              " 'ami-transcripts/TS3012a.transcript.txt',\n",
              " 'ami-transcripts/TS3003c.transcript.txt',\n",
              " 'ami-transcripts/ES2005a.transcript.txt',\n",
              " 'ami-transcripts/ES2013c.transcript.txt',\n",
              " 'ami-transcripts/ES2003d.transcript.txt',\n",
              " 'ami-transcripts/IS1003b.transcript.txt',\n",
              " 'ami-transcripts/ES2004d.transcript.txt',\n",
              " 'ami-transcripts/IS1004b.transcript.txt',\n",
              " 'ami-transcripts/IS1009a.transcript.txt',\n",
              " 'ami-transcripts/ES2011d.transcript.txt',\n",
              " 'ami-transcripts/TS3006d.transcript.txt',\n",
              " 'ami-transcripts/ES2016d.transcript.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhJy2gzgfJwd",
        "outputId": "2256cb57-0e89-49f5-a10d-85ea8dda9b5f"
      },
      "source": [
        "# извлечение аудиофайлов \n",
        "'''\n",
        "import tarfile\n",
        "folder_audio = 'Array1-01.tar.gz'\n",
        "tar_audio = tarfile.open(folder_audio , \"r:gz\")\n",
        "tar_audio.extractall()\n",
        "tar_audio.close()\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport tarfile\\nfolder_audio = \\'Array1-01.tar.gz\\'\\ntar_audio = tarfile.open(folder_audio , \"r:gz\")\\ntar_audio.extractall()\\ntar_audio.close()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViPJ-dQrfJwe"
      },
      "source": [
        "Теперь у нас есть список с названиями файлов кратких саммари и соответсвующими транскрипциями и для extractive, и для abstractive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYa8xboanVX"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz7vwV5RanVX"
      },
      "source": [
        "Что включает в себя обработка тектстовых данных (preprocessing)?  \n",
        "1. Убираем знаки препинания \n",
        "2. Лемматизация/токенизация \n",
        "3. Убираем лишние символы (числа, знак новой строки, иные знаки) \n",
        "4. Делим текст на предложения \n",
        "5. Делим предложение на отдельные слова\n",
        "6. Убираем стоп-слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLn4PZRKanVX",
        "outputId": "9cfb93a7-65aa-42b0-c73b-764e6cb840a8"
      },
      "source": [
        "# libraries\n",
        "import re, numpy as np, pandas as pd\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "# Gensim\n",
        "import gensim, spacy, logging, warnings\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import lemmatize, simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "stopwords_english = stopwords.words('english')\n",
        "stopwords_english.append('um')\n",
        "stopwords_english.append('Hmm')\n",
        "stopwords_english.append('Um')\n",
        "stopwords_english.append('okay')\n",
        "stopwords_english.append('uh')\n",
        "stopwords_english.append('Yeah')\n",
        "stopwords_english.append('yeah')\n",
        "stopwords_english.append('Mm')\n",
        "stopwords_english.append('well')\n",
        "stopwords_english.append('Well')\n",
        "stopwords_english.append('hmm')\n",
        "stopwords_english.append('weve')\n",
        "stopwords_english.append('ive')\n",
        "stopwords_english.append('yep')\n",
        "stopwords_english.append('alright')\n",
        "stopwords_english.append('Alright')\n",
        "stopwords_english.append('mm')\n",
        "stopwords_english.append('kay')\n",
        "stopwords_english.append('gosh')\n",
        "stopwords_english.append('like')\n",
        "from nltk.stem import WordNetLemmatizer "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/alina/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXutK62BanVY",
        "outputId": "cd6a3191-8b50-4028-c74c-fb866913ff9e"
      },
      "source": [
        "# убираем знак новой строки \n",
        "a2 = transcript.replace(\"\\n\", \"\")\n",
        "a = a2.replace(\"\\ \", \" \")\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Um I'm Craig and I'm User Interface. Yeah. Well, my favourite animal would be a monkey. Then they're small cute and furry, and uh when planet of the apes becomes real, I'm gonna be up there with them. Yeah. I know um My parents went out and bought um remote controls because um they got fed up of having four or five different remote controls for each things the house. So um for them it was just how many devices control. Uh.Mm-hmm. Great. And I'm Andrew and I'm uh our marketing expert. Mm-hmm. Mm-hmm. Yeah, that's that's it. Yeah. I will go. That's fine. Alright. So This one here, right? Okay. Very nice. Alright. My favourite animal is like A beagle. Um charac favourite characteristics of it? Is that right? Uh, right, well basically um high priority for any animal for me is that they be willing to take a lot of physical affection from their family. And, yeah that they have lots of personality and uh be fit and in robust good health. So this is blue. Blue beagle. My family's beagle. I coulda told you a whole lot more about beagles. Boy, let me tell you. Impressionist. Alright. Mm. Superb sketch, by the way. Yep. I see a dog in there. Yep. Now I see a rooster. What kind is it? Is he aware that th it's his own cha tail he's chasing? Hmm. Probably when he was little he got lots of attention for doing it and has forever been conditioned. 'Kay. Um, can we just go over that again? Uh, so bas at twel Alright, yeah. Okay. So cost like production cost is twelve fifty, but selling price is is that wholesale or retail? Like on the shelf. Our sale our sale anyway. Yeah, okay okay. Okay. Mm-hmm. Alright. Yes. Mm-hmm. Mm-hmm. Well right away I'm wondering if there's um th th uh, like with D_V_D_ players, if there are zones. Um f frequencies or something um as well as uh characters, um different uh keypad styles and s symbols. Um. I don't know. Yeah. Yeah. Yeah. And then a and then al the other thing international is on top of the price. I'm thinking the price might might appeal to a certain market in one region, whereas in another it'll be different, so Just a chara just a characteristic of the Just Or just like, basic product podi positioning, the twenty five Euro remote control might be a big hit in London, might not be such a big hit in Greece, who knows, something like that, yeah. Yep. Right away I'm making some kind of assumptions about what what information we're given here, thinking, 'kay trendy probably means something other than just basic, something other than just standard. Um so I'm wondering right away, is selling twenty five Euros, is that sort of the thi is this gonna to be like the premium product kinda thing or Uh-huh. Mm-hmm. Yep. Yeah, I'd say so, yeah. No. Yeah, yeah. Mm-hmm. Do we have any other background information on like how that compares to other other Yeah. Mm-hmm. Yeah, interesting thing about discussing um production of a remote control for me is that l as you point out, I just don't think of remote controls as somethin something people consciously assess in their purchasing habits. It's just like getting shoelaces with shoes or something. It just comes along. Do you know what I mean? Like so sort of like how do you I I mean one one way of looking at it would be, well the people producing television sets, maybe they have to buy remote controls. Or another way is maybe people who have T_V_ sets are really fed up with their remote control and they really want a better one or something. But Right. Right. Okay so Right, so in function one of the priorities might be to combine as many uses I think so. Yeah, yeah. Yeah. Well like um, maybe what we could use is a sort of like a example of a successful other piece technology is palm palm pilots. They're gone from being just like little sort of scribble boards to cameras, M_P_ three players, telephones, everything, agenda. So, like, I wonder if we might add something new to the to the remote control market, such as the lighting in your house, or um Yeah, yeah. An Yeah. Like, p personally for me, at home I've I've combined the um the audio video of my television set and my D_V_D_ player and my C_D_ player. So they w all work actually function together but I have different remote controls for each of them. So it's sort of ironic that that then they're in there um you know, the sound and everything it's just one system. But each one's got its own little part. Mm. Mm. Mm. Mm-hmm. Mm-hmm. Yeah. Yeah. That's just really good id Yep. Uh, sure. I remember when the first remote control my my family had was on a cable. Actually had a cable between it and the T_V_ and big like buttons that sort of like, like on a blender or something. And um, you know, when I think about what they are now, it's better, but actually it's still kind of, I dunno, like a massive junky thing on the table. Maybe we could think about how, could be more, you know, streamlined. S Something like that, yeah. Or whatever would be technologically reasonable. 'Cause it could b it could it could be that f it could be that functionally that doesn't make it any better, but that just the appeal of of not having You know, these days there's a r pe things in people's homes are becoming more and more like chic, you know. Um, nicer materials and might be be worth exploring anyway. Okay. Um. Before we wrap up, just to make sure we're all on the same page here, um, do we We were given sort of an example of a coffee machine or something, right? Well, um are we at ma right now on the assumption that our television remote control may have features which go beyond the television? Or are we keeping sort of like a a design commitment to television features? I I don't know. Yep. Yeah, sure. Okay. Okay, yeah. Okay. Okay. Okay. Alright.Okay. Right. Um well this is the kick-off meeting for our our project. Um and um this is just what we're gonna be doing over the next twenty five minutes. Um so first of all, just to kind of make sure that we all know each other, I'm Laura and I'm the project manager. Do you want to introduce yourself again? Okay. Great. Okay. Um so we're designing a new remote control and um Oh I have to record who's here actually. So that's David, Andrew and Craig, isn't it? And you all arrived on time. Um yeah so des uh design a new remote control. Um, as you can see it's supposed to be original, trendy and user friendly. Um so that's kind of our our brief, as it were. Um and so there are three different stages to the design. Um I'm not really sure what what you guys have already received um in your emails. What did you get? Mm-hmm. Is that what everybody got? Okay. Um. So we're gonna have like individual work and then a meeting about it. And repeat that process three times. Um and at this point we get try out the whiteboard over there. Um. So uh you get to draw your favourite animal and sum up your favourite characteristics of it. So who would like to go first? Very good. Mm-hmm. Yeah. Yeah. Right. Lovely. Right. You can take as long over this as you like, because we haven't got an awful lot to discuss. Ok oh we do we do. Don't feel like you're in a rush, anyway. Ach why not We might have to get you up again then. I don't know what mine is. I'm gonna have to think on the spot now. Is that a whale? Ah. Okay. God, I still don't know what I'm gonna write about. Um. I was gonna choose a dog as well. But I'll just draw a different kind of dog. M my favourite animal is my own dog at home. Um That doesn't really look like him, actually. He looks more like a pig, actually. Ah well. Do you? Oh that's very good of you. Uh. Um he's a mixture of uh various things. Um and what do I like about him, um That's just to suggest that his tail wags. Um he's very friendly and cheery and always pleased to see you, and very kind of affectionate and um uh and he's quite quite wee as well so you know he can doesn't take up too much space. Um and uh And he does a funny thing where he chases his tail as well, which is quite amusing, so It is. I think it is. He only does it after he's had his dinner and um he'll just all of a sudden just get up and start chasing his tail 'round the living room. Yeah, so uh Yeah, maybe. Maybe. Right, um where did you find this? Just down here? Yeah. Okay. Um what are we doing next? Uh um. Okay, uh we now need to discuss the project finance. Um so according to the brief um we're gonna be selling this remote control for twenty five Euro, um and we're aiming to make fifty million Euro. Um so we're gonna be selling this on an international scale. And uh we don't want it to cost any more than uh twelve fifty Euros, so fifty percent of the selling price. Sure. All together. Um I dunno. I imagine That's a good question. I imagine it probably is our sale actually because it's probably up to the the um the retailer to uh sell it for whatever price they want. Um. But I I don't know, I mean do you think the fact that it's going to be sold internationally will have a bearing on how we design it at all? Think it will? Um. Hmm. Oh yeah, regions and stuff, yeah. Yeah. Okay. Yeah. Well for a remote control, do you think that will be I suppose it's depends on how complicated our remote control is. Yeah, yeah. Okay. What, just like in terms of like the wealth of the country? Like how much money people have to spend on things like? Aye, I see what you mean, yeah. Marketing. Good marketing thoughts. Oh gosh, I should be writing all this down. Um. Mm. Yeah. Yeah, yeah. Like how much does, you know, a remote control cost. Well twenty five Euro, I mean that's um that's about like eighteen pounds or something, isn't it? Or no, is it as much as that? Sixteen seventeen eighteen pounds. Um, I dunno, I've never bought a remote control, so I don't know how how good a remote control that would get you. Um. But yeah, I suppose it has to look kind of cool and gimmicky. Um right, okay. Let me just scoot on ahead here. Okay. Um well d Does anybody have anything to add to uh to the finance issue at all? Thin No, actually. That would be useful, though, wouldn't it, if you knew like what your money would get you now. Mm-hmm. Yeah, yeah. Oh. Five minutes to end of meeting. Oh, okay. We're a bit behind. Yeah. Right, so do you think that should be like a main design aim of our remote control d you know, do your your satellite and your regular telly and your V_C_R_ and everything? Mm-hmm. Yeah. Or even like, you know, notes about um what you wanna watch. Like you might put in there oh I want to watch such and such and look a Oh that's a good idea. So extra functionalities. Mm-hmm. Hmm. Um okay, uh I'd wel we're gonna have to wrap up pretty quickly in the next couple of minutes. Um I'll just check we've nothing else. Okay. Um so anything else anybody wants to add about what they don't like about remote controls they've used, what they would really like to be part of this new one at all? You keep losing them. Okay. Yeah. W You get those ones where you can, if you like, whistle or make a really high pitched noise they beep. There I mean is that something we'd want to include, do you think? Dunno. Okay maybe. My goodness. Still feels quite primitive. Maybe like a touch screen or something? Okay. Uh-huh, okay. Well I guess that's up to our industrial designer. It looks better. Yeah. Okay. Okay. Right, well um so just to wrap up, the next meeting's gonna be in thirty minutes. So that's about um about ten to twelve by my watch. Um so inbetween now and then, um as the industrial designer, you're gonna be working on you know the actual working design of it so y you know what you're doing there. Um for user interface, technical functions, I guess that's you know like what we've been talking about, what it'll actually do. Um and uh marketing executive, you'll be just thinking about what it actually what, you know, what requirements it has to has to fulfil and you'll all get instructions emailed to you, I guess. Um. Yeah, so it's th the functional design stage is next, I guess. And uh and that's the end of the meeting. So I got that little message a lot sooner than I thought I would, so Mm-hmm. Uh-huh, yeah. Th Okay, well just very quickly 'cause this we're supposed to finish now. Um I guess that's up to us, I mean you probably want some kind of unique selling point of it, so um, you know Yeah. Mm-hmm. Yeah. Okay. Right, okay, we'll that's that's the end of the meeting, then. Um. So, uh thank you all for coming.Hi, I'm David and I'm supposed to be an industrial designer. Um, I just got the project announcement about what the project is. Designing a remote control. That's about it, didn't get anything else. Did you get the same thing? Cool. There's too much gear. Okay. Can't draw. Um. Yeah. Um, well anyway, I don't know, it's just the first animal I can think off the top of my head. Um. Yes. Big reason is 'cause I'm allergic to most animals. Allergic to animal fur, so um fish was a natural choice. Um, yeah, and I kind of like whales. They come in and go eat everything in sight. And they're quite harmless and mild and interesting. Tail's a bit big, I think. It's an after dinner dog then. Hmm. It does make sense from maybe the design point of view 'cause you have more complicated characters like European languages, then you need more buttons. So, possibly. Hmm. Yeah. And you keep losing them. Finding them is really a pain, you know. I mean it's usually quite small, or when you want it right, it slipped behind the couch or it's kicked under the table. You know. Yep. Mm-hmm. I think one factor would be production cost. Because there's a cap there, so um depends on how much you can cram into that price. Um. I think that that's the main factor. Cool.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fer3xj7EanVY"
      },
      "source": [
        "# убираем знаки препинания из текста \n",
        "text =[]\n",
        "split_regex = re.compile(r'[.|!|?|…]')\n",
        "sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(a)])\n",
        "for s in sentences:\n",
        "    text.append(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV8B7eLxanVZ",
        "outputId": "7ef102c5-d5ce-4232-a454-9ed98e515bfd"
      },
      "source": [
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Um I'm Craig and I'm User Interface\",\n",
              " 'Yeah',\n",
              " 'Well, my favourite animal would be a monkey',\n",
              " \"Then they're small cute and furry, and uh when planet of the apes becomes real, I'm gonna be up there with them\",\n",
              " 'Yeah',\n",
              " 'I know um My parents went out and bought um remote controls because um they got fed up of having four or five different remote controls for each things the house',\n",
              " 'So um for them it was just how many devices control',\n",
              " 'Uh',\n",
              " 'Mm-hmm',\n",
              " 'Great',\n",
              " \"And I'm Andrew and I'm uh our marketing expert\",\n",
              " 'Mm-hmm',\n",
              " 'Mm-hmm',\n",
              " \"Yeah, that's that's it\",\n",
              " 'Yeah',\n",
              " 'I will go',\n",
              " \"That's fine\",\n",
              " 'Alright',\n",
              " 'So This one here, right',\n",
              " 'Okay',\n",
              " 'Very nice',\n",
              " 'Alright',\n",
              " 'My favourite animal is like A beagle',\n",
              " 'Um charac favourite characteristics of it',\n",
              " 'Is that right',\n",
              " 'Uh, right, well basically um high priority for any animal for me is that they be willing to take a lot of physical affection from their family',\n",
              " 'And, yeah that they have lots of personality and uh be fit and in robust good health',\n",
              " 'So this is blue',\n",
              " 'Blue beagle',\n",
              " \"My family's beagle\",\n",
              " 'I coulda told you a whole lot more about beagles',\n",
              " 'Boy, let me tell you',\n",
              " 'Impressionist',\n",
              " 'Alright',\n",
              " 'Mm',\n",
              " 'Superb sketch, by the way',\n",
              " 'Yep',\n",
              " 'I see a dog in there',\n",
              " 'Yep',\n",
              " 'Now I see a rooster',\n",
              " 'What kind is it',\n",
              " \"Is he aware that th it's his own cha tail he's chasing\",\n",
              " 'Hmm',\n",
              " 'Probably when he was little he got lots of attention for doing it and has forever been conditioned',\n",
              " \"'Kay\",\n",
              " 'Um, can we just go over that again',\n",
              " 'Uh, so bas at twel Alright, yeah',\n",
              " 'Okay',\n",
              " 'So cost like production cost is twelve fifty, but selling price is is that wholesale or retail',\n",
              " 'Like on the shelf',\n",
              " 'Our sale our sale anyway',\n",
              " 'Yeah, okay okay',\n",
              " 'Okay',\n",
              " 'Mm-hmm',\n",
              " 'Alright',\n",
              " 'Yes',\n",
              " 'Mm-hmm',\n",
              " 'Mm-hmm',\n",
              " \"Well right away I'm wondering if there's um th th uh, like with D_V_D_ players, if there are zones\",\n",
              " 'Um f frequencies or something um as well as uh characters, um different uh keypad styles and s symbols',\n",
              " 'Um',\n",
              " \"I don't know\",\n",
              " 'Yeah',\n",
              " 'Yeah',\n",
              " 'Yeah',\n",
              " 'And then a and then al the other thing international is on top of the price',\n",
              " \"I'm thinking the price might might appeal to a certain market in one region, whereas in another it'll be different, so Just a chara just a characteristic of the Just Or just like, basic product podi positioning, the twenty five Euro remote control might be a big hit in London, might not be such a big hit in Greece, who knows, something like that, yeah\",\n",
              " 'Yep',\n",
              " \"Right away I'm making some kind of assumptions about what what information we're given here, thinking, 'kay trendy probably means something other than just basic, something other than just standard\",\n",
              " \"Um so I'm wondering right away, is selling twenty five Euros, is that sort of the thi is this gonna to be like the premium product kinda thing or Uh-huh\",\n",
              " 'Mm-hmm',\n",
              " 'Yep',\n",
              " \"Yeah, I'd say so, yeah\",\n",
              " 'No',\n",
              " 'Yeah, yeah',\n",
              " 'Mm-hmm',\n",
              " 'Do we have any other background information on like how that compares to other other Yeah',\n",
              " 'Mm-hmm',\n",
              " \"Yeah, interesting thing about discussing um production of a remote control for me is that l as you point out, I just don't think of remote controls as somethin something people consciously assess in their purchasing habits\",\n",
              " \"It's just like getting shoelaces with shoes or something\",\n",
              " 'It just comes along',\n",
              " 'Do you know what I mean',\n",
              " 'Like so sort of like how do you I I mean one one way of looking at it would be, well the people producing television sets, maybe they have to buy remote controls',\n",
              " 'Or another way is maybe people who have T_V_ sets are really fed up with their remote control and they really want a better one or something',\n",
              " 'But Right',\n",
              " 'Right',\n",
              " 'Okay so Right, so in function one of the priorities might be to combine as many uses I think so',\n",
              " 'Yeah, yeah',\n",
              " 'Yeah',\n",
              " 'Well like um, maybe what we could use is a sort of like a example of a successful other piece technology is palm palm pilots',\n",
              " \"They're gone from being just like little sort of scribble boards to cameras, M_P_ three players, telephones, everything, agenda\",\n",
              " 'So, like, I wonder if we might add something new to the to the remote control market, such as the lighting in your house, or um Yeah, yeah',\n",
              " 'An Yeah',\n",
              " \"Like, p personally for me, at home I've I've combined the um the audio video of my television set and my D_V_D_ player and my C_D_ player\",\n",
              " 'So they w all work actually function together but I have different remote controls for each of them',\n",
              " \"So it's sort of ironic that that then they're in there um you know, the sound and everything it's just one system\",\n",
              " \"But each one's got its own little part\",\n",
              " 'Mm',\n",
              " 'Mm',\n",
              " 'Mm',\n",
              " 'Mm-hmm',\n",
              " 'Mm-hmm',\n",
              " 'Yeah',\n",
              " 'Yeah',\n",
              " \"That's just really good id Yep\",\n",
              " 'Uh, sure',\n",
              " 'I remember when the first remote control my my family had was on a cable',\n",
              " 'Actually had a cable between it and the T_V_ and big like buttons that sort of like, like on a blender or something',\n",
              " \"And um, you know, when I think about what they are now, it's better, but actually it's still kind of, I dunno, like a massive junky thing on the table\",\n",
              " 'Maybe we could think about how, could be more, you know, streamlined',\n",
              " 'S Something like that, yeah',\n",
              " 'Or whatever would be technologically reasonable',\n",
              " \"'Cause it could b it could it could be that f it could be that functionally that doesn't make it any better, but that just the appeal of of not having You know, these days there's a r pe things in people's homes are becoming more and more like chic, you know\",\n",
              " 'Um, nicer materials and might be be worth exploring anyway',\n",
              " 'Okay',\n",
              " 'Um',\n",
              " \"Before we wrap up, just to make sure we're all on the same page here, um, do we We were given sort of an example of a coffee machine or something, right\",\n",
              " 'Well, um are we at ma right now on the assumption that our television remote control may have features which go beyond the television',\n",
              " 'Or are we keeping sort of like a a design commitment to television features',\n",
              " \"I I don't know\",\n",
              " 'Yep',\n",
              " 'Yeah, sure',\n",
              " 'Okay',\n",
              " 'Okay, yeah',\n",
              " 'Okay',\n",
              " 'Okay',\n",
              " 'Okay',\n",
              " 'Alright',\n",
              " 'Okay',\n",
              " 'Right',\n",
              " 'Um well this is the kick-off meeting for our our project',\n",
              " \"Um and um this is just what we're gonna be doing over the next twenty five minutes\",\n",
              " \"Um so first of all, just to kind of make sure that we all know each other, I'm Laura and I'm the project manager\",\n",
              " 'Do you want to introduce yourself again',\n",
              " 'Okay',\n",
              " 'Great',\n",
              " 'Okay',\n",
              " \"Um so we're designing a new remote control and um Oh I have to record who's here actually\",\n",
              " \"So that's David, Andrew and Craig, isn't it\",\n",
              " 'And you all arrived on time',\n",
              " 'Um yeah so des uh design a new remote control',\n",
              " \"Um, as you can see it's supposed to be original, trendy and user friendly\",\n",
              " \"Um so that's kind of our our brief, as it were\",\n",
              " 'Um and so there are three different stages to the design',\n",
              " \"Um I'm not really sure what what you guys have already received um in your emails\",\n",
              " 'What did you get',\n",
              " 'Mm-hmm',\n",
              " 'Is that what everybody got',\n",
              " 'Okay',\n",
              " 'Um',\n",
              " \"So we're gonna have like individual work and then a meeting about it\",\n",
              " 'And repeat that process three times',\n",
              " 'Um and at this point we get try out the whiteboard over there',\n",
              " 'Um',\n",
              " 'So uh you get to draw your favourite animal and sum up your favourite characteristics of it',\n",
              " 'So who would like to go first',\n",
              " 'Very good',\n",
              " 'Mm-hmm',\n",
              " 'Yeah',\n",
              " 'Yeah',\n",
              " 'Right',\n",
              " 'Lovely',\n",
              " 'Right',\n",
              " \"You can take as long over this as you like, because we haven't got an awful lot to discuss\",\n",
              " 'Ok oh we do we do',\n",
              " \"Don't feel like you're in a rush, anyway\",\n",
              " 'Ach why not We might have to get you up again then',\n",
              " \"I don't know what mine is\",\n",
              " \"I'm gonna have to think on the spot now\",\n",
              " 'Is that a whale',\n",
              " 'Ah',\n",
              " 'Okay',\n",
              " \"God, I still don't know what I'm gonna write about\",\n",
              " 'Um',\n",
              " 'I was gonna choose a dog as well',\n",
              " \"But I'll just draw a different kind of dog\",\n",
              " 'M my favourite animal is my own dog at home',\n",
              " \"Um That doesn't really look like him, actually\",\n",
              " 'He looks more like a pig, actually',\n",
              " 'Ah well',\n",
              " 'Do you',\n",
              " \"Oh that's very good of you\",\n",
              " 'Uh',\n",
              " \"Um he's a mixture of uh various things\",\n",
              " \"Um and what do I like about him, um That's just to suggest that his tail wags\",\n",
              " \"Um he's very friendly and cheery and always pleased to see you, and very kind of affectionate and um uh and he's quite quite wee as well so you know he can doesn't take up too much space\",\n",
              " 'Um and uh And he does a funny thing where he chases his tail as well, which is quite amusing, so It is',\n",
              " 'I think it is',\n",
              " \"He only does it after he's had his dinner and um he'll just all of a sudden just get up and start chasing his tail 'round the living room\",\n",
              " 'Yeah, so uh Yeah, maybe',\n",
              " 'Maybe',\n",
              " 'Right, um where did you find this',\n",
              " 'Just down here',\n",
              " 'Yeah',\n",
              " 'Okay',\n",
              " 'Um what are we doing next',\n",
              " 'Uh um',\n",
              " 'Okay, uh we now need to discuss the project finance',\n",
              " \"Um so according to the brief um we're gonna be selling this remote control for twenty five Euro, um and we're aiming to make fifty million Euro\",\n",
              " \"Um so we're gonna be selling this on an international scale\",\n",
              " \"And uh we don't want it to cost any more than uh twelve fifty Euros, so fifty percent of the selling price\",\n",
              " 'Sure',\n",
              " 'All together',\n",
              " 'Um I dunno',\n",
              " \"I imagine That's a good question\",\n",
              " \"I imagine it probably is our sale actually because it's probably up to the the um the retailer to uh sell it for whatever price they want\",\n",
              " 'Um',\n",
              " \"But I I don't know, I mean do you think the fact that it's going to be sold internationally will have a bearing on how we design it at all\",\n",
              " 'Think it will',\n",
              " 'Um',\n",
              " 'Hmm',\n",
              " 'Oh yeah, regions and stuff, yeah',\n",
              " 'Yeah',\n",
              " 'Okay',\n",
              " 'Yeah',\n",
              " \"Well for a remote control, do you think that will be I suppose it's depends on how complicated our remote control is\",\n",
              " 'Yeah, yeah',\n",
              " 'Okay',\n",
              " 'What, just like in terms of like the wealth of the country',\n",
              " 'Like how much money people have to spend on things like',\n",
              " 'Aye, I see what you mean, yeah',\n",
              " 'Marketing',\n",
              " 'Good marketing thoughts',\n",
              " 'Oh gosh, I should be writing all this down',\n",
              " 'Um',\n",
              " 'Mm',\n",
              " 'Yeah',\n",
              " 'Yeah, yeah',\n",
              " 'Like how much does, you know, a remote control cost',\n",
              " \"Well twenty five Euro, I mean that's um that's about like eighteen pounds or something, isn't it\",\n",
              " 'Or no, is it as much as that',\n",
              " 'Sixteen seventeen eighteen pounds',\n",
              " \"Um, I dunno, I've never bought a remote control, so I don't know how how good a remote control that would get you\",\n",
              " 'Um',\n",
              " 'But yeah, I suppose it has to look kind of cool and gimmicky',\n",
              " 'Um right, okay',\n",
              " 'Let me just scoot on ahead here',\n",
              " 'Okay',\n",
              " 'Um well d Does anybody have anything to add to uh to the finance issue at all',\n",
              " 'Thin No, actually',\n",
              " \"That would be useful, though, wouldn't it, if you knew like what your money would get you now\",\n",
              " 'Mm-hmm',\n",
              " 'Yeah, yeah',\n",
              " 'Oh',\n",
              " 'Five minutes to end of meeting',\n",
              " 'Oh, okay',\n",
              " \"We're a bit behind\",\n",
              " 'Yeah',\n",
              " 'Right, so do you think that should be like a main design aim of our remote control d you know, do your your satellite and your regular telly and your V_C_R_ and everything',\n",
              " 'Mm-hmm',\n",
              " 'Yeah',\n",
              " 'Or even like, you know, notes about um what you wanna watch',\n",
              " \"Like you might put in there oh I want to watch such and such and look a Oh that's a good idea\",\n",
              " 'So extra functionalities',\n",
              " 'Mm-hmm',\n",
              " 'Hmm',\n",
              " \"Um okay, uh I'd wel we're gonna have to wrap up pretty quickly in the next couple of minutes\",\n",
              " \"Um I'll just check we've nothing else\",\n",
              " 'Okay',\n",
              " \"Um so anything else anybody wants to add about what they don't like about remote controls they've used, what they would really like to be part of this new one at all\",\n",
              " 'You keep losing them',\n",
              " 'Okay',\n",
              " 'Yeah',\n",
              " 'W You get those ones where you can, if you like, whistle or make a really high pitched noise they beep',\n",
              " \"There I mean is that something we'd want to include, do you think\",\n",
              " 'Dunno',\n",
              " 'Okay maybe',\n",
              " 'My goodness',\n",
              " 'Still feels quite primitive',\n",
              " 'Maybe like a touch screen or something',\n",
              " 'Okay',\n",
              " 'Uh-huh, okay',\n",
              " \"Well I guess that's up to our industrial designer\",\n",
              " 'It looks better',\n",
              " 'Yeah',\n",
              " 'Okay',\n",
              " 'Okay',\n",
              " \"Right, well um so just to wrap up, the next meeting's gonna be in thirty minutes\",\n",
              " \"So that's about um about ten to twelve by my watch\",\n",
              " \"Um so inbetween now and then, um as the industrial designer, you're gonna be working on you know the actual working design of it so y you know what you're doing there\",\n",
              " \"Um for user interface, technical functions, I guess that's you know like what we've been talking about, what it'll actually do\",\n",
              " \"Um and uh marketing executive, you'll be just thinking about what it actually what, you know, what requirements it has to has to fulfil and you'll all get instructions emailed to you, I guess\",\n",
              " 'Um',\n",
              " \"Yeah, so it's th the functional design stage is next, I guess\",\n",
              " \"And uh and that's the end of the meeting\",\n",
              " 'So I got that little message a lot sooner than I thought I would, so Mm-hmm',\n",
              " 'Uh-huh, yeah',\n",
              " \"Th Okay, well just very quickly 'cause this we're supposed to finish now\",\n",
              " \"Um I guess that's up to us, I mean you probably want some kind of unique selling point of it, so um, you know Yeah\",\n",
              " 'Mm-hmm',\n",
              " 'Yeah',\n",
              " 'Okay',\n",
              " \"Right, okay, we'll that's that's the end of the meeting, then\",\n",
              " 'Um',\n",
              " 'So, uh thank you all for coming',\n",
              " \"Hi, I'm David and I'm supposed to be an industrial designer\",\n",
              " 'Um, I just got the project announcement about what the project is',\n",
              " 'Designing a remote control',\n",
              " \"That's about it, didn't get anything else\",\n",
              " 'Did you get the same thing',\n",
              " 'Cool',\n",
              " \"There's too much gear\",\n",
              " 'Okay',\n",
              " \"Can't draw\",\n",
              " 'Um',\n",
              " 'Yeah',\n",
              " \"Um, well anyway, I don't know, it's just the first animal I can think off the top of my head\",\n",
              " 'Um',\n",
              " 'Yes',\n",
              " \"Big reason is 'cause I'm allergic to most animals\",\n",
              " 'Allergic to animal fur, so um fish was a natural choice',\n",
              " 'Um, yeah, and I kind of like whales',\n",
              " 'They come in and go eat everything in sight',\n",
              " \"And they're quite harmless and mild and interesting\",\n",
              " \"Tail's a bit big, I think\",\n",
              " \"It's an after dinner dog then\",\n",
              " 'Hmm',\n",
              " \"It does make sense from maybe the design point of view 'cause you have more complicated characters like European languages, then you need more buttons\",\n",
              " 'So, possibly',\n",
              " 'Hmm',\n",
              " 'Yeah',\n",
              " 'And you keep losing them',\n",
              " 'Finding them is really a pain, you know',\n",
              " \"I mean it's usually quite small, or when you want it right, it slipped behind the couch or it's kicked under the table\",\n",
              " 'You know',\n",
              " 'Yep',\n",
              " 'Mm-hmm',\n",
              " 'I think one factor would be production cost',\n",
              " \"Because there's a cap there, so um depends on how much you can cram into that price\",\n",
              " 'Um',\n",
              " \"I think that that's the main factor\",\n",
              " 'Cool']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ARByqRHanVZ"
      },
      "source": [
        "# убираем ненужные символы и делим предложение на слова\n",
        "def sent_to_words(sentences):\n",
        "    for sent in sentences:\n",
        "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
        "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
        "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
        "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
        "        yield(sent) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxH5y-y_anVa",
        "outputId": "ab1192f5-f592-4fc4-c2dc-45be2c6b46ea"
      },
      "source": [
        "res = list(sent_to_words(text))\n",
        "res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['um', 'im', 'craig', 'and', 'im', 'user', 'interface'],\n",
              " ['yeah'],\n",
              " ['well', 'my', 'favourite', 'animal', 'would', 'be', 'monkey'],\n",
              " ['then',\n",
              "  'theyre',\n",
              "  'small',\n",
              "  'cute',\n",
              "  'and',\n",
              "  'furry',\n",
              "  'and',\n",
              "  'uh',\n",
              "  'when',\n",
              "  'planet',\n",
              "  'of',\n",
              "  'the',\n",
              "  'apes',\n",
              "  'becomes',\n",
              "  'real',\n",
              "  'im',\n",
              "  'gonna',\n",
              "  'be',\n",
              "  'up',\n",
              "  'there',\n",
              "  'with',\n",
              "  'them'],\n",
              " ['yeah'],\n",
              " ['know',\n",
              "  'um',\n",
              "  'my',\n",
              "  'parents',\n",
              "  'went',\n",
              "  'out',\n",
              "  'and',\n",
              "  'bought',\n",
              "  'um',\n",
              "  'remote',\n",
              "  'controls',\n",
              "  'because',\n",
              "  'um',\n",
              "  'they',\n",
              "  'got',\n",
              "  'fed',\n",
              "  'up',\n",
              "  'of',\n",
              "  'having',\n",
              "  'four',\n",
              "  'or',\n",
              "  'five',\n",
              "  'different',\n",
              "  'remote',\n",
              "  'controls',\n",
              "  'for',\n",
              "  'each',\n",
              "  'things',\n",
              "  'the',\n",
              "  'house'],\n",
              " ['so',\n",
              "  'um',\n",
              "  'for',\n",
              "  'them',\n",
              "  'it',\n",
              "  'was',\n",
              "  'just',\n",
              "  'how',\n",
              "  'many',\n",
              "  'devices',\n",
              "  'control'],\n",
              " ['uh'],\n",
              " ['mm', 'hmm'],\n",
              " ['great'],\n",
              " ['and', 'im', 'andrew', 'and', 'im', 'uh', 'our', 'marketing', 'expert'],\n",
              " ['mm', 'hmm'],\n",
              " ['mm', 'hmm'],\n",
              " ['yeah', 'thats', 'thats', 'it'],\n",
              " ['yeah'],\n",
              " ['will', 'go'],\n",
              " ['thats', 'fine'],\n",
              " ['alright'],\n",
              " ['so', 'this', 'one', 'here', 'right'],\n",
              " ['okay'],\n",
              " ['very', 'nice'],\n",
              " ['alright'],\n",
              " ['my', 'favourite', 'animal', 'is', 'like', 'beagle'],\n",
              " ['um', 'charac', 'favourite', 'characteristics', 'of', 'it'],\n",
              " ['is', 'that', 'right'],\n",
              " ['uh',\n",
              "  'right',\n",
              "  'well',\n",
              "  'basically',\n",
              "  'um',\n",
              "  'high',\n",
              "  'priority',\n",
              "  'for',\n",
              "  'any',\n",
              "  'animal',\n",
              "  'for',\n",
              "  'me',\n",
              "  'is',\n",
              "  'that',\n",
              "  'they',\n",
              "  'be',\n",
              "  'willing',\n",
              "  'to',\n",
              "  'take',\n",
              "  'lot',\n",
              "  'of',\n",
              "  'physical',\n",
              "  'affection',\n",
              "  'from',\n",
              "  'their',\n",
              "  'family'],\n",
              " ['and',\n",
              "  'yeah',\n",
              "  'that',\n",
              "  'they',\n",
              "  'have',\n",
              "  'lots',\n",
              "  'of',\n",
              "  'personality',\n",
              "  'and',\n",
              "  'uh',\n",
              "  'be',\n",
              "  'fit',\n",
              "  'and',\n",
              "  'in',\n",
              "  'robust',\n",
              "  'good',\n",
              "  'health'],\n",
              " ['so', 'this', 'is', 'blue'],\n",
              " ['blue', 'beagle'],\n",
              " ['my', 'familys', 'beagle'],\n",
              " ['coulda', 'told', 'you', 'whole', 'lot', 'more', 'about', 'beagles'],\n",
              " ['boy', 'let', 'me', 'tell', 'you'],\n",
              " ['impressionist'],\n",
              " ['alright'],\n",
              " ['mm'],\n",
              " ['superb', 'sketch', 'by', 'the', 'way'],\n",
              " ['yep'],\n",
              " ['see', 'dog', 'in', 'there'],\n",
              " ['yep'],\n",
              " ['now', 'see', 'rooster'],\n",
              " ['what', 'kind', 'is', 'it'],\n",
              " ['is',\n",
              "  'he',\n",
              "  'aware',\n",
              "  'that',\n",
              "  'th',\n",
              "  'its',\n",
              "  'his',\n",
              "  'own',\n",
              "  'cha',\n",
              "  'tail',\n",
              "  'hes',\n",
              "  'chasing'],\n",
              " ['hmm'],\n",
              " ['probably',\n",
              "  'when',\n",
              "  'he',\n",
              "  'was',\n",
              "  'little',\n",
              "  'he',\n",
              "  'got',\n",
              "  'lots',\n",
              "  'of',\n",
              "  'attention',\n",
              "  'for',\n",
              "  'doing',\n",
              "  'it',\n",
              "  'and',\n",
              "  'has',\n",
              "  'forever',\n",
              "  'been',\n",
              "  'conditioned'],\n",
              " ['kay'],\n",
              " ['um', 'can', 'we', 'just', 'go', 'over', 'that', 'again'],\n",
              " ['uh', 'so', 'bas', 'at', 'twel', 'alright', 'yeah'],\n",
              " ['okay'],\n",
              " ['so',\n",
              "  'cost',\n",
              "  'like',\n",
              "  'production',\n",
              "  'cost',\n",
              "  'is',\n",
              "  'twelve',\n",
              "  'fifty',\n",
              "  'but',\n",
              "  'selling',\n",
              "  'price',\n",
              "  'is',\n",
              "  'is',\n",
              "  'that',\n",
              "  'wholesale',\n",
              "  'or',\n",
              "  'retail'],\n",
              " ['like', 'on', 'the', 'shelf'],\n",
              " ['our', 'sale', 'our', 'sale', 'anyway'],\n",
              " ['yeah', 'okay', 'okay'],\n",
              " ['okay'],\n",
              " ['mm', 'hmm'],\n",
              " ['alright'],\n",
              " ['yes'],\n",
              " ['mm', 'hmm'],\n",
              " ['mm', 'hmm'],\n",
              " ['well',\n",
              "  'right',\n",
              "  'away',\n",
              "  'im',\n",
              "  'wondering',\n",
              "  'if',\n",
              "  'theres',\n",
              "  'um',\n",
              "  'th',\n",
              "  'th',\n",
              "  'uh',\n",
              "  'like',\n",
              "  'with',\n",
              "  'd_v_d_',\n",
              "  'players',\n",
              "  'if',\n",
              "  'there',\n",
              "  'are',\n",
              "  'zones'],\n",
              " ['um',\n",
              "  'frequencies',\n",
              "  'or',\n",
              "  'something',\n",
              "  'um',\n",
              "  'as',\n",
              "  'well',\n",
              "  'as',\n",
              "  'uh',\n",
              "  'characters',\n",
              "  'um',\n",
              "  'different',\n",
              "  'uh',\n",
              "  'keypad',\n",
              "  'styles',\n",
              "  'and',\n",
              "  'symbols'],\n",
              " ['um'],\n",
              " ['dont', 'know'],\n",
              " ['yeah'],\n",
              " ['yeah'],\n",
              " ['yeah'],\n",
              " ['and',\n",
              "  'then',\n",
              "  'and',\n",
              "  'then',\n",
              "  'al',\n",
              "  'the',\n",
              "  'other',\n",
              "  'thing',\n",
              "  'international',\n",
              "  'is',\n",
              "  'on',\n",
              "  'top',\n",
              "  'of',\n",
              "  'the',\n",
              "  'price'],\n",
              " ['im',\n",
              "  'thinking',\n",
              "  'the',\n",
              "  'price',\n",
              "  'might',\n",
              "  'might',\n",
              "  'appeal',\n",
              "  'to',\n",
              "  'certain',\n",
              "  'market',\n",
              "  'in',\n",
              "  'one',\n",
              "  'region',\n",
              "  'whereas',\n",
              "  'in',\n",
              "  'another',\n",
              "  'itll',\n",
              "  'be',\n",
              "  'different',\n",
              "  'so',\n",
              "  'just',\n",
              "  'chara',\n",
              "  'just',\n",
              "  'characteristic',\n",
              "  'of',\n",
              "  'the',\n",
              "  'just',\n",
              "  'or',\n",
              "  'just',\n",
              "  'like',\n",
              "  'basic',\n",
              "  'product',\n",
              "  'podi',\n",
              "  'positioning',\n",
              "  'the',\n",
              "  'twenty',\n",
              "  'five',\n",
              "  'euro',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'might',\n",
              "  'be',\n",
              "  'big',\n",
              "  'hit',\n",
              "  'in',\n",
              "  'london',\n",
              "  'might',\n",
              "  'not',\n",
              "  'be',\n",
              "  'such',\n",
              "  'big',\n",
              "  'hit',\n",
              "  'in',\n",
              "  'greece',\n",
              "  'who',\n",
              "  'knows',\n",
              "  'something',\n",
              "  'like',\n",
              "  'that',\n",
              "  'yeah'],\n",
              " ['yep'],\n",
              " ['right',\n",
              "  'away',\n",
              "  'im',\n",
              "  'making',\n",
              "  'some',\n",
              "  'kind',\n",
              "  'of',\n",
              "  'assumptions',\n",
              "  'about',\n",
              "  'what',\n",
              "  'what',\n",
              "  'information',\n",
              "  'were',\n",
              "  'given',\n",
              "  'here',\n",
              "  'thinking',\n",
              "  'kay',\n",
              "  'trendy',\n",
              "  'probably',\n",
              "  'means',\n",
              "  'something',\n",
              "  'other',\n",
              "  'than',\n",
              "  'just',\n",
              "  'basic',\n",
              "  'something',\n",
              "  'other',\n",
              "  'than',\n",
              "  'just',\n",
              "  'standard'],\n",
              " ['um',\n",
              "  'so',\n",
              "  'im',\n",
              "  'wondering',\n",
              "  'right',\n",
              "  'away',\n",
              "  'is',\n",
              "  'selling',\n",
              "  'twenty',\n",
              "  'five',\n",
              "  'euros',\n",
              "  'is',\n",
              "  'that',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'the',\n",
              "  'thi',\n",
              "  'is',\n",
              "  'this',\n",
              "  'gonna',\n",
              "  'to',\n",
              "  'be',\n",
              "  'like',\n",
              "  'the',\n",
              "  'premium',\n",
              "  'product',\n",
              "  'kinda',\n",
              "  'thing',\n",
              "  'or',\n",
              "  'uh',\n",
              "  'huh'],\n",
              " ['mm', 'hmm'],\n",
              " ['yep'],\n",
              " ['yeah', 'id', 'say', 'so', 'yeah'],\n",
              " ['no'],\n",
              " ['yeah', 'yeah'],\n",
              " ['mm', 'hmm'],\n",
              " ['do',\n",
              "  'we',\n",
              "  'have',\n",
              "  'any',\n",
              "  'other',\n",
              "  'background',\n",
              "  'information',\n",
              "  'on',\n",
              "  'like',\n",
              "  'how',\n",
              "  'that',\n",
              "  'compares',\n",
              "  'to',\n",
              "  'other',\n",
              "  'other',\n",
              "  'yeah'],\n",
              " ['mm', 'hmm'],\n",
              " ['yeah',\n",
              "  'interesting',\n",
              "  'thing',\n",
              "  'about',\n",
              "  'discussing',\n",
              "  'um',\n",
              "  'production',\n",
              "  'of',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'for',\n",
              "  'me',\n",
              "  'is',\n",
              "  'that',\n",
              "  'as',\n",
              "  'you',\n",
              "  'point',\n",
              "  'out',\n",
              "  'just',\n",
              "  'dont',\n",
              "  'think',\n",
              "  'of',\n",
              "  'remote',\n",
              "  'controls',\n",
              "  'as',\n",
              "  'somethin',\n",
              "  'something',\n",
              "  'people',\n",
              "  'consciously',\n",
              "  'assess',\n",
              "  'in',\n",
              "  'their',\n",
              "  'purchasing',\n",
              "  'habits'],\n",
              " ['its',\n",
              "  'just',\n",
              "  'like',\n",
              "  'getting',\n",
              "  'shoelaces',\n",
              "  'with',\n",
              "  'shoes',\n",
              "  'or',\n",
              "  'something'],\n",
              " ['it', 'just', 'comes', 'along'],\n",
              " ['do', 'you', 'know', 'what', 'mean'],\n",
              " ['like',\n",
              "  'so',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'like',\n",
              "  'how',\n",
              "  'do',\n",
              "  'you',\n",
              "  'mean',\n",
              "  'one',\n",
              "  'one',\n",
              "  'way',\n",
              "  'of',\n",
              "  'looking',\n",
              "  'at',\n",
              "  'it',\n",
              "  'would',\n",
              "  'be',\n",
              "  'well',\n",
              "  'the',\n",
              "  'people',\n",
              "  'producing',\n",
              "  'television',\n",
              "  'sets',\n",
              "  'maybe',\n",
              "  'they',\n",
              "  'have',\n",
              "  'to',\n",
              "  'buy',\n",
              "  'remote',\n",
              "  'controls'],\n",
              " ['or',\n",
              "  'another',\n",
              "  'way',\n",
              "  'is',\n",
              "  'maybe',\n",
              "  'people',\n",
              "  'who',\n",
              "  'have',\n",
              "  't_v_',\n",
              "  'sets',\n",
              "  'are',\n",
              "  'really',\n",
              "  'fed',\n",
              "  'up',\n",
              "  'with',\n",
              "  'their',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'and',\n",
              "  'they',\n",
              "  'really',\n",
              "  'want',\n",
              "  'better',\n",
              "  'one',\n",
              "  'or',\n",
              "  'something'],\n",
              " ['but', 'right'],\n",
              " ['right'],\n",
              " ['okay',\n",
              "  'so',\n",
              "  'right',\n",
              "  'so',\n",
              "  'in',\n",
              "  'function',\n",
              "  'one',\n",
              "  'of',\n",
              "  'the',\n",
              "  'priorities',\n",
              "  'might',\n",
              "  'be',\n",
              "  'to',\n",
              "  'combine',\n",
              "  'as',\n",
              "  'many',\n",
              "  'uses',\n",
              "  'think',\n",
              "  'so'],\n",
              " ['yeah', 'yeah'],\n",
              " ['yeah'],\n",
              " ['well',\n",
              "  'like',\n",
              "  'um',\n",
              "  'maybe',\n",
              "  'what',\n",
              "  'we',\n",
              "  'could',\n",
              "  'use',\n",
              "  'is',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'like',\n",
              "  'example',\n",
              "  'of',\n",
              "  'successful',\n",
              "  'other',\n",
              "  'piece',\n",
              "  'technology',\n",
              "  'is',\n",
              "  'palm',\n",
              "  'palm',\n",
              "  'pilots'],\n",
              " ['theyre',\n",
              "  'gone',\n",
              "  'from',\n",
              "  'being',\n",
              "  'just',\n",
              "  'like',\n",
              "  'little',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'scribble',\n",
              "  'boards',\n",
              "  'to',\n",
              "  'cameras',\n",
              "  'm_p_',\n",
              "  'three',\n",
              "  'players',\n",
              "  'telephones',\n",
              "  'everything',\n",
              "  'agenda'],\n",
              " ['so',\n",
              "  'like',\n",
              "  'wonder',\n",
              "  'if',\n",
              "  'we',\n",
              "  'might',\n",
              "  'add',\n",
              "  'something',\n",
              "  'new',\n",
              "  'to',\n",
              "  'the',\n",
              "  'to',\n",
              "  'the',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'market',\n",
              "  'such',\n",
              "  'as',\n",
              "  'the',\n",
              "  'lighting',\n",
              "  'in',\n",
              "  'your',\n",
              "  'house',\n",
              "  'or',\n",
              "  'um',\n",
              "  'yeah',\n",
              "  'yeah'],\n",
              " ['an', 'yeah'],\n",
              " ['like',\n",
              "  'personally',\n",
              "  'for',\n",
              "  'me',\n",
              "  'at',\n",
              "  'home',\n",
              "  'ive',\n",
              "  'ive',\n",
              "  'combined',\n",
              "  'the',\n",
              "  'um',\n",
              "  'the',\n",
              "  'audio',\n",
              "  'video',\n",
              "  'of',\n",
              "  'my',\n",
              "  'television',\n",
              "  'set',\n",
              "  'and',\n",
              "  'my',\n",
              "  'd_v_d_',\n",
              "  'player',\n",
              "  'and',\n",
              "  'my',\n",
              "  'c_d_',\n",
              "  'player'],\n",
              " ['so',\n",
              "  'they',\n",
              "  'all',\n",
              "  'work',\n",
              "  'actually',\n",
              "  'function',\n",
              "  'together',\n",
              "  'but',\n",
              "  'have',\n",
              "  'different',\n",
              "  'remote',\n",
              "  'controls',\n",
              "  'for',\n",
              "  'each',\n",
              "  'of',\n",
              "  'them'],\n",
              " ['so',\n",
              "  'its',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'ironic',\n",
              "  'that',\n",
              "  'that',\n",
              "  'then',\n",
              "  'theyre',\n",
              "  'in',\n",
              "  'there',\n",
              "  'um',\n",
              "  'you',\n",
              "  'know',\n",
              "  'the',\n",
              "  'sound',\n",
              "  'and',\n",
              "  'everything',\n",
              "  'its',\n",
              "  'just',\n",
              "  'one',\n",
              "  'system'],\n",
              " ['but', 'each', 'ones', 'got', 'its', 'own', 'little', 'part'],\n",
              " ['mm'],\n",
              " ['mm'],\n",
              " ['mm'],\n",
              " ['mm', 'hmm'],\n",
              " ['mm', 'hmm'],\n",
              " ['yeah'],\n",
              " ['yeah'],\n",
              " ['thats', 'just', 'really', 'good', 'id', 'yep'],\n",
              " ['uh', 'sure'],\n",
              " ['remember',\n",
              "  'when',\n",
              "  'the',\n",
              "  'first',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'my',\n",
              "  'my',\n",
              "  'family',\n",
              "  'had',\n",
              "  'was',\n",
              "  'on',\n",
              "  'cable'],\n",
              " ['actually',\n",
              "  'had',\n",
              "  'cable',\n",
              "  'between',\n",
              "  'it',\n",
              "  'and',\n",
              "  'the',\n",
              "  't_v_',\n",
              "  'and',\n",
              "  'big',\n",
              "  'like',\n",
              "  'buttons',\n",
              "  'that',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'like',\n",
              "  'like',\n",
              "  'on',\n",
              "  'blender',\n",
              "  'or',\n",
              "  'something'],\n",
              " ['and',\n",
              "  'um',\n",
              "  'you',\n",
              "  'know',\n",
              "  'when',\n",
              "  'think',\n",
              "  'about',\n",
              "  'what',\n",
              "  'they',\n",
              "  'are',\n",
              "  'now',\n",
              "  'its',\n",
              "  'better',\n",
              "  'but',\n",
              "  'actually',\n",
              "  'its',\n",
              "  'still',\n",
              "  'kind',\n",
              "  'of',\n",
              "  'dunno',\n",
              "  'like',\n",
              "  'massive',\n",
              "  'junky',\n",
              "  'thing',\n",
              "  'on',\n",
              "  'the',\n",
              "  'table'],\n",
              " ['maybe',\n",
              "  'we',\n",
              "  'could',\n",
              "  'think',\n",
              "  'about',\n",
              "  'how',\n",
              "  'could',\n",
              "  'be',\n",
              "  'more',\n",
              "  'you',\n",
              "  'know',\n",
              "  'streamlined'],\n",
              " ['something', 'like', 'that', 'yeah'],\n",
              " ['or', 'whatever', 'would', 'be', 'technologically', 'reasonable'],\n",
              " ['cause',\n",
              "  'it',\n",
              "  'could',\n",
              "  'it',\n",
              "  'could',\n",
              "  'it',\n",
              "  'could',\n",
              "  'be',\n",
              "  'that',\n",
              "  'it',\n",
              "  'could',\n",
              "  'be',\n",
              "  'that',\n",
              "  'functionally',\n",
              "  'that',\n",
              "  'doesnt',\n",
              "  'make',\n",
              "  'it',\n",
              "  'any',\n",
              "  'better',\n",
              "  'but',\n",
              "  'that',\n",
              "  'just',\n",
              "  'the',\n",
              "  'appeal',\n",
              "  'of',\n",
              "  'of',\n",
              "  'not',\n",
              "  'having',\n",
              "  'you',\n",
              "  'know',\n",
              "  'these',\n",
              "  'days',\n",
              "  'theres',\n",
              "  'pe',\n",
              "  'things',\n",
              "  'in',\n",
              "  'peoples',\n",
              "  'homes',\n",
              "  'are',\n",
              "  'becoming',\n",
              "  'more',\n",
              "  'and',\n",
              "  'more',\n",
              "  'like',\n",
              "  'chic',\n",
              "  'you',\n",
              "  'know'],\n",
              " ['um',\n",
              "  'nicer',\n",
              "  'materials',\n",
              "  'and',\n",
              "  'might',\n",
              "  'be',\n",
              "  'be',\n",
              "  'worth',\n",
              "  'exploring',\n",
              "  'anyway'],\n",
              " ['okay'],\n",
              " ['um'],\n",
              " ['before',\n",
              "  'we',\n",
              "  'wrap',\n",
              "  'up',\n",
              "  'just',\n",
              "  'to',\n",
              "  'make',\n",
              "  'sure',\n",
              "  'were',\n",
              "  'all',\n",
              "  'on',\n",
              "  'the',\n",
              "  'same',\n",
              "  'page',\n",
              "  'here',\n",
              "  'um',\n",
              "  'do',\n",
              "  'we',\n",
              "  'we',\n",
              "  'were',\n",
              "  'given',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'an',\n",
              "  'example',\n",
              "  'of',\n",
              "  'coffee',\n",
              "  'machine',\n",
              "  'or',\n",
              "  'something',\n",
              "  'right'],\n",
              " ['well',\n",
              "  'um',\n",
              "  'are',\n",
              "  'we',\n",
              "  'at',\n",
              "  'ma',\n",
              "  'right',\n",
              "  'now',\n",
              "  'on',\n",
              "  'the',\n",
              "  'assumption',\n",
              "  'that',\n",
              "  'our',\n",
              "  'television',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'may',\n",
              "  'have',\n",
              "  'features',\n",
              "  'which',\n",
              "  'go',\n",
              "  'beyond',\n",
              "  'the',\n",
              "  'television'],\n",
              " ['or',\n",
              "  'are',\n",
              "  'we',\n",
              "  'keeping',\n",
              "  'sort',\n",
              "  'of',\n",
              "  'like',\n",
              "  'design',\n",
              "  'commitment',\n",
              "  'to',\n",
              "  'television',\n",
              "  'features'],\n",
              " ['dont', 'know'],\n",
              " ['yep'],\n",
              " ['yeah', 'sure'],\n",
              " ['okay'],\n",
              " ['okay', 'yeah'],\n",
              " ['okay'],\n",
              " ['okay'],\n",
              " ['okay'],\n",
              " ['alright'],\n",
              " ['okay'],\n",
              " ['right'],\n",
              " ['um',\n",
              "  'well',\n",
              "  'this',\n",
              "  'is',\n",
              "  'the',\n",
              "  'kick',\n",
              "  'off',\n",
              "  'meeting',\n",
              "  'for',\n",
              "  'our',\n",
              "  'our',\n",
              "  'project'],\n",
              " ['um',\n",
              "  'and',\n",
              "  'um',\n",
              "  'this',\n",
              "  'is',\n",
              "  'just',\n",
              "  'what',\n",
              "  'were',\n",
              "  'gonna',\n",
              "  'be',\n",
              "  'doing',\n",
              "  'over',\n",
              "  'the',\n",
              "  'next',\n",
              "  'twenty',\n",
              "  'five',\n",
              "  'minutes'],\n",
              " ['um',\n",
              "  'so',\n",
              "  'first',\n",
              "  'of',\n",
              "  'all',\n",
              "  'just',\n",
              "  'to',\n",
              "  'kind',\n",
              "  'of',\n",
              "  'make',\n",
              "  'sure',\n",
              "  'that',\n",
              "  'we',\n",
              "  'all',\n",
              "  'know',\n",
              "  'each',\n",
              "  'other',\n",
              "  'im',\n",
              "  'laura',\n",
              "  'and',\n",
              "  'im',\n",
              "  'the',\n",
              "  'project',\n",
              "  'manager'],\n",
              " ['do', 'you', 'want', 'to', 'introduce', 'yourself', 'again'],\n",
              " ['okay'],\n",
              " ['great'],\n",
              " ['okay'],\n",
              " ['um',\n",
              "  'so',\n",
              "  'were',\n",
              "  'designing',\n",
              "  'new',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'and',\n",
              "  'um',\n",
              "  'oh',\n",
              "  'have',\n",
              "  'to',\n",
              "  'record',\n",
              "  'whos',\n",
              "  'here',\n",
              "  'actually'],\n",
              " ['so', 'thats', 'david', 'andrew', 'and', 'craig', 'isnt', 'it'],\n",
              " ['and', 'you', 'all', 'arrived', 'on', 'time'],\n",
              " ['um', 'yeah', 'so', 'des', 'uh', 'design', 'new', 'remote', 'control'],\n",
              " ['um',\n",
              "  'as',\n",
              "  'you',\n",
              "  'can',\n",
              "  'see',\n",
              "  'its',\n",
              "  'supposed',\n",
              "  'to',\n",
              "  'be',\n",
              "  'original',\n",
              "  'trendy',\n",
              "  'and',\n",
              "  'user',\n",
              "  'friendly'],\n",
              " ['um',\n",
              "  'so',\n",
              "  'thats',\n",
              "  'kind',\n",
              "  'of',\n",
              "  'our',\n",
              "  'our',\n",
              "  'brief',\n",
              "  'as',\n",
              "  'it',\n",
              "  'were'],\n",
              " ['um',\n",
              "  'and',\n",
              "  'so',\n",
              "  'there',\n",
              "  'are',\n",
              "  'three',\n",
              "  'different',\n",
              "  'stages',\n",
              "  'to',\n",
              "  'the',\n",
              "  'design'],\n",
              " ['um',\n",
              "  'im',\n",
              "  'not',\n",
              "  'really',\n",
              "  'sure',\n",
              "  'what',\n",
              "  'what',\n",
              "  'you',\n",
              "  'guys',\n",
              "  'have',\n",
              "  'already',\n",
              "  'received',\n",
              "  'um',\n",
              "  'in',\n",
              "  'your',\n",
              "  'emails'],\n",
              " ['what', 'did', 'you', 'get'],\n",
              " ['mm', 'hmm'],\n",
              " ['is', 'that', 'what', 'everybody', 'got'],\n",
              " ['okay'],\n",
              " ['um'],\n",
              " ['so',\n",
              "  'were',\n",
              "  'gonna',\n",
              "  'have',\n",
              "  'like',\n",
              "  'individual',\n",
              "  'work',\n",
              "  'and',\n",
              "  'then',\n",
              "  'meeting',\n",
              "  'about',\n",
              "  'it'],\n",
              " ['and', 'repeat', 'that', 'process', 'three', 'times'],\n",
              " ['um',\n",
              "  'and',\n",
              "  'at',\n",
              "  'this',\n",
              "  'point',\n",
              "  'we',\n",
              "  'get',\n",
              "  'try',\n",
              "  'out',\n",
              "  'the',\n",
              "  'whiteboard',\n",
              "  'over',\n",
              "  'there'],\n",
              " ['um'],\n",
              " ['so',\n",
              "  'uh',\n",
              "  'you',\n",
              "  'get',\n",
              "  'to',\n",
              "  'draw',\n",
              "  'your',\n",
              "  'favourite',\n",
              "  'animal',\n",
              "  'and',\n",
              "  'sum',\n",
              "  'up',\n",
              "  'your',\n",
              "  'favourite',\n",
              "  'characteristics',\n",
              "  'of',\n",
              "  'it'],\n",
              " ['so', 'who', 'would', 'like', 'to', 'go', 'first'],\n",
              " ['very', 'good'],\n",
              " ['mm', 'hmm'],\n",
              " ['yeah'],\n",
              " ['yeah'],\n",
              " ['right'],\n",
              " ['lovely'],\n",
              " ['right'],\n",
              " ['you',\n",
              "  'can',\n",
              "  'take',\n",
              "  'as',\n",
              "  'long',\n",
              "  'over',\n",
              "  'this',\n",
              "  'as',\n",
              "  'you',\n",
              "  'like',\n",
              "  'because',\n",
              "  'we',\n",
              "  'havent',\n",
              "  'got',\n",
              "  'an',\n",
              "  'awful',\n",
              "  'lot',\n",
              "  'to',\n",
              "  'discuss'],\n",
              " ['ok', 'oh', 'we', 'do', 'we', 'do'],\n",
              " ['dont', 'feel', 'like', 'youre', 'in', 'rush', 'anyway'],\n",
              " ['ach',\n",
              "  'why',\n",
              "  'not',\n",
              "  'we',\n",
              "  'might',\n",
              "  'have',\n",
              "  'to',\n",
              "  'get',\n",
              "  'you',\n",
              "  'up',\n",
              "  'again',\n",
              "  'then'],\n",
              " ['dont', 'know', 'what', 'mine', 'is'],\n",
              " ['im', 'gonna', 'have', 'to', 'think', 'on', 'the', 'spot', 'now'],\n",
              " ['is', 'that', 'whale'],\n",
              " ['ah'],\n",
              " ['okay'],\n",
              " ['god', 'still', 'dont', 'know', 'what', 'im', 'gonna', 'write', 'about'],\n",
              " ['um'],\n",
              " ['was', 'gonna', 'choose', 'dog', 'as', 'well'],\n",
              " ['but', 'ill', 'just', 'draw', 'different', 'kind', 'of', 'dog'],\n",
              " ['my', 'favourite', 'animal', 'is', 'my', 'own', 'dog', 'at', 'home'],\n",
              " ['um', 'that', 'doesnt', 'really', 'look', 'like', 'him', 'actually'],\n",
              " ['he', 'looks', 'more', 'like', 'pig', 'actually'],\n",
              " ['ah', 'well'],\n",
              " ['do', 'you'],\n",
              " ['oh', 'thats', 'very', 'good', 'of', 'you'],\n",
              " ['uh'],\n",
              " ['um', 'hes', 'mixture', 'of', 'uh', 'various', 'things'],\n",
              " ['um',\n",
              "  'and',\n",
              "  'what',\n",
              "  'do',\n",
              "  'like',\n",
              "  'about',\n",
              "  'him',\n",
              "  'um',\n",
              "  'thats',\n",
              "  'just',\n",
              "  'to',\n",
              "  'suggest',\n",
              "  'that',\n",
              "  'his',\n",
              "  'tail',\n",
              "  'wags'],\n",
              " ['um',\n",
              "  'hes',\n",
              "  'very',\n",
              "  'friendly',\n",
              "  'and',\n",
              "  'cheery',\n",
              "  'and',\n",
              "  'always',\n",
              "  'pleased',\n",
              "  'to',\n",
              "  'see',\n",
              "  'you',\n",
              "  'and',\n",
              "  'very',\n",
              "  'kind',\n",
              "  'of',\n",
              "  'affectionate',\n",
              "  'and',\n",
              "  'um',\n",
              "  'uh',\n",
              "  'and',\n",
              "  'hes',\n",
              "  'quite',\n",
              "  'quite',\n",
              "  'wee',\n",
              "  'as',\n",
              "  'well',\n",
              "  'so',\n",
              "  'you',\n",
              "  'know',\n",
              "  'he',\n",
              "  'can',\n",
              "  'doesnt',\n",
              "  'take',\n",
              "  'up',\n",
              "  'too',\n",
              "  'much',\n",
              "  'space'],\n",
              " ['um',\n",
              "  'and',\n",
              "  'uh',\n",
              "  'and',\n",
              "  'he',\n",
              "  'does',\n",
              "  'funny',\n",
              "  'thing',\n",
              "  'where',\n",
              "  'he',\n",
              "  'chases',\n",
              "  'his',\n",
              "  'tail',\n",
              "  'as',\n",
              "  'well',\n",
              "  'which',\n",
              "  'is',\n",
              "  'quite',\n",
              "  'amusing',\n",
              "  'so',\n",
              "  'it',\n",
              "  'is'],\n",
              " ['think', 'it', 'is'],\n",
              " ['he',\n",
              "  'only',\n",
              "  'does',\n",
              "  'it',\n",
              "  'after',\n",
              "  'hes',\n",
              "  'had',\n",
              "  'his',\n",
              "  'dinner',\n",
              "  'and',\n",
              "  'um',\n",
              "  'hell',\n",
              "  'just',\n",
              "  'all',\n",
              "  'of',\n",
              "  'sudden',\n",
              "  'just',\n",
              "  'get',\n",
              "  'up',\n",
              "  'and',\n",
              "  'start',\n",
              "  'chasing',\n",
              "  'his',\n",
              "  'tail',\n",
              "  'round',\n",
              "  'the',\n",
              "  'living',\n",
              "  'room'],\n",
              " ['yeah', 'so', 'uh', 'yeah', 'maybe'],\n",
              " ['maybe'],\n",
              " ['right', 'um', 'where', 'did', 'you', 'find', 'this'],\n",
              " ['just', 'down', 'here'],\n",
              " ['yeah'],\n",
              " ['okay'],\n",
              " ['um', 'what', 'are', 'we', 'doing', 'next'],\n",
              " ['uh', 'um'],\n",
              " ['okay',\n",
              "  'uh',\n",
              "  'we',\n",
              "  'now',\n",
              "  'need',\n",
              "  'to',\n",
              "  'discuss',\n",
              "  'the',\n",
              "  'project',\n",
              "  'finance'],\n",
              " ['um',\n",
              "  'so',\n",
              "  'according',\n",
              "  'to',\n",
              "  'the',\n",
              "  'brief',\n",
              "  'um',\n",
              "  'were',\n",
              "  'gonna',\n",
              "  'be',\n",
              "  'selling',\n",
              "  'this',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'for',\n",
              "  'twenty',\n",
              "  'five',\n",
              "  'euro',\n",
              "  'um',\n",
              "  'and',\n",
              "  'were',\n",
              "  'aiming',\n",
              "  'to',\n",
              "  'make',\n",
              "  'fifty',\n",
              "  'million',\n",
              "  'euro'],\n",
              " ['um',\n",
              "  'so',\n",
              "  'were',\n",
              "  'gonna',\n",
              "  'be',\n",
              "  'selling',\n",
              "  'this',\n",
              "  'on',\n",
              "  'an',\n",
              "  'international',\n",
              "  'scale'],\n",
              " ['and',\n",
              "  'uh',\n",
              "  'we',\n",
              "  'dont',\n",
              "  'want',\n",
              "  'it',\n",
              "  'to',\n",
              "  'cost',\n",
              "  'any',\n",
              "  'more',\n",
              "  'than',\n",
              "  'uh',\n",
              "  'twelve',\n",
              "  'fifty',\n",
              "  'euros',\n",
              "  'so',\n",
              "  'fifty',\n",
              "  'percent',\n",
              "  'of',\n",
              "  'the',\n",
              "  'selling',\n",
              "  'price'],\n",
              " ['sure'],\n",
              " ['all', 'together'],\n",
              " ['um', 'dunno'],\n",
              " ['imagine', 'thats', 'good', 'question'],\n",
              " ['imagine',\n",
              "  'it',\n",
              "  'probably',\n",
              "  'is',\n",
              "  'our',\n",
              "  'sale',\n",
              "  'actually',\n",
              "  'because',\n",
              "  'its',\n",
              "  'probably',\n",
              "  'up',\n",
              "  'to',\n",
              "  'the',\n",
              "  'the',\n",
              "  'um',\n",
              "  'the',\n",
              "  'retailer',\n",
              "  'to',\n",
              "  'uh',\n",
              "  'sell',\n",
              "  'it',\n",
              "  'for',\n",
              "  'whatever',\n",
              "  'price',\n",
              "  'they',\n",
              "  'want'],\n",
              " ['um'],\n",
              " ['but',\n",
              "  'dont',\n",
              "  'know',\n",
              "  'mean',\n",
              "  'do',\n",
              "  'you',\n",
              "  'think',\n",
              "  'the',\n",
              "  'fact',\n",
              "  'that',\n",
              "  'its',\n",
              "  'going',\n",
              "  'to',\n",
              "  'be',\n",
              "  'sold',\n",
              "  'internationally',\n",
              "  'will',\n",
              "  'have',\n",
              "  'bearing',\n",
              "  'on',\n",
              "  'how',\n",
              "  'we',\n",
              "  'design',\n",
              "  'it',\n",
              "  'at',\n",
              "  'all'],\n",
              " ['think', 'it', 'will'],\n",
              " ['um'],\n",
              " ['hmm'],\n",
              " ['oh', 'yeah', 'regions', 'and', 'stuff', 'yeah'],\n",
              " ['yeah'],\n",
              " ['okay'],\n",
              " ['yeah'],\n",
              " ['well',\n",
              "  'for',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'do',\n",
              "  'you',\n",
              "  'think',\n",
              "  'that',\n",
              "  'will',\n",
              "  'be',\n",
              "  'suppose',\n",
              "  'its',\n",
              "  'depends',\n",
              "  'on',\n",
              "  'how',\n",
              "  'complicated',\n",
              "  'our',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'is'],\n",
              " ['yeah', 'yeah'],\n",
              " ['okay'],\n",
              " ['what',\n",
              "  'just',\n",
              "  'like',\n",
              "  'in',\n",
              "  'terms',\n",
              "  'of',\n",
              "  'like',\n",
              "  'the',\n",
              "  'wealth',\n",
              "  'of',\n",
              "  'the',\n",
              "  'country'],\n",
              " ['like',\n",
              "  'how',\n",
              "  'much',\n",
              "  'money',\n",
              "  'people',\n",
              "  'have',\n",
              "  'to',\n",
              "  'spend',\n",
              "  'on',\n",
              "  'things',\n",
              "  'like'],\n",
              " ['aye', 'see', 'what', 'you', 'mean', 'yeah'],\n",
              " ['marketing'],\n",
              " ['good', 'marketing', 'thoughts'],\n",
              " ['oh', 'gosh', 'should', 'be', 'writing', 'all', 'this', 'down'],\n",
              " ['um'],\n",
              " ['mm'],\n",
              " ['yeah'],\n",
              " ['yeah', 'yeah'],\n",
              " ['like', 'how', 'much', 'does', 'you', 'know', 'remote', 'control', 'cost'],\n",
              " ['well',\n",
              "  'twenty',\n",
              "  'five',\n",
              "  'euro',\n",
              "  'mean',\n",
              "  'thats',\n",
              "  'um',\n",
              "  'thats',\n",
              "  'about',\n",
              "  'like',\n",
              "  'eighteen',\n",
              "  'pounds',\n",
              "  'or',\n",
              "  'something',\n",
              "  'isnt',\n",
              "  'it'],\n",
              " ['or', 'no', 'is', 'it', 'as', 'much', 'as', 'that'],\n",
              " ['sixteen', 'seventeen', 'eighteen', 'pounds'],\n",
              " ['um',\n",
              "  'dunno',\n",
              "  'ive',\n",
              "  'never',\n",
              "  'bought',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'so',\n",
              "  'dont',\n",
              "  'know',\n",
              "  'how',\n",
              "  'how',\n",
              "  'good',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'that',\n",
              "  'would',\n",
              "  'get',\n",
              "  'you'],\n",
              " ['um'],\n",
              " ['but',\n",
              "  'yeah',\n",
              "  'suppose',\n",
              "  'it',\n",
              "  'has',\n",
              "  'to',\n",
              "  'look',\n",
              "  'kind',\n",
              "  'of',\n",
              "  'cool',\n",
              "  'and',\n",
              "  'gimmicky'],\n",
              " ['um', 'right', 'okay'],\n",
              " ['let', 'me', 'just', 'scoot', 'on', 'ahead', 'here'],\n",
              " ['okay'],\n",
              " ['um',\n",
              "  'well',\n",
              "  'does',\n",
              "  'anybody',\n",
              "  'have',\n",
              "  'anything',\n",
              "  'to',\n",
              "  'add',\n",
              "  'to',\n",
              "  'uh',\n",
              "  'to',\n",
              "  'the',\n",
              "  'finance',\n",
              "  'issue',\n",
              "  'at',\n",
              "  'all'],\n",
              " ['thin', 'no', 'actually'],\n",
              " ['that',\n",
              "  'would',\n",
              "  'be',\n",
              "  'useful',\n",
              "  'though',\n",
              "  'wouldnt',\n",
              "  'it',\n",
              "  'if',\n",
              "  'you',\n",
              "  'knew',\n",
              "  'like',\n",
              "  'what',\n",
              "  'your',\n",
              "  'money',\n",
              "  'would',\n",
              "  'get',\n",
              "  'you',\n",
              "  'now'],\n",
              " ['mm', 'hmm'],\n",
              " ['yeah', 'yeah'],\n",
              " ['oh'],\n",
              " ['five', 'minutes', 'to', 'end', 'of', 'meeting'],\n",
              " ['oh', 'okay'],\n",
              " ['were', 'bit', 'behind'],\n",
              " ['yeah'],\n",
              " ['right',\n",
              "  'so',\n",
              "  'do',\n",
              "  'you',\n",
              "  'think',\n",
              "  'that',\n",
              "  'should',\n",
              "  'be',\n",
              "  'like',\n",
              "  'main',\n",
              "  'design',\n",
              "  'aim',\n",
              "  'of',\n",
              "  'our',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'you',\n",
              "  'know',\n",
              "  'do',\n",
              "  'your',\n",
              "  'your',\n",
              "  'satellite',\n",
              "  'and',\n",
              "  'your',\n",
              "  'regular',\n",
              "  'telly',\n",
              "  'and',\n",
              "  'your',\n",
              "  'v_c_r_',\n",
              "  'and',\n",
              "  'everything'],\n",
              " ['mm', 'hmm'],\n",
              " ['yeah'],\n",
              " ['or',\n",
              "  'even',\n",
              "  'like',\n",
              "  'you',\n",
              "  'know',\n",
              "  'notes',\n",
              "  'about',\n",
              "  'um',\n",
              "  'what',\n",
              "  'you',\n",
              "  'wanna',\n",
              "  'watch'],\n",
              " ['like',\n",
              "  'you',\n",
              "  'might',\n",
              "  'put',\n",
              "  'in',\n",
              "  'there',\n",
              "  'oh',\n",
              "  'want',\n",
              "  'to',\n",
              "  'watch',\n",
              "  'such',\n",
              "  'and',\n",
              "  'such',\n",
              "  'and',\n",
              "  'look',\n",
              "  'oh',\n",
              "  'thats',\n",
              "  'good',\n",
              "  'idea'],\n",
              " ['so', 'extra', 'functionalities'],\n",
              " ['mm', 'hmm'],\n",
              " ['hmm'],\n",
              " ['um',\n",
              "  'okay',\n",
              "  'uh',\n",
              "  'id',\n",
              "  'wel',\n",
              "  'were',\n",
              "  'gonna',\n",
              "  'have',\n",
              "  'to',\n",
              "  'wrap',\n",
              "  'up',\n",
              "  'pretty',\n",
              "  'quickly',\n",
              "  'in',\n",
              "  'the',\n",
              "  'next',\n",
              "  'couple',\n",
              "  'of',\n",
              "  'minutes'],\n",
              " ['um', 'ill', 'just', 'check', 'weve', 'nothing', 'else'],\n",
              " ['okay'],\n",
              " ['um',\n",
              "  'so',\n",
              "  'anything',\n",
              "  'else',\n",
              "  'anybody',\n",
              "  'wants',\n",
              "  'to',\n",
              "  'add',\n",
              "  'about',\n",
              "  'what',\n",
              "  'they',\n",
              "  'dont',\n",
              "  'like',\n",
              "  'about',\n",
              "  'remote',\n",
              "  'controls',\n",
              "  'theyve',\n",
              "  'used',\n",
              "  'what',\n",
              "  'they',\n",
              "  'would',\n",
              "  'really',\n",
              "  'like',\n",
              "  'to',\n",
              "  'be',\n",
              "  'part',\n",
              "  'of',\n",
              "  'this',\n",
              "  'new',\n",
              "  'one',\n",
              "  'at',\n",
              "  'all'],\n",
              " ['you', 'keep', 'losing', 'them'],\n",
              " ['okay'],\n",
              " ['yeah'],\n",
              " ['you',\n",
              "  'get',\n",
              "  'those',\n",
              "  'ones',\n",
              "  'where',\n",
              "  'you',\n",
              "  'can',\n",
              "  'if',\n",
              "  'you',\n",
              "  'like',\n",
              "  'whistle',\n",
              "  'or',\n",
              "  'make',\n",
              "  'really',\n",
              "  'high',\n",
              "  'pitched',\n",
              "  'noise',\n",
              "  'they',\n",
              "  'beep'],\n",
              " ['there',\n",
              "  'mean',\n",
              "  'is',\n",
              "  'that',\n",
              "  'something',\n",
              "  'wed',\n",
              "  'want',\n",
              "  'to',\n",
              "  'include',\n",
              "  'do',\n",
              "  'you',\n",
              "  'think'],\n",
              " ['dunno'],\n",
              " ['okay', 'maybe'],\n",
              " ['my', 'goodness'],\n",
              " ['still', 'feels', 'quite', 'primitive'],\n",
              " ['maybe', 'like', 'touch', 'screen', 'or', 'something'],\n",
              " ['okay'],\n",
              " ['uh', 'huh', 'okay'],\n",
              " ['well', 'guess', 'thats', 'up', 'to', 'our', 'industrial', 'designer'],\n",
              " ['it', 'looks', 'better'],\n",
              " ['yeah'],\n",
              " ['okay'],\n",
              " ['okay'],\n",
              " ['right',\n",
              "  'well',\n",
              "  'um',\n",
              "  'so',\n",
              "  'just',\n",
              "  'to',\n",
              "  'wrap',\n",
              "  'up',\n",
              "  'the',\n",
              "  'next',\n",
              "  'meetings',\n",
              "  'gonna',\n",
              "  'be',\n",
              "  'in',\n",
              "  'thirty',\n",
              "  'minutes'],\n",
              " ['so',\n",
              "  'thats',\n",
              "  'about',\n",
              "  'um',\n",
              "  'about',\n",
              "  'ten',\n",
              "  'to',\n",
              "  'twelve',\n",
              "  'by',\n",
              "  'my',\n",
              "  'watch'],\n",
              " ['um',\n",
              "  'so',\n",
              "  'inbetween',\n",
              "  'now',\n",
              "  'and',\n",
              "  'then',\n",
              "  'um',\n",
              "  'as',\n",
              "  'the',\n",
              "  'industrial',\n",
              "  'designer',\n",
              "  'youre',\n",
              "  'gonna',\n",
              "  'be',\n",
              "  'working',\n",
              "  'on',\n",
              "  'you',\n",
              "  'know',\n",
              "  'the',\n",
              "  'actual',\n",
              "  'working',\n",
              "  'design',\n",
              "  'of',\n",
              "  'it',\n",
              "  'so',\n",
              "  'you',\n",
              "  'know',\n",
              "  'what',\n",
              "  'youre',\n",
              "  'doing',\n",
              "  'there'],\n",
              " ['um',\n",
              "  'for',\n",
              "  'user',\n",
              "  'interface',\n",
              "  'technical',\n",
              "  'functions',\n",
              "  'guess',\n",
              "  'thats',\n",
              "  'you',\n",
              "  'know',\n",
              "  'like',\n",
              "  'what',\n",
              "  'weve',\n",
              "  'been',\n",
              "  'talking',\n",
              "  'about',\n",
              "  'what',\n",
              "  'itll',\n",
              "  'actually',\n",
              "  'do'],\n",
              " ['um',\n",
              "  'and',\n",
              "  'uh',\n",
              "  'marketing',\n",
              "  'executive',\n",
              "  'youll',\n",
              "  'be',\n",
              "  'just',\n",
              "  'thinking',\n",
              "  'about',\n",
              "  'what',\n",
              "  'it',\n",
              "  'actually',\n",
              "  'what',\n",
              "  'you',\n",
              "  'know',\n",
              "  'what',\n",
              "  'requirements',\n",
              "  'it',\n",
              "  'has',\n",
              "  'to',\n",
              "  'has',\n",
              "  'to',\n",
              "  'fulfil',\n",
              "  'and',\n",
              "  'youll',\n",
              "  'all',\n",
              "  'get',\n",
              "  'instructions',\n",
              "  'emailed',\n",
              "  'to',\n",
              "  'you',\n",
              "  'guess'],\n",
              " ['um'],\n",
              " ['yeah',\n",
              "  'so',\n",
              "  'its',\n",
              "  'th',\n",
              "  'the',\n",
              "  'functional',\n",
              "  'design',\n",
              "  'stage',\n",
              "  'is',\n",
              "  'next',\n",
              "  'guess'],\n",
              " ['and', 'uh', 'and', 'thats', 'the', 'end', 'of', 'the', 'meeting'],\n",
              " ['so',\n",
              "  'got',\n",
              "  'that',\n",
              "  'little',\n",
              "  'message',\n",
              "  'lot',\n",
              "  'sooner',\n",
              "  'than',\n",
              "  'thought',\n",
              "  'would',\n",
              "  'so',\n",
              "  'mm',\n",
              "  'hmm'],\n",
              " ['uh', 'huh', 'yeah'],\n",
              " ['th',\n",
              "  'okay',\n",
              "  'well',\n",
              "  'just',\n",
              "  'very',\n",
              "  'quickly',\n",
              "  'cause',\n",
              "  'this',\n",
              "  'were',\n",
              "  'supposed',\n",
              "  'to',\n",
              "  'finish',\n",
              "  'now'],\n",
              " ['um',\n",
              "  'guess',\n",
              "  'thats',\n",
              "  'up',\n",
              "  'to',\n",
              "  'us',\n",
              "  'mean',\n",
              "  'you',\n",
              "  'probably',\n",
              "  'want',\n",
              "  'some',\n",
              "  'kind',\n",
              "  'of',\n",
              "  'unique',\n",
              "  'selling',\n",
              "  'point',\n",
              "  'of',\n",
              "  'it',\n",
              "  'so',\n",
              "  'um',\n",
              "  'you',\n",
              "  'know',\n",
              "  'yeah'],\n",
              " ['mm', 'hmm'],\n",
              " ['yeah'],\n",
              " ['okay'],\n",
              " ['right',\n",
              "  'okay',\n",
              "  'well',\n",
              "  'thats',\n",
              "  'thats',\n",
              "  'the',\n",
              "  'end',\n",
              "  'of',\n",
              "  'the',\n",
              "  'meeting',\n",
              "  'then'],\n",
              " ['um'],\n",
              " ['so', 'uh', 'thank', 'you', 'all', 'for', 'coming'],\n",
              " ['hi',\n",
              "  'im',\n",
              "  'david',\n",
              "  'and',\n",
              "  'im',\n",
              "  'supposed',\n",
              "  'to',\n",
              "  'be',\n",
              "  'an',\n",
              "  'industrial',\n",
              "  'designer'],\n",
              " ['um',\n",
              "  'just',\n",
              "  'got',\n",
              "  'the',\n",
              "  'project',\n",
              "  'announcement',\n",
              "  'about',\n",
              "  'what',\n",
              "  'the',\n",
              "  'project',\n",
              "  'is'],\n",
              " ['designing', 'remote', 'control'],\n",
              " ['thats', 'about', 'it', 'didnt', 'get', 'anything', 'else'],\n",
              " ['did', 'you', 'get', 'the', 'same', 'thing'],\n",
              " ['cool'],\n",
              " ['theres', 'too', 'much', 'gear'],\n",
              " ['okay'],\n",
              " ['cant', 'draw'],\n",
              " ['um'],\n",
              " ['yeah'],\n",
              " ['um',\n",
              "  'well',\n",
              "  'anyway',\n",
              "  'dont',\n",
              "  'know',\n",
              "  'its',\n",
              "  'just',\n",
              "  'the',\n",
              "  'first',\n",
              "  'animal',\n",
              "  'can',\n",
              "  'think',\n",
              "  'off',\n",
              "  'the',\n",
              "  'top',\n",
              "  'of',\n",
              "  'my',\n",
              "  'head'],\n",
              " ['um'],\n",
              " ['yes'],\n",
              " ['big', 'reason', 'is', 'cause', 'im', 'allergic', 'to', 'most', 'animals'],\n",
              " ['allergic',\n",
              "  'to',\n",
              "  'animal',\n",
              "  'fur',\n",
              "  'so',\n",
              "  'um',\n",
              "  'fish',\n",
              "  'was',\n",
              "  'natural',\n",
              "  'choice'],\n",
              " ['um', 'yeah', 'and', 'kind', 'of', 'like', 'whales'],\n",
              " ['they', 'come', 'in', 'and', 'go', 'eat', 'everything', 'in', 'sight'],\n",
              " ['and', 'theyre', 'quite', 'harmless', 'and', 'mild', 'and', 'interesting'],\n",
              " ['tails', 'bit', 'big', 'think'],\n",
              " ['its', 'an', 'after', 'dinner', 'dog', 'then'],\n",
              " ['hmm'],\n",
              " ['it',\n",
              "  'does',\n",
              "  'make',\n",
              "  'sense',\n",
              "  'from',\n",
              "  'maybe',\n",
              "  'the',\n",
              "  'design',\n",
              "  'point',\n",
              "  'of',\n",
              "  'view',\n",
              "  'cause',\n",
              "  'you',\n",
              "  'have',\n",
              "  'more',\n",
              "  'complicated',\n",
              "  'characters',\n",
              "  'like',\n",
              "  'european',\n",
              "  'languages',\n",
              "  'then',\n",
              "  'you',\n",
              "  'need',\n",
              "  'more',\n",
              "  'buttons'],\n",
              " ['so', 'possibly'],\n",
              " ['hmm'],\n",
              " ['yeah'],\n",
              " ['and', 'you', 'keep', 'losing', 'them'],\n",
              " ['finding', 'them', 'is', 'really', 'pain', 'you', 'know'],\n",
              " ['mean',\n",
              "  'its',\n",
              "  'usually',\n",
              "  'quite',\n",
              "  'small',\n",
              "  'or',\n",
              "  'when',\n",
              "  'you',\n",
              "  'want',\n",
              "  'it',\n",
              "  'right',\n",
              "  'it',\n",
              "  'slipped',\n",
              "  'behind',\n",
              "  'the',\n",
              "  'couch',\n",
              "  'or',\n",
              "  'its',\n",
              "  'kicked',\n",
              "  'under',\n",
              "  'the',\n",
              "  'table'],\n",
              " ['you', 'know'],\n",
              " ['yep'],\n",
              " ['mm', 'hmm'],\n",
              " ['think', 'one', 'factor', 'would', 'be', 'production', 'cost'],\n",
              " ['because',\n",
              "  'theres',\n",
              "  'cap',\n",
              "  'there',\n",
              "  'so',\n",
              "  'um',\n",
              "  'depends',\n",
              "  'on',\n",
              "  'how',\n",
              "  'much',\n",
              "  'you',\n",
              "  'can',\n",
              "  'cram',\n",
              "  'into',\n",
              "  'that',\n",
              "  'price'],\n",
              " ['um'],\n",
              " ['think', 'that', 'thats', 'the', 'main', 'factor'],\n",
              " ['cool']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDqAWR2kanVg"
      },
      "source": [
        "# убираем стоп-слова\n",
        "res_ = []\n",
        "s_ =[]\n",
        "for s in res:\n",
        "    for word in s:\n",
        "        if word not in stopwords_english:\n",
        "            s_.append(word)\n",
        "    res_.append(s_)\n",
        "    s_ = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K30Fm0kfanVg",
        "outputId": "6d0fce77-0f8f-427f-de2b-ebc4d08384a9"
      },
      "source": [
        "res_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['im', 'craig', 'im', 'user', 'interface'],\n",
              " [],\n",
              " ['favourite', 'animal', 'would', 'monkey'],\n",
              " ['theyre',\n",
              "  'small',\n",
              "  'cute',\n",
              "  'furry',\n",
              "  'planet',\n",
              "  'apes',\n",
              "  'becomes',\n",
              "  'real',\n",
              "  'im',\n",
              "  'gonna'],\n",
              " [],\n",
              " ['know',\n",
              "  'parents',\n",
              "  'went',\n",
              "  'bought',\n",
              "  'remote',\n",
              "  'controls',\n",
              "  'got',\n",
              "  'fed',\n",
              "  'four',\n",
              "  'five',\n",
              "  'different',\n",
              "  'remote',\n",
              "  'controls',\n",
              "  'things',\n",
              "  'house'],\n",
              " ['many', 'devices', 'control'],\n",
              " [],\n",
              " [],\n",
              " ['great'],\n",
              " ['im', 'andrew', 'im', 'marketing', 'expert'],\n",
              " [],\n",
              " [],\n",
              " ['thats', 'thats'],\n",
              " [],\n",
              " ['go'],\n",
              " ['thats', 'fine'],\n",
              " [],\n",
              " ['one', 'right'],\n",
              " [],\n",
              " ['nice'],\n",
              " [],\n",
              " ['favourite', 'animal', 'beagle'],\n",
              " ['charac', 'favourite', 'characteristics'],\n",
              " ['right'],\n",
              " ['right',\n",
              "  'basically',\n",
              "  'high',\n",
              "  'priority',\n",
              "  'animal',\n",
              "  'willing',\n",
              "  'take',\n",
              "  'lot',\n",
              "  'physical',\n",
              "  'affection',\n",
              "  'family'],\n",
              " ['lots', 'personality', 'fit', 'robust', 'good', 'health'],\n",
              " ['blue'],\n",
              " ['blue', 'beagle'],\n",
              " ['familys', 'beagle'],\n",
              " ['coulda', 'told', 'whole', 'lot', 'beagles'],\n",
              " ['boy', 'let', 'tell'],\n",
              " ['impressionist'],\n",
              " [],\n",
              " [],\n",
              " ['superb', 'sketch', 'way'],\n",
              " [],\n",
              " ['see', 'dog'],\n",
              " [],\n",
              " ['see', 'rooster'],\n",
              " ['kind'],\n",
              " ['aware', 'th', 'cha', 'tail', 'hes', 'chasing'],\n",
              " [],\n",
              " ['probably', 'little', 'got', 'lots', 'attention', 'forever', 'conditioned'],\n",
              " [],\n",
              " ['go'],\n",
              " ['bas', 'twel'],\n",
              " [],\n",
              " ['cost',\n",
              "  'production',\n",
              "  'cost',\n",
              "  'twelve',\n",
              "  'fifty',\n",
              "  'selling',\n",
              "  'price',\n",
              "  'wholesale',\n",
              "  'retail'],\n",
              " ['shelf'],\n",
              " ['sale', 'sale', 'anyway'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['yes'],\n",
              " [],\n",
              " [],\n",
              " ['right',\n",
              "  'away',\n",
              "  'im',\n",
              "  'wondering',\n",
              "  'theres',\n",
              "  'th',\n",
              "  'th',\n",
              "  'd_v_d_',\n",
              "  'players',\n",
              "  'zones'],\n",
              " ['frequencies',\n",
              "  'something',\n",
              "  'characters',\n",
              "  'different',\n",
              "  'keypad',\n",
              "  'styles',\n",
              "  'symbols'],\n",
              " [],\n",
              " ['dont', 'know'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['al', 'thing', 'international', 'top', 'price'],\n",
              " ['im',\n",
              "  'thinking',\n",
              "  'price',\n",
              "  'might',\n",
              "  'might',\n",
              "  'appeal',\n",
              "  'certain',\n",
              "  'market',\n",
              "  'one',\n",
              "  'region',\n",
              "  'whereas',\n",
              "  'another',\n",
              "  'itll',\n",
              "  'different',\n",
              "  'chara',\n",
              "  'characteristic',\n",
              "  'basic',\n",
              "  'product',\n",
              "  'podi',\n",
              "  'positioning',\n",
              "  'twenty',\n",
              "  'five',\n",
              "  'euro',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'might',\n",
              "  'big',\n",
              "  'hit',\n",
              "  'london',\n",
              "  'might',\n",
              "  'big',\n",
              "  'hit',\n",
              "  'greece',\n",
              "  'knows',\n",
              "  'something'],\n",
              " [],\n",
              " ['right',\n",
              "  'away',\n",
              "  'im',\n",
              "  'making',\n",
              "  'kind',\n",
              "  'assumptions',\n",
              "  'information',\n",
              "  'given',\n",
              "  'thinking',\n",
              "  'trendy',\n",
              "  'probably',\n",
              "  'means',\n",
              "  'something',\n",
              "  'basic',\n",
              "  'something',\n",
              "  'standard'],\n",
              " ['im',\n",
              "  'wondering',\n",
              "  'right',\n",
              "  'away',\n",
              "  'selling',\n",
              "  'twenty',\n",
              "  'five',\n",
              "  'euros',\n",
              "  'sort',\n",
              "  'thi',\n",
              "  'gonna',\n",
              "  'premium',\n",
              "  'product',\n",
              "  'kinda',\n",
              "  'thing',\n",
              "  'huh'],\n",
              " [],\n",
              " [],\n",
              " ['id', 'say'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['background', 'information', 'compares'],\n",
              " [],\n",
              " ['interesting',\n",
              "  'thing',\n",
              "  'discussing',\n",
              "  'production',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'point',\n",
              "  'dont',\n",
              "  'think',\n",
              "  'remote',\n",
              "  'controls',\n",
              "  'somethin',\n",
              "  'something',\n",
              "  'people',\n",
              "  'consciously',\n",
              "  'assess',\n",
              "  'purchasing',\n",
              "  'habits'],\n",
              " ['getting', 'shoelaces', 'shoes', 'something'],\n",
              " ['comes', 'along'],\n",
              " ['know', 'mean'],\n",
              " ['sort',\n",
              "  'mean',\n",
              "  'one',\n",
              "  'one',\n",
              "  'way',\n",
              "  'looking',\n",
              "  'would',\n",
              "  'people',\n",
              "  'producing',\n",
              "  'television',\n",
              "  'sets',\n",
              "  'maybe',\n",
              "  'buy',\n",
              "  'remote',\n",
              "  'controls'],\n",
              " ['another',\n",
              "  'way',\n",
              "  'maybe',\n",
              "  'people',\n",
              "  't_v_',\n",
              "  'sets',\n",
              "  'really',\n",
              "  'fed',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'really',\n",
              "  'want',\n",
              "  'better',\n",
              "  'one',\n",
              "  'something'],\n",
              " ['right'],\n",
              " ['right'],\n",
              " ['right',\n",
              "  'function',\n",
              "  'one',\n",
              "  'priorities',\n",
              "  'might',\n",
              "  'combine',\n",
              "  'many',\n",
              "  'uses',\n",
              "  'think'],\n",
              " [],\n",
              " [],\n",
              " ['maybe',\n",
              "  'could',\n",
              "  'use',\n",
              "  'sort',\n",
              "  'example',\n",
              "  'successful',\n",
              "  'piece',\n",
              "  'technology',\n",
              "  'palm',\n",
              "  'palm',\n",
              "  'pilots'],\n",
              " ['theyre',\n",
              "  'gone',\n",
              "  'little',\n",
              "  'sort',\n",
              "  'scribble',\n",
              "  'boards',\n",
              "  'cameras',\n",
              "  'm_p_',\n",
              "  'three',\n",
              "  'players',\n",
              "  'telephones',\n",
              "  'everything',\n",
              "  'agenda'],\n",
              " ['wonder',\n",
              "  'might',\n",
              "  'add',\n",
              "  'something',\n",
              "  'new',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'market',\n",
              "  'lighting',\n",
              "  'house'],\n",
              " [],\n",
              " ['personally',\n",
              "  'home',\n",
              "  'combined',\n",
              "  'audio',\n",
              "  'video',\n",
              "  'television',\n",
              "  'set',\n",
              "  'd_v_d_',\n",
              "  'player',\n",
              "  'c_d_',\n",
              "  'player'],\n",
              " ['work',\n",
              "  'actually',\n",
              "  'function',\n",
              "  'together',\n",
              "  'different',\n",
              "  'remote',\n",
              "  'controls'],\n",
              " ['sort', 'ironic', 'theyre', 'know', 'sound', 'everything', 'one', 'system'],\n",
              " ['ones', 'got', 'little', 'part'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['thats', 'really', 'good', 'id'],\n",
              " ['sure'],\n",
              " ['remember', 'first', 'remote', 'control', 'family', 'cable'],\n",
              " ['actually',\n",
              "  'cable',\n",
              "  't_v_',\n",
              "  'big',\n",
              "  'buttons',\n",
              "  'sort',\n",
              "  'blender',\n",
              "  'something'],\n",
              " ['know',\n",
              "  'think',\n",
              "  'better',\n",
              "  'actually',\n",
              "  'still',\n",
              "  'kind',\n",
              "  'dunno',\n",
              "  'massive',\n",
              "  'junky',\n",
              "  'thing',\n",
              "  'table'],\n",
              " ['maybe', 'could', 'think', 'could', 'know', 'streamlined'],\n",
              " ['something'],\n",
              " ['whatever', 'would', 'technologically', 'reasonable'],\n",
              " ['cause',\n",
              "  'could',\n",
              "  'could',\n",
              "  'could',\n",
              "  'could',\n",
              "  'functionally',\n",
              "  'doesnt',\n",
              "  'make',\n",
              "  'better',\n",
              "  'appeal',\n",
              "  'know',\n",
              "  'days',\n",
              "  'theres',\n",
              "  'pe',\n",
              "  'things',\n",
              "  'peoples',\n",
              "  'homes',\n",
              "  'becoming',\n",
              "  'chic',\n",
              "  'know'],\n",
              " ['nicer', 'materials', 'might', 'worth', 'exploring', 'anyway'],\n",
              " [],\n",
              " [],\n",
              " ['wrap',\n",
              "  'make',\n",
              "  'sure',\n",
              "  'page',\n",
              "  'given',\n",
              "  'sort',\n",
              "  'example',\n",
              "  'coffee',\n",
              "  'machine',\n",
              "  'something',\n",
              "  'right'],\n",
              " ['right',\n",
              "  'assumption',\n",
              "  'television',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'may',\n",
              "  'features',\n",
              "  'go',\n",
              "  'beyond',\n",
              "  'television'],\n",
              " ['keeping', 'sort', 'design', 'commitment', 'television', 'features'],\n",
              " ['dont', 'know'],\n",
              " [],\n",
              " ['sure'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['right'],\n",
              " ['kick', 'meeting', 'project'],\n",
              " ['gonna', 'next', 'twenty', 'five', 'minutes'],\n",
              " ['first',\n",
              "  'kind',\n",
              "  'make',\n",
              "  'sure',\n",
              "  'know',\n",
              "  'im',\n",
              "  'laura',\n",
              "  'im',\n",
              "  'project',\n",
              "  'manager'],\n",
              " ['want', 'introduce'],\n",
              " [],\n",
              " ['great'],\n",
              " [],\n",
              " ['designing', 'new', 'remote', 'control', 'oh', 'record', 'whos', 'actually'],\n",
              " ['thats', 'david', 'andrew', 'craig', 'isnt'],\n",
              " ['arrived', 'time'],\n",
              " ['des', 'design', 'new', 'remote', 'control'],\n",
              " ['see', 'supposed', 'original', 'trendy', 'user', 'friendly'],\n",
              " ['thats', 'kind', 'brief'],\n",
              " ['three', 'different', 'stages', 'design'],\n",
              " ['im', 'really', 'sure', 'guys', 'already', 'received', 'emails'],\n",
              " ['get'],\n",
              " [],\n",
              " ['everybody', 'got'],\n",
              " [],\n",
              " [],\n",
              " ['gonna', 'individual', 'work', 'meeting'],\n",
              " ['repeat', 'process', 'three', 'times'],\n",
              " ['point', 'get', 'try', 'whiteboard'],\n",
              " [],\n",
              " ['get', 'draw', 'favourite', 'animal', 'sum', 'favourite', 'characteristics'],\n",
              " ['would', 'go', 'first'],\n",
              " ['good'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['right'],\n",
              " ['lovely'],\n",
              " ['right'],\n",
              " ['take', 'long', 'havent', 'got', 'awful', 'lot', 'discuss'],\n",
              " ['ok', 'oh'],\n",
              " ['dont', 'feel', 'youre', 'rush', 'anyway'],\n",
              " ['ach', 'might', 'get'],\n",
              " ['dont', 'know', 'mine'],\n",
              " ['im', 'gonna', 'think', 'spot'],\n",
              " ['whale'],\n",
              " ['ah'],\n",
              " [],\n",
              " ['god', 'still', 'dont', 'know', 'im', 'gonna', 'write'],\n",
              " [],\n",
              " ['gonna', 'choose', 'dog'],\n",
              " ['ill', 'draw', 'different', 'kind', 'dog'],\n",
              " ['favourite', 'animal', 'dog', 'home'],\n",
              " ['doesnt', 'really', 'look', 'actually'],\n",
              " ['looks', 'pig', 'actually'],\n",
              " ['ah'],\n",
              " [],\n",
              " ['oh', 'thats', 'good'],\n",
              " [],\n",
              " ['hes', 'mixture', 'various', 'things'],\n",
              " ['thats', 'suggest', 'tail', 'wags'],\n",
              " ['hes',\n",
              "  'friendly',\n",
              "  'cheery',\n",
              "  'always',\n",
              "  'pleased',\n",
              "  'see',\n",
              "  'kind',\n",
              "  'affectionate',\n",
              "  'hes',\n",
              "  'quite',\n",
              "  'quite',\n",
              "  'wee',\n",
              "  'know',\n",
              "  'doesnt',\n",
              "  'take',\n",
              "  'much',\n",
              "  'space'],\n",
              " ['funny', 'thing', 'chases', 'tail', 'quite', 'amusing'],\n",
              " ['think'],\n",
              " ['hes',\n",
              "  'dinner',\n",
              "  'hell',\n",
              "  'sudden',\n",
              "  'get',\n",
              "  'start',\n",
              "  'chasing',\n",
              "  'tail',\n",
              "  'round',\n",
              "  'living',\n",
              "  'room'],\n",
              " ['maybe'],\n",
              " ['maybe'],\n",
              " ['right', 'find'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['next'],\n",
              " [],\n",
              " ['need', 'discuss', 'project', 'finance'],\n",
              " ['according',\n",
              "  'brief',\n",
              "  'gonna',\n",
              "  'selling',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'twenty',\n",
              "  'five',\n",
              "  'euro',\n",
              "  'aiming',\n",
              "  'make',\n",
              "  'fifty',\n",
              "  'million',\n",
              "  'euro'],\n",
              " ['gonna', 'selling', 'international', 'scale'],\n",
              " ['dont',\n",
              "  'want',\n",
              "  'cost',\n",
              "  'twelve',\n",
              "  'fifty',\n",
              "  'euros',\n",
              "  'fifty',\n",
              "  'percent',\n",
              "  'selling',\n",
              "  'price'],\n",
              " ['sure'],\n",
              " ['together'],\n",
              " ['dunno'],\n",
              " ['imagine', 'thats', 'good', 'question'],\n",
              " ['imagine',\n",
              "  'probably',\n",
              "  'sale',\n",
              "  'actually',\n",
              "  'probably',\n",
              "  'retailer',\n",
              "  'sell',\n",
              "  'whatever',\n",
              "  'price',\n",
              "  'want'],\n",
              " [],\n",
              " ['dont',\n",
              "  'know',\n",
              "  'mean',\n",
              "  'think',\n",
              "  'fact',\n",
              "  'going',\n",
              "  'sold',\n",
              "  'internationally',\n",
              "  'bearing',\n",
              "  'design'],\n",
              " ['think'],\n",
              " [],\n",
              " [],\n",
              " ['oh', 'regions', 'stuff'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['remote',\n",
              "  'control',\n",
              "  'think',\n",
              "  'suppose',\n",
              "  'depends',\n",
              "  'complicated',\n",
              "  'remote',\n",
              "  'control'],\n",
              " [],\n",
              " [],\n",
              " ['terms', 'wealth', 'country'],\n",
              " ['much', 'money', 'people', 'spend', 'things'],\n",
              " ['aye', 'see', 'mean'],\n",
              " ['marketing'],\n",
              " ['good', 'marketing', 'thoughts'],\n",
              " ['oh', 'writing'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['much', 'know', 'remote', 'control', 'cost'],\n",
              " ['twenty',\n",
              "  'five',\n",
              "  'euro',\n",
              "  'mean',\n",
              "  'thats',\n",
              "  'thats',\n",
              "  'eighteen',\n",
              "  'pounds',\n",
              "  'something',\n",
              "  'isnt'],\n",
              " ['much'],\n",
              " ['sixteen', 'seventeen', 'eighteen', 'pounds'],\n",
              " ['dunno',\n",
              "  'never',\n",
              "  'bought',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'dont',\n",
              "  'know',\n",
              "  'good',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'would',\n",
              "  'get'],\n",
              " [],\n",
              " ['suppose', 'look', 'kind', 'cool', 'gimmicky'],\n",
              " ['right'],\n",
              " ['let', 'scoot', 'ahead'],\n",
              " [],\n",
              " ['anybody', 'anything', 'add', 'finance', 'issue'],\n",
              " ['thin', 'actually'],\n",
              " ['would', 'useful', 'though', 'wouldnt', 'knew', 'money', 'would', 'get'],\n",
              " [],\n",
              " [],\n",
              " ['oh'],\n",
              " ['five', 'minutes', 'end', 'meeting'],\n",
              " ['oh'],\n",
              " ['bit', 'behind'],\n",
              " [],\n",
              " ['right',\n",
              "  'think',\n",
              "  'main',\n",
              "  'design',\n",
              "  'aim',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'know',\n",
              "  'satellite',\n",
              "  'regular',\n",
              "  'telly',\n",
              "  'v_c_r_',\n",
              "  'everything'],\n",
              " [],\n",
              " [],\n",
              " ['even', 'know', 'notes', 'wanna', 'watch'],\n",
              " ['might',\n",
              "  'put',\n",
              "  'oh',\n",
              "  'want',\n",
              "  'watch',\n",
              "  'look',\n",
              "  'oh',\n",
              "  'thats',\n",
              "  'good',\n",
              "  'idea'],\n",
              " ['extra', 'functionalities'],\n",
              " [],\n",
              " [],\n",
              " ['id',\n",
              "  'wel',\n",
              "  'gonna',\n",
              "  'wrap',\n",
              "  'pretty',\n",
              "  'quickly',\n",
              "  'next',\n",
              "  'couple',\n",
              "  'minutes'],\n",
              " ['ill', 'check', 'nothing', 'else'],\n",
              " [],\n",
              " ['anything',\n",
              "  'else',\n",
              "  'anybody',\n",
              "  'wants',\n",
              "  'add',\n",
              "  'dont',\n",
              "  'remote',\n",
              "  'controls',\n",
              "  'theyve',\n",
              "  'used',\n",
              "  'would',\n",
              "  'really',\n",
              "  'part',\n",
              "  'new',\n",
              "  'one'],\n",
              " ['keep', 'losing'],\n",
              " [],\n",
              " [],\n",
              " ['get',\n",
              "  'ones',\n",
              "  'whistle',\n",
              "  'make',\n",
              "  'really',\n",
              "  'high',\n",
              "  'pitched',\n",
              "  'noise',\n",
              "  'beep'],\n",
              " ['mean', 'something', 'wed', 'want', 'include', 'think'],\n",
              " ['dunno'],\n",
              " ['maybe'],\n",
              " ['goodness'],\n",
              " ['still', 'feels', 'quite', 'primitive'],\n",
              " ['maybe', 'touch', 'screen', 'something'],\n",
              " [],\n",
              " ['huh'],\n",
              " ['guess', 'thats', 'industrial', 'designer'],\n",
              " ['looks', 'better'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['right', 'wrap', 'next', 'meetings', 'gonna', 'thirty', 'minutes'],\n",
              " ['thats', 'ten', 'twelve', 'watch'],\n",
              " ['inbetween',\n",
              "  'industrial',\n",
              "  'designer',\n",
              "  'youre',\n",
              "  'gonna',\n",
              "  'working',\n",
              "  'know',\n",
              "  'actual',\n",
              "  'working',\n",
              "  'design',\n",
              "  'know',\n",
              "  'youre'],\n",
              " ['user',\n",
              "  'interface',\n",
              "  'technical',\n",
              "  'functions',\n",
              "  'guess',\n",
              "  'thats',\n",
              "  'know',\n",
              "  'talking',\n",
              "  'itll',\n",
              "  'actually'],\n",
              " ['marketing',\n",
              "  'executive',\n",
              "  'youll',\n",
              "  'thinking',\n",
              "  'actually',\n",
              "  'know',\n",
              "  'requirements',\n",
              "  'fulfil',\n",
              "  'youll',\n",
              "  'get',\n",
              "  'instructions',\n",
              "  'emailed',\n",
              "  'guess'],\n",
              " [],\n",
              " ['th', 'functional', 'design', 'stage', 'next', 'guess'],\n",
              " ['thats', 'end', 'meeting'],\n",
              " ['got', 'little', 'message', 'lot', 'sooner', 'thought', 'would'],\n",
              " ['huh'],\n",
              " ['th', 'quickly', 'cause', 'supposed', 'finish'],\n",
              " ['guess',\n",
              "  'thats',\n",
              "  'us',\n",
              "  'mean',\n",
              "  'probably',\n",
              "  'want',\n",
              "  'kind',\n",
              "  'unique',\n",
              "  'selling',\n",
              "  'point',\n",
              "  'know'],\n",
              " [],\n",
              " [],\n",
              " [],\n",
              " ['right', 'thats', 'thats', 'end', 'meeting'],\n",
              " [],\n",
              " ['thank', 'coming'],\n",
              " ['hi', 'im', 'david', 'im', 'supposed', 'industrial', 'designer'],\n",
              " ['got', 'project', 'announcement', 'project'],\n",
              " ['designing', 'remote', 'control'],\n",
              " ['thats', 'didnt', 'get', 'anything', 'else'],\n",
              " ['get', 'thing'],\n",
              " ['cool'],\n",
              " ['theres', 'much', 'gear'],\n",
              " [],\n",
              " ['cant', 'draw'],\n",
              " [],\n",
              " [],\n",
              " ['anyway', 'dont', 'know', 'first', 'animal', 'think', 'top', 'head'],\n",
              " [],\n",
              " ['yes'],\n",
              " ['big', 'reason', 'cause', 'im', 'allergic', 'animals'],\n",
              " ['allergic', 'animal', 'fur', 'fish', 'natural', 'choice'],\n",
              " ['kind', 'whales'],\n",
              " ['come', 'go', 'eat', 'everything', 'sight'],\n",
              " ['theyre', 'quite', 'harmless', 'mild', 'interesting'],\n",
              " ['tails', 'bit', 'big', 'think'],\n",
              " ['dinner', 'dog'],\n",
              " [],\n",
              " ['make',\n",
              "  'sense',\n",
              "  'maybe',\n",
              "  'design',\n",
              "  'point',\n",
              "  'view',\n",
              "  'cause',\n",
              "  'complicated',\n",
              "  'characters',\n",
              "  'european',\n",
              "  'languages',\n",
              "  'need',\n",
              "  'buttons'],\n",
              " ['possibly'],\n",
              " [],\n",
              " [],\n",
              " ['keep', 'losing'],\n",
              " ['finding', 'really', 'pain', 'know'],\n",
              " ['mean',\n",
              "  'usually',\n",
              "  'quite',\n",
              "  'small',\n",
              "  'want',\n",
              "  'right',\n",
              "  'slipped',\n",
              "  'behind',\n",
              "  'couch',\n",
              "  'kicked',\n",
              "  'table'],\n",
              " ['know'],\n",
              " [],\n",
              " [],\n",
              " ['think', 'one', 'factor', 'would', 'production', 'cost'],\n",
              " ['theres', 'cap', 'depends', 'much', 'cram', 'price'],\n",
              " [],\n",
              " ['think', 'thats', 'main', 'factor'],\n",
              " ['cool']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIY58p9JfJwh",
        "outputId": "7a67e71c-f1b9-4120-ff14-16c217e7e6d2"
      },
      "source": [
        "# лемматизация \n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /Users/alina/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijIrYs3YanVh",
        "scrolled": true
      },
      "source": [
        "res_fin = []\n",
        "s_fin = []\n",
        "for s in res_:\n",
        "    for word in s:\n",
        "        s_fin.append(lemmatizer.lemmatize(word))\n",
        "    # проверка на пустой список \n",
        "    if s_fin != []:\n",
        "        res_fin.append(s_fin)\n",
        "    s_fin = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP77s8pTanVi",
        "outputId": "56da2c64-baac-4197-e29a-5d11b4bf94c3"
      },
      "source": [
        "res_fin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['im', 'craig', 'im', 'user', 'interface'],\n",
              " ['favourite', 'animal', 'would', 'monkey'],\n",
              " ['theyre',\n",
              "  'small',\n",
              "  'cute',\n",
              "  'furry',\n",
              "  'planet',\n",
              "  'ape',\n",
              "  'becomes',\n",
              "  'real',\n",
              "  'im',\n",
              "  'gonna'],\n",
              " ['know',\n",
              "  'parent',\n",
              "  'went',\n",
              "  'bought',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'got',\n",
              "  'fed',\n",
              "  'four',\n",
              "  'five',\n",
              "  'different',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'thing',\n",
              "  'house'],\n",
              " ['many', 'device', 'control'],\n",
              " ['great'],\n",
              " ['im', 'andrew', 'im', 'marketing', 'expert'],\n",
              " ['thats', 'thats'],\n",
              " ['go'],\n",
              " ['thats', 'fine'],\n",
              " ['one', 'right'],\n",
              " ['nice'],\n",
              " ['favourite', 'animal', 'beagle'],\n",
              " ['charac', 'favourite', 'characteristic'],\n",
              " ['right'],\n",
              " ['right',\n",
              "  'basically',\n",
              "  'high',\n",
              "  'priority',\n",
              "  'animal',\n",
              "  'willing',\n",
              "  'take',\n",
              "  'lot',\n",
              "  'physical',\n",
              "  'affection',\n",
              "  'family'],\n",
              " ['lot', 'personality', 'fit', 'robust', 'good', 'health'],\n",
              " ['blue'],\n",
              " ['blue', 'beagle'],\n",
              " ['family', 'beagle'],\n",
              " ['coulda', 'told', 'whole', 'lot', 'beagle'],\n",
              " ['boy', 'let', 'tell'],\n",
              " ['impressionist'],\n",
              " ['superb', 'sketch', 'way'],\n",
              " ['see', 'dog'],\n",
              " ['see', 'rooster'],\n",
              " ['kind'],\n",
              " ['aware', 'th', 'cha', 'tail', 'he', 'chasing'],\n",
              " ['probably', 'little', 'got', 'lot', 'attention', 'forever', 'conditioned'],\n",
              " ['go'],\n",
              " ['ba', 'twel'],\n",
              " ['cost',\n",
              "  'production',\n",
              "  'cost',\n",
              "  'twelve',\n",
              "  'fifty',\n",
              "  'selling',\n",
              "  'price',\n",
              "  'wholesale',\n",
              "  'retail'],\n",
              " ['shelf'],\n",
              " ['sale', 'sale', 'anyway'],\n",
              " ['yes'],\n",
              " ['right',\n",
              "  'away',\n",
              "  'im',\n",
              "  'wondering',\n",
              "  'there',\n",
              "  'th',\n",
              "  'th',\n",
              "  'd_v_d_',\n",
              "  'player',\n",
              "  'zone'],\n",
              " ['frequency',\n",
              "  'something',\n",
              "  'character',\n",
              "  'different',\n",
              "  'keypad',\n",
              "  'style',\n",
              "  'symbol'],\n",
              " ['dont', 'know'],\n",
              " ['al', 'thing', 'international', 'top', 'price'],\n",
              " ['im',\n",
              "  'thinking',\n",
              "  'price',\n",
              "  'might',\n",
              "  'might',\n",
              "  'appeal',\n",
              "  'certain',\n",
              "  'market',\n",
              "  'one',\n",
              "  'region',\n",
              "  'whereas',\n",
              "  'another',\n",
              "  'itll',\n",
              "  'different',\n",
              "  'chara',\n",
              "  'characteristic',\n",
              "  'basic',\n",
              "  'product',\n",
              "  'podi',\n",
              "  'positioning',\n",
              "  'twenty',\n",
              "  'five',\n",
              "  'euro',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'might',\n",
              "  'big',\n",
              "  'hit',\n",
              "  'london',\n",
              "  'might',\n",
              "  'big',\n",
              "  'hit',\n",
              "  'greece',\n",
              "  'know',\n",
              "  'something'],\n",
              " ['right',\n",
              "  'away',\n",
              "  'im',\n",
              "  'making',\n",
              "  'kind',\n",
              "  'assumption',\n",
              "  'information',\n",
              "  'given',\n",
              "  'thinking',\n",
              "  'trendy',\n",
              "  'probably',\n",
              "  'mean',\n",
              "  'something',\n",
              "  'basic',\n",
              "  'something',\n",
              "  'standard'],\n",
              " ['im',\n",
              "  'wondering',\n",
              "  'right',\n",
              "  'away',\n",
              "  'selling',\n",
              "  'twenty',\n",
              "  'five',\n",
              "  'euro',\n",
              "  'sort',\n",
              "  'thi',\n",
              "  'gonna',\n",
              "  'premium',\n",
              "  'product',\n",
              "  'kinda',\n",
              "  'thing',\n",
              "  'huh'],\n",
              " ['id', 'say'],\n",
              " ['background', 'information', 'compare'],\n",
              " ['interesting',\n",
              "  'thing',\n",
              "  'discussing',\n",
              "  'production',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'point',\n",
              "  'dont',\n",
              "  'think',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'somethin',\n",
              "  'something',\n",
              "  'people',\n",
              "  'consciously',\n",
              "  'ass',\n",
              "  'purchasing',\n",
              "  'habit'],\n",
              " ['getting', 'shoelace', 'shoe', 'something'],\n",
              " ['come', 'along'],\n",
              " ['know', 'mean'],\n",
              " ['sort',\n",
              "  'mean',\n",
              "  'one',\n",
              "  'one',\n",
              "  'way',\n",
              "  'looking',\n",
              "  'would',\n",
              "  'people',\n",
              "  'producing',\n",
              "  'television',\n",
              "  'set',\n",
              "  'maybe',\n",
              "  'buy',\n",
              "  'remote',\n",
              "  'control'],\n",
              " ['another',\n",
              "  'way',\n",
              "  'maybe',\n",
              "  'people',\n",
              "  't_v_',\n",
              "  'set',\n",
              "  'really',\n",
              "  'fed',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'really',\n",
              "  'want',\n",
              "  'better',\n",
              "  'one',\n",
              "  'something'],\n",
              " ['right'],\n",
              " ['right'],\n",
              " ['right',\n",
              "  'function',\n",
              "  'one',\n",
              "  'priority',\n",
              "  'might',\n",
              "  'combine',\n",
              "  'many',\n",
              "  'us',\n",
              "  'think'],\n",
              " ['maybe',\n",
              "  'could',\n",
              "  'use',\n",
              "  'sort',\n",
              "  'example',\n",
              "  'successful',\n",
              "  'piece',\n",
              "  'technology',\n",
              "  'palm',\n",
              "  'palm',\n",
              "  'pilot'],\n",
              " ['theyre',\n",
              "  'gone',\n",
              "  'little',\n",
              "  'sort',\n",
              "  'scribble',\n",
              "  'board',\n",
              "  'camera',\n",
              "  'm_p_',\n",
              "  'three',\n",
              "  'player',\n",
              "  'telephone',\n",
              "  'everything',\n",
              "  'agenda'],\n",
              " ['wonder',\n",
              "  'might',\n",
              "  'add',\n",
              "  'something',\n",
              "  'new',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'market',\n",
              "  'lighting',\n",
              "  'house'],\n",
              " ['personally',\n",
              "  'home',\n",
              "  'combined',\n",
              "  'audio',\n",
              "  'video',\n",
              "  'television',\n",
              "  'set',\n",
              "  'd_v_d_',\n",
              "  'player',\n",
              "  'c_d_',\n",
              "  'player'],\n",
              " ['work',\n",
              "  'actually',\n",
              "  'function',\n",
              "  'together',\n",
              "  'different',\n",
              "  'remote',\n",
              "  'control'],\n",
              " ['sort', 'ironic', 'theyre', 'know', 'sound', 'everything', 'one', 'system'],\n",
              " ['one', 'got', 'little', 'part'],\n",
              " ['thats', 'really', 'good', 'id'],\n",
              " ['sure'],\n",
              " ['remember', 'first', 'remote', 'control', 'family', 'cable'],\n",
              " ['actually',\n",
              "  'cable',\n",
              "  't_v_',\n",
              "  'big',\n",
              "  'button',\n",
              "  'sort',\n",
              "  'blender',\n",
              "  'something'],\n",
              " ['know',\n",
              "  'think',\n",
              "  'better',\n",
              "  'actually',\n",
              "  'still',\n",
              "  'kind',\n",
              "  'dunno',\n",
              "  'massive',\n",
              "  'junky',\n",
              "  'thing',\n",
              "  'table'],\n",
              " ['maybe', 'could', 'think', 'could', 'know', 'streamlined'],\n",
              " ['something'],\n",
              " ['whatever', 'would', 'technologically', 'reasonable'],\n",
              " ['cause',\n",
              "  'could',\n",
              "  'could',\n",
              "  'could',\n",
              "  'could',\n",
              "  'functionally',\n",
              "  'doesnt',\n",
              "  'make',\n",
              "  'better',\n",
              "  'appeal',\n",
              "  'know',\n",
              "  'day',\n",
              "  'there',\n",
              "  'pe',\n",
              "  'thing',\n",
              "  'people',\n",
              "  'home',\n",
              "  'becoming',\n",
              "  'chic',\n",
              "  'know'],\n",
              " ['nicer', 'material', 'might', 'worth', 'exploring', 'anyway'],\n",
              " ['wrap',\n",
              "  'make',\n",
              "  'sure',\n",
              "  'page',\n",
              "  'given',\n",
              "  'sort',\n",
              "  'example',\n",
              "  'coffee',\n",
              "  'machine',\n",
              "  'something',\n",
              "  'right'],\n",
              " ['right',\n",
              "  'assumption',\n",
              "  'television',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'may',\n",
              "  'feature',\n",
              "  'go',\n",
              "  'beyond',\n",
              "  'television'],\n",
              " ['keeping', 'sort', 'design', 'commitment', 'television', 'feature'],\n",
              " ['dont', 'know'],\n",
              " ['sure'],\n",
              " ['right'],\n",
              " ['kick', 'meeting', 'project'],\n",
              " ['gonna', 'next', 'twenty', 'five', 'minute'],\n",
              " ['first',\n",
              "  'kind',\n",
              "  'make',\n",
              "  'sure',\n",
              "  'know',\n",
              "  'im',\n",
              "  'laura',\n",
              "  'im',\n",
              "  'project',\n",
              "  'manager'],\n",
              " ['want', 'introduce'],\n",
              " ['great'],\n",
              " ['designing', 'new', 'remote', 'control', 'oh', 'record', 'who', 'actually'],\n",
              " ['thats', 'david', 'andrew', 'craig', 'isnt'],\n",
              " ['arrived', 'time'],\n",
              " ['de', 'design', 'new', 'remote', 'control'],\n",
              " ['see', 'supposed', 'original', 'trendy', 'user', 'friendly'],\n",
              " ['thats', 'kind', 'brief'],\n",
              " ['three', 'different', 'stage', 'design'],\n",
              " ['im', 'really', 'sure', 'guy', 'already', 'received', 'email'],\n",
              " ['get'],\n",
              " ['everybody', 'got'],\n",
              " ['gonna', 'individual', 'work', 'meeting'],\n",
              " ['repeat', 'process', 'three', 'time'],\n",
              " ['point', 'get', 'try', 'whiteboard'],\n",
              " ['get', 'draw', 'favourite', 'animal', 'sum', 'favourite', 'characteristic'],\n",
              " ['would', 'go', 'first'],\n",
              " ['good'],\n",
              " ['right'],\n",
              " ['lovely'],\n",
              " ['right'],\n",
              " ['take', 'long', 'havent', 'got', 'awful', 'lot', 'discus'],\n",
              " ['ok', 'oh'],\n",
              " ['dont', 'feel', 'youre', 'rush', 'anyway'],\n",
              " ['ach', 'might', 'get'],\n",
              " ['dont', 'know', 'mine'],\n",
              " ['im', 'gonna', 'think', 'spot'],\n",
              " ['whale'],\n",
              " ['ah'],\n",
              " ['god', 'still', 'dont', 'know', 'im', 'gonna', 'write'],\n",
              " ['gonna', 'choose', 'dog'],\n",
              " ['ill', 'draw', 'different', 'kind', 'dog'],\n",
              " ['favourite', 'animal', 'dog', 'home'],\n",
              " ['doesnt', 'really', 'look', 'actually'],\n",
              " ['look', 'pig', 'actually'],\n",
              " ['ah'],\n",
              " ['oh', 'thats', 'good'],\n",
              " ['he', 'mixture', 'various', 'thing'],\n",
              " ['thats', 'suggest', 'tail', 'wag'],\n",
              " ['he',\n",
              "  'friendly',\n",
              "  'cheery',\n",
              "  'always',\n",
              "  'pleased',\n",
              "  'see',\n",
              "  'kind',\n",
              "  'affectionate',\n",
              "  'he',\n",
              "  'quite',\n",
              "  'quite',\n",
              "  'wee',\n",
              "  'know',\n",
              "  'doesnt',\n",
              "  'take',\n",
              "  'much',\n",
              "  'space'],\n",
              " ['funny', 'thing', 'chase', 'tail', 'quite', 'amusing'],\n",
              " ['think'],\n",
              " ['he',\n",
              "  'dinner',\n",
              "  'hell',\n",
              "  'sudden',\n",
              "  'get',\n",
              "  'start',\n",
              "  'chasing',\n",
              "  'tail',\n",
              "  'round',\n",
              "  'living',\n",
              "  'room'],\n",
              " ['maybe'],\n",
              " ['maybe'],\n",
              " ['right', 'find'],\n",
              " ['next'],\n",
              " ['need', 'discus', 'project', 'finance'],\n",
              " ['according',\n",
              "  'brief',\n",
              "  'gonna',\n",
              "  'selling',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'twenty',\n",
              "  'five',\n",
              "  'euro',\n",
              "  'aiming',\n",
              "  'make',\n",
              "  'fifty',\n",
              "  'million',\n",
              "  'euro'],\n",
              " ['gonna', 'selling', 'international', 'scale'],\n",
              " ['dont',\n",
              "  'want',\n",
              "  'cost',\n",
              "  'twelve',\n",
              "  'fifty',\n",
              "  'euro',\n",
              "  'fifty',\n",
              "  'percent',\n",
              "  'selling',\n",
              "  'price'],\n",
              " ['sure'],\n",
              " ['together'],\n",
              " ['dunno'],\n",
              " ['imagine', 'thats', 'good', 'question'],\n",
              " ['imagine',\n",
              "  'probably',\n",
              "  'sale',\n",
              "  'actually',\n",
              "  'probably',\n",
              "  'retailer',\n",
              "  'sell',\n",
              "  'whatever',\n",
              "  'price',\n",
              "  'want'],\n",
              " ['dont',\n",
              "  'know',\n",
              "  'mean',\n",
              "  'think',\n",
              "  'fact',\n",
              "  'going',\n",
              "  'sold',\n",
              "  'internationally',\n",
              "  'bearing',\n",
              "  'design'],\n",
              " ['think'],\n",
              " ['oh', 'region', 'stuff'],\n",
              " ['remote',\n",
              "  'control',\n",
              "  'think',\n",
              "  'suppose',\n",
              "  'depends',\n",
              "  'complicated',\n",
              "  'remote',\n",
              "  'control'],\n",
              " ['term', 'wealth', 'country'],\n",
              " ['much', 'money', 'people', 'spend', 'thing'],\n",
              " ['aye', 'see', 'mean'],\n",
              " ['marketing'],\n",
              " ['good', 'marketing', 'thought'],\n",
              " ['oh', 'writing'],\n",
              " ['much', 'know', 'remote', 'control', 'cost'],\n",
              " ['twenty',\n",
              "  'five',\n",
              "  'euro',\n",
              "  'mean',\n",
              "  'thats',\n",
              "  'thats',\n",
              "  'eighteen',\n",
              "  'pound',\n",
              "  'something',\n",
              "  'isnt'],\n",
              " ['much'],\n",
              " ['sixteen', 'seventeen', 'eighteen', 'pound'],\n",
              " ['dunno',\n",
              "  'never',\n",
              "  'bought',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'dont',\n",
              "  'know',\n",
              "  'good',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'would',\n",
              "  'get'],\n",
              " ['suppose', 'look', 'kind', 'cool', 'gimmicky'],\n",
              " ['right'],\n",
              " ['let', 'scoot', 'ahead'],\n",
              " ['anybody', 'anything', 'add', 'finance', 'issue'],\n",
              " ['thin', 'actually'],\n",
              " ['would', 'useful', 'though', 'wouldnt', 'knew', 'money', 'would', 'get'],\n",
              " ['oh'],\n",
              " ['five', 'minute', 'end', 'meeting'],\n",
              " ['oh'],\n",
              " ['bit', 'behind'],\n",
              " ['right',\n",
              "  'think',\n",
              "  'main',\n",
              "  'design',\n",
              "  'aim',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'know',\n",
              "  'satellite',\n",
              "  'regular',\n",
              "  'telly',\n",
              "  'v_c_r_',\n",
              "  'everything'],\n",
              " ['even', 'know', 'note', 'wanna', 'watch'],\n",
              " ['might',\n",
              "  'put',\n",
              "  'oh',\n",
              "  'want',\n",
              "  'watch',\n",
              "  'look',\n",
              "  'oh',\n",
              "  'thats',\n",
              "  'good',\n",
              "  'idea'],\n",
              " ['extra', 'functionality'],\n",
              " ['id',\n",
              "  'wel',\n",
              "  'gonna',\n",
              "  'wrap',\n",
              "  'pretty',\n",
              "  'quickly',\n",
              "  'next',\n",
              "  'couple',\n",
              "  'minute'],\n",
              " ['ill', 'check', 'nothing', 'else'],\n",
              " ['anything',\n",
              "  'else',\n",
              "  'anybody',\n",
              "  'want',\n",
              "  'add',\n",
              "  'dont',\n",
              "  'remote',\n",
              "  'control',\n",
              "  'theyve',\n",
              "  'used',\n",
              "  'would',\n",
              "  'really',\n",
              "  'part',\n",
              "  'new',\n",
              "  'one'],\n",
              " ['keep', 'losing'],\n",
              " ['get',\n",
              "  'one',\n",
              "  'whistle',\n",
              "  'make',\n",
              "  'really',\n",
              "  'high',\n",
              "  'pitched',\n",
              "  'noise',\n",
              "  'beep'],\n",
              " ['mean', 'something', 'wed', 'want', 'include', 'think'],\n",
              " ['dunno'],\n",
              " ['maybe'],\n",
              " ['goodness'],\n",
              " ['still', 'feel', 'quite', 'primitive'],\n",
              " ['maybe', 'touch', 'screen', 'something'],\n",
              " ['huh'],\n",
              " ['guess', 'thats', 'industrial', 'designer'],\n",
              " ['look', 'better'],\n",
              " ['right', 'wrap', 'next', 'meeting', 'gonna', 'thirty', 'minute'],\n",
              " ['thats', 'ten', 'twelve', 'watch'],\n",
              " ['inbetween',\n",
              "  'industrial',\n",
              "  'designer',\n",
              "  'youre',\n",
              "  'gonna',\n",
              "  'working',\n",
              "  'know',\n",
              "  'actual',\n",
              "  'working',\n",
              "  'design',\n",
              "  'know',\n",
              "  'youre'],\n",
              " ['user',\n",
              "  'interface',\n",
              "  'technical',\n",
              "  'function',\n",
              "  'guess',\n",
              "  'thats',\n",
              "  'know',\n",
              "  'talking',\n",
              "  'itll',\n",
              "  'actually'],\n",
              " ['marketing',\n",
              "  'executive',\n",
              "  'youll',\n",
              "  'thinking',\n",
              "  'actually',\n",
              "  'know',\n",
              "  'requirement',\n",
              "  'fulfil',\n",
              "  'youll',\n",
              "  'get',\n",
              "  'instruction',\n",
              "  'emailed',\n",
              "  'guess'],\n",
              " ['th', 'functional', 'design', 'stage', 'next', 'guess'],\n",
              " ['thats', 'end', 'meeting'],\n",
              " ['got', 'little', 'message', 'lot', 'sooner', 'thought', 'would'],\n",
              " ['huh'],\n",
              " ['th', 'quickly', 'cause', 'supposed', 'finish'],\n",
              " ['guess',\n",
              "  'thats',\n",
              "  'u',\n",
              "  'mean',\n",
              "  'probably',\n",
              "  'want',\n",
              "  'kind',\n",
              "  'unique',\n",
              "  'selling',\n",
              "  'point',\n",
              "  'know'],\n",
              " ['right', 'thats', 'thats', 'end', 'meeting'],\n",
              " ['thank', 'coming'],\n",
              " ['hi', 'im', 'david', 'im', 'supposed', 'industrial', 'designer'],\n",
              " ['got', 'project', 'announcement', 'project'],\n",
              " ['designing', 'remote', 'control'],\n",
              " ['thats', 'didnt', 'get', 'anything', 'else'],\n",
              " ['get', 'thing'],\n",
              " ['cool'],\n",
              " ['there', 'much', 'gear'],\n",
              " ['cant', 'draw'],\n",
              " ['anyway', 'dont', 'know', 'first', 'animal', 'think', 'top', 'head'],\n",
              " ['yes'],\n",
              " ['big', 'reason', 'cause', 'im', 'allergic', 'animal'],\n",
              " ['allergic', 'animal', 'fur', 'fish', 'natural', 'choice'],\n",
              " ['kind', 'whale'],\n",
              " ['come', 'go', 'eat', 'everything', 'sight'],\n",
              " ['theyre', 'quite', 'harmless', 'mild', 'interesting'],\n",
              " ['tail', 'bit', 'big', 'think'],\n",
              " ['dinner', 'dog'],\n",
              " ['make',\n",
              "  'sense',\n",
              "  'maybe',\n",
              "  'design',\n",
              "  'point',\n",
              "  'view',\n",
              "  'cause',\n",
              "  'complicated',\n",
              "  'character',\n",
              "  'european',\n",
              "  'language',\n",
              "  'need',\n",
              "  'button'],\n",
              " ['possibly'],\n",
              " ['keep', 'losing'],\n",
              " ['finding', 'really', 'pain', 'know'],\n",
              " ['mean',\n",
              "  'usually',\n",
              "  'quite',\n",
              "  'small',\n",
              "  'want',\n",
              "  'right',\n",
              "  'slipped',\n",
              "  'behind',\n",
              "  'couch',\n",
              "  'kicked',\n",
              "  'table'],\n",
              " ['know'],\n",
              " ['think', 'one', 'factor', 'would', 'production', 'cost'],\n",
              " ['there', 'cap', 'depends', 'much', 'cram', 'price'],\n",
              " ['think', 'thats', 'main', 'factor'],\n",
              " ['cool']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Q82dbOanVi"
      },
      "source": [
        "# функция, которая объединяет все выше сделанные действия  \n",
        "def preprocess(filename):\n",
        "    f = open(filename, \"r\") \n",
        "    a = f.read()\n",
        "    text =[]\n",
        "    split_regex = re.compile(r'[.|!|?|…]')\n",
        "    sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(a)])\n",
        "    for s in sentences:\n",
        "        text.append(s)\n",
        "    res = list(sent_to_words(text))\n",
        "    res_ = []\n",
        "    s_ =[]\n",
        "    for s in res:\n",
        "        for word in s:\n",
        "            if word not in stopwords_english:\n",
        "                s_.append(word)\n",
        "        res_.append(s_)\n",
        "        s_ = []\n",
        "    res_fin = []\n",
        "    s_fin = []\n",
        "    for s in res_:\n",
        "        for word in s:\n",
        "            s_fin.append(lemmatizer.lemmatize(word))\n",
        "        if s_fin != []:\n",
        "            res_fin.append(s_fin)\n",
        "        s_fin = []\n",
        "    return res_fin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3yPowwVanVj"
      },
      "source": [
        "## Метрики "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MMSDFqYanVj"
      },
      "source": [
        "https://dspace.spbu.ru/bitstream/11701/11076/1/Diploma_Novosyolova_Anastasiia.pdf\n",
        "https://core.ac.uk/download/pdf/287382511.pdf\n",
        "\n",
        "Качество оценивается с помощью двух основных метрик BLEU и ROUGE. Можем провести параллель с известными нами метриками (precision и recall).\n",
        "1. BLEU (машинный перевод -> человеческий текст)  \n",
        "Основная задача при разработке BLEU состояла в том, чтобы сравнивать N-граммы машинного перевода с N-граммами эталонного перевода и посчитать количество совпадений. Такие совпадения не зависят от порядка слов. Чем их больше, тем лучше предъявляемый перевод.  \n",
        "Как она считается? Считают максимальное вхождение n-gram из маинного перевода  в человеческий текст и делят на общее количество n-gram в машинном переводе, а затем измеряется средняя точность для N-грамм разной длины (униграммы, биграммы, триграммы и т.д.)\n",
        "\n",
        "2. ROUGE-метрики (человеческий текст -> машинный перевод, больше для extractive)\n",
        "ROUGE-N:  \n",
        "Ситуация, аналогичная ситуации в BLEU, только мы считаем количество совпадаений n-gram из человеческого текста в машинном переводе. \n",
        "ROUGE-L:  длина наибольшей общей подпоследовательности между машинным переводом и человеческим текстом  \n",
        "\n",
        "3. Можно использовать меру F1, чтобы эти показатели работали вместе:\n",
        "F1 = 2 * (Bleu * Rouge)/(Bleu + Rouge)\n",
        "4. WER (world error rate) для распознавания речи . \n",
        "$WER = \\frac{S + D + I}{N}$, где  \n",
        "S - количество замен  \n",
        "D - количество удалений  \n",
        "I - количество вставок  \n",
        "N - количество слов  \n",
        "Это просто расстояние по Левенштейну от того, что мы распознали, до того, что реально было сказано в тексте, поделить на количество слов, реально сказанных в тексте.  \n",
        "5. METEOR – метрика для оценки перевода с явным контролем, является метрикой для оценки вывода машинного перевода. Эта метрика - модификация метрики BLEU и ROUGE. Опишем подробнее ее алгоритм: \n",
        "-Точное установление соответствия — определяются строки, которые являются идентичными в эталонном и машинном переводе. (новое)\n",
        "-Установление соответствия основ — проводится стемминг (выделение основы слова), и определяются слова с одинаковым корнем в эталонном и машинном переводе.  \n",
        "-Установление соответствия синонимов — определяются слова, которые являются синонимами в соответствии с WordNet. (новое)  \n",
        "Дальше вычислим метрику BLEU (P) и метрику ROUGE (R) . Они комбинируются, используя формулу гармонического среднего. \n",
        "$F_{mean} = \\frac{10PR}{R + 9P}$, а затем определим формулу, которая вычисляется по следующей формуле: \n",
        "$p = 0.5 * (\\frac{c}{u_{m}}) ^ 3$, где c  - число груп n-gram , а u - количество n-gram, тогда финальный показатель качества: \n",
        "M = f(1 - p)\n",
        "\n",
        "6.  Расстояние Дженсона-Шеннона показывает, сколько информации будет потеряно, если заменить оригинал рефератом. Соответственно, чем ближе показатель к нулю, тем лучше качество.\n",
        "\n",
        "7. NIST - это метод оценки качества текста, который был переведен с использованием машинного перевода. Он основан на метрике BLEU, но с некоторыми изменениями. Где BLEU просто вычисляет точность nграмм, добавляя равный вес каждому из них, NIST также вычисляет,\n",
        "насколько информативным является конкретный n-грамм\n",
        " То есть, когда найден правильный n-грамм , чем реже, чем n-грамм, тем больше веса\n",
        "ему будет дано. Например, если биграмм «на» правильно подобран, он будет получать более низкий вес, чем правильное совпадение «интересных вычислений» bigram, поскольку это менее вероятно. NIST также отличается от BLEU в расчете штрафа за краткость, поскольку небольшие вариации в длине перевода не влияют на общий балл  Для нашей задачи подходит метрика BLEU, изначально она создавалась на основе показателя вероятности ошибок, используемого для оценки качества распознавания речи. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NufNSogEfJwj"
      },
      "source": [
        "Наиболее полпулярными являются rouge и bleu. (объяснить, почему meteor, расстояние дженсона-шеннона, nist нам не подходят) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wod-V3vAanVj"
      },
      "source": [
        "# загружаем нужные нам библиотеки \n",
        "import spacy \n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "punctuation += '\\n'\n",
        "from heapq import nlargest\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import networkx as nx\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge import Rouge\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B5loPiWanVj"
      },
      "source": [
        "## Extractive methods "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyFrkFIQanVk"
      },
      "source": [
        "### Word frequency method  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZPSNmrUanVk"
      },
      "source": [
        "Попробуем применить самый простой подход, основанный на частоте слов, пошагово. Что мы будем делать? Мы будем смотреть на то, как часто слово встречается в параграфе, например, если слово \"cool\" появилось в предложении 5 раз, то его вес будет равняться 5.  Так мы сможем создать score для каждого предложения, но в score каждого предложения мы будем брать нормализированный вес слова, то есть его частоту, деленную на длину предложения. Тем самым, в конце мы выберем "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3fqnq6zanVk"
      },
      "source": [
        "Рассмотрим алгоритм на примере одного файла. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIw7qdWWanVk"
      },
      "source": [
        "a = open('ami-transcripts/EN2001a.transcript.txt', \"r\", errors = 'ignore')\n",
        "transcript_sum = a.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gCG_-OeanVl",
        "outputId": "8b61622f-8899-4b16-aa8b-fc468eb90088"
      },
      "source": [
        "transcript_sum"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"'Kay. Gosh. 'Kay. Is there much more in it than he d Is there much more in it than he said yesterday? Mm. Hmm. Hmm? Yeah, now I'd say if for the prototype if we just like wherever possible p chunk in the stuff that we have um pre-annotated and stuff, and for the stuff that we don't have pre-annotated write like a stupid baseline, then we should probably be able to basically that means we focus on on the interface first sort of, so that we we take the the ready-made parts and just see how we get them work together in the interface the way we want and and then we have a working prototype. And then we can go back and replace pieces either by our own components or by more sophisticated compo po components of our own. So it's probably feasible. The thing is I'm away this weekend. So that's for me Oh yeah, um yeah. No. But also I might like the the similarity thing, like my just my matrix itself for my stuff, I c I I think I can do that fairly quickly because I have the algorithms. Yeah, I think today's meeting is really the one where we where we sort of settle down the data structure and as soon as we have that, uh probably like after today's meeting, we then actually need to well go back first of all and look at NITE X_M_L_ to see in how far that that which we want is compatible with that which NITE X_M_L_ offers us. And then just sort of everyone make sure everyone understand the interface. So I think if today we decide on what data we wanna have now, and and later, maybe even today, we go and look at NITE X_M_L_ or some of us look at NITE X_M_L_ in a bit more detail, just trying to make some sense of that code and see how does the representation work in their system. And then sort of with that knowledge we should be able to then say okay, that type of NITE X_M_L_ data we wanna load into it, and this is how everyone can access it, and then we should be able to go from there. No. I've looked looked at the documentation and n like seen enough to make me think that we want to use the NITE X_M_L_ framework because um they have a good a event model that synchronizes sort of the data and and every display element. So that takes a lot of work away from us. Sort of that would be a reason for staying within their framework and using their general classes. But beyond that I haven't looked at it at all, which is something we should really do. Who actually like for this whole discussion I mean, who of us is doing stuff that is happening on-line and who of us is doing stuff that's happening off-line? Like my data is coming c Hmm? Yeah. Okay. Okay. 'Kay. So basically apart from the display module, the i the display itself, we don't have an extremely high degree of interaction between sort of our modules that create the stuff and and the interface, so the interface is mainly while it's running just working on data that's just loaded from a file, I guess. There isn't Yeah, I know. Th Yeah, the search is I guess the search is sort of a strange beast anyway because for the search we're leaving the NITE X_M_L_ framework. Um but that's still sort of that's good. That means that at least like we don't have the type of situation where somebody has to do like a billion calculations on on data on-line. 'Cause that would make it a lot more like that would mean that our interface for the data would have to be a lot more careful about how it performs and and everything. And nobody is modifying that data at at on-line time at all it seems. Nobody's making any changes to the actual data on-line. So that's actually making it a lot easier. That basically means our browser really is a viewer mostly, which isn't doing much with the data except for sort of selecting a piece piece of it and and displaying it. Hmm? Well some parts relevant for the search, yes. I'd say so. Hmm? Yeah, but nobody of us is doing much of searching from the data in the on-line stage. And for all together, like the display itself, I think we are easier if we if it's sitting on the X_M_L_ than if it's sitting on the S_Q_L_ stuff, because if it's sitting on the X_M_L_, we have the the NITE X_M_L_ framework with all its functionality for synchronizing through all the different levels whenever there's a change, whenever something's moving forward and stuff. And we can just more or less look at their code, like how their player moves forward, and how that moving forward is represented in different windows and stuff. So I think in the actual browser itself I don't wanna sit on the S_Q_L_ if we can sit on the X_M_L_ because sitting on the X_M_L_ we have all we have so much help. And for y for like the p the calculations that we're doing apart from the search, it seems that everyone needs some special representations anyway. You mean our results? Yeah, in in the NITE X_M_L_ X_M_L_ format, so with their time stamps and stuff, so that it's easy to to tie together st things. What I'm like what we have to think about is if we go with this multi-level idea, like this idea that sort of if you start with a whole meeting series as one entity, as one thing that you display, as one whole sort of, that then the individual chunks of the individual meetings, whereas and then you can click on a meeting, and then sort of the meeting is the whole thing and the chunks are the individual segments, that means sort of we have multiple levels of of representation, which we probably If we if we do it this way like we f we have to discuss that if we do it this way, then we should probably find some abstraction model, so that the interface in the sense like deals with it as if it's same so that the interface doesn't really have to worry whether it's a meeting in the whole meeting series or a segment within a meeting, you know what I mean? And that's probably stuff that we have to sort of like process twice then. Like for example that like the summary of a meeting within the whole meeting corpus or meeting series y is meeting series a good word for that? I don't really know what how to call it. You know what I mean, like not not the whole corpus, but every meeting that has to do with one topic. Um so in in the meeting se series so that a summary for a meeting within the meeting series, are sort of compiled off-line by a summary module. And that is separate from a summary of a segment within a meeting. 'Cause I don't think we can So are we doing that at all levels? Are we um And just have different like fine-grainedness levels sort of. Mm. 'Kay. So the only thing that yeah, so the only thing that would happen basically if I double-click let's say from the whole meeting series on a single meeting, is that the zoom level changes. Like the th the start and the end position changes and the zoom level changes. I I thought we couldn't do that. Like I was under the impression that we couldn't do that because we couldn't load the data for all that. But I don't know, I mean that So I'm s not sure if I got it. I was Mm-hmm. Mm-hmm. Mm-hmm. Mm-hmm. Okay. So Okay. I wa I was just worried about the total memory complexity of it. But I I completely admit, I mean, I just sort of like th took that from some thing that Jonathan once said about not loading everything. But maybe I was just wrong about it. How many utterances w Yeah, and I w yeah. Yeah. Yeah. Yeah. So what we have is we would have a word. Like we would have words with some priority levels. And they would basically be because even the selection would would the summaries automatically feed from just how prioritized an individual word or how indiv uh prioritized an individual utterance is? Or i are the summaries sort of refined from it and made by a machine to make sentences and stuff? Or are they just sort of taking out the words with the highest priority and then the words of the second highest priority? And the u okay. Are we doing it on th the whole thing on the utterance level? Or are we doing it on word level, like the information density calculation? We I think we have start and end times for words actually, but it's yeah, but it m it might s but it might sound crazy in the player. We should really maybe we can do that together at some point today that we check out how the player works. But there's maybe some merit in altogether doing it on an utterance level in the end. So Yeah. Well but also about the displays, I mean the displays in the in the text body, in the in the latest draft that we had sort of we came up with the idea that it isn't displaying utterance for utterance, but it's also displaying uh a summarised version in you know, like below the below the graph, the part. Maybe Yeah, r Hmm? Oh yeah, f it's just like there there's like audio skimming and there's displayed skimming. Yeah. Ma maybe there's some merit of going altogether for utterance level and not even bother to calculate I mean if you have to do it internally, then you can do it. But maybe like not even store the importance levels for individual words and just sort of rank utterances as a whole. Hmm? Yeah. 'Cause it it might be better skimming and less memory required at the same time. And I mean if you if you know how to do it for individual words, then you can just in the worst case, if you can't find anything else, just sort of make the mean of the words over the utterance. You know what I mean? W it's it's Well what's the smallest chunk at the moment you're thinking of of assigning an importance measure to, is it a word or is it an utterance? So we're thinking of like maybe just storing it on a per utterance level. Because it's it's less stuff to store probably for Dave in the in the audio playing. And for in the display it's probably better if you have whole utterances than I don't know, like what it's like if you just take single words out of utterances. That probably doesn't make any sense at all, whereas if you just uh show important utterances but the utterance as a whole it makes more sense. So it doesn't actually make a difference for your algorithm, 'cause it just means that if you're working on a word level, then we just mean it over the utterance. They are on Oh so that's good anyway then, yeah. Because that makes it a lot easier than to t put it on utterance level. Oh yeah. No but I mean like how how Jasmine does it internally I don't know, but it's probably, yeah, you probably have to work on word levels for importance. But there should be ways of easily going from a word level to an utterance level. Okay. Yeah, prob Hmm. Well we do a pre-filtering of sort of the whole thing, sort of like but that, like the problem with that is it's easy to do in the text level. But that would mean it would still play the uh in your audio, unless we sort of also store what pieces we cut out for the audio. Yeah. I think before we can like answer that specific question how we c deal with that, it's probably good for us to look at what the audio player is capable of doing. Yes. So what do you mean by buffering? Like you think directly feeding But yeah, but not but not stored on the hard disk and then loaded in, but loaded in directly from memory. But it's probably a stream if it exists in Java, it would be probably some binary stream going in of some type. Okay, yeah. Okay. Okay, so I mean so that means that there's probably, even if you go on an per utterance level, there's still some merit on within utterances cutting out stuff which clearly isn't relevant at all, and that maybe also for the audio we'd have to do. So let's say we play the whole au phrase, but then in addition to that, we have some information that says minus that part of something. That's okay, that we can do. Yeah, maybe even I mean that's sort of that depends on how how advanced we get. If maybe if we realise that there's massive differences in in gain or in something, you can probably just make some simple simple normalization, but that really depends on how much time we have and and how much is necessary. Yeah, if like I d I don't know anything about audio and I have never seen the player. So if you find that the player accepts some n input from memory, and if it's easy to do, then I guess that's that's fairly doable. So but that means in the general structure we're actually quite lucky, so we we have we load into memory for the whole series of meetings just the utterances and rankings for the utterances and some information probably that says, well, the I guess that goes with the utterance, who's speaking. Because then we can also do the display about who's speaking. Yeah. But I'm I'm still confused 'cause I thought like that's just what Jonathan said we do c that we can't do, like load a massive document of that size. On the other hand The other hand, I mean it shouldn't be like should be like fifty mega-byte in RAM or something, it shouldn't be massive, should it? Actually fifty hundred megabyte is quite big in RAM. Just thinking, what's the simp so We do get an error message with the project if we load everything into the project with all the data they load. So we know that doesn't work. So our hope is essentially that we load less into it. What's this lazy loading thing, somebody explain lazy loading to me. Ah, okay. So that is that only by type of file. Like if if if the same thing is in different files, would it then maybe like, you know, if if utterances are split over three or ten or w hundred different files, is then a chance maybe that it doesn't try to load them all into memory at the same time, but just So why does it fail then in the first place? Then it shouldn't ever fail, because then it should never Yeah, but yeah, but um it uh it it failed right when you load it, right, the NITE X_M_L_ kit, so that's interesting. Hmm. Let's check that out. Um I'll p I'll probably ask Jonathan about it. So alternatively, if we realise we can't do the whole thing in one go, we can probably just process some sort of meta-data, you know what I mean, like sort of sort of for the whole series chunks representing the individual meetings or some Like something that represents the whole series in in a v in a structure very similar to the structure in which we represent individual um meetings, but with data sort of always combined from the whole series. so instead of having an single utterance that we display, it would probably be like that would be representing a whole um topic, a segment in a meeting. And sort of so that wi using the same data st Well, in a sense Uh I'm I'm thinking of in a sense of like creating a virtual a virtual meeting out of the whole meeting series, sort of. Yeah, sort of like off-line create a virtual meeting, which which basically treats the meeting series as if it was a meeting, and treats the individual meetings within the series as if they were segments, and treats the individual segments within meetings as if they were um utterances. You know, so we just sort of we shift it one level up. And in that way we could probably use the same algorithm and just like make vir like one or two ifs that say okay, if you are on a whole document uh a whole series level and that was a double-click, then don't just go into that um segment, but load a new file or something like it, but in general use the same algorithm. That would be an alternative if we can't actually load the whole thing and 'Cause also like even if we maybe this whole like maybe I'm worrying too much about the whole series in one thing display, because actually I mean probably users wouldn't view that one too often. Yeah, but I'm I'm still worried. Like for example for the display, if you actually if you want a display uh like for the whole series, the information density levels based on and and the f and the only granularity you have is individual utterances, that means you have to through every single utterance in a series of seventy hours of meetings. Yeah. Yeah, and if you make that structurally very similar to the the le like one level down, like the way how we uh store individual utterances and stuff, then maybe we can more or less use the same code and just make a few ifs and stuff. Yeah, so so but still so in in general we're having we're having utterances and they have a score. And that's as much as we really need. And of cou and they also have a time a time information of course. Hmm? And a and a s and a speaker information, yeah. Yeah, so an information which topic they're in, yeah. And and probably separate to that an information about the different topics like that Yeah. So so the skimming can work on that because the skimming just sort of sorts the utterances and puts as many in as it needs Yeah. Yeah, it'll it'll play them in some order in which they were set because otherwise it's gonna be more entertaining. Um but that that's enough data for the skimming and the the searching, so what the searching does is the searching leaves the whole framework, goes to the S_Q_L_ database and gets like basically in the end gets just a time marker for where that is, like that utterance that we are concerned with. And then we have to find I'm sure there's some way in in NITE X_M_L_ to just say set position to that time mark. And then it shifts the whole frame and it alerts every single element of the display and the display updates. Yeah, yeah. That we can ju yeah, but so so if if somethi so yeah. So if in that tree display somebody clicks on something Yeah, and then you sort of feed the time stamp to and the NITE X_M_L_ central manager, and that central manager alerts everything that's there, like alerts the skim like the the audio display, alerts the text display, alerts the visual display and says we have a new time frame and then they all sort of do their update routines with respect to the current level of zoom. So how much do they display, and starting position at where the or maybe the mid-position of it, I don't know, like w if start where the thing was found or if that thing wa was found it's in the middle of the part that we display, that I don't know. But that we can decide about, but a general sort of It's the same thing if like whether you play and it moves forward or whether you jump to a position through search, it's essentially for all the window handling, it's the same event. It's only that the event gets triggered by the search routine which sort of push that into NITE X_M_L_ and says please go there now. Why do we have to do it in memory? But that stuff's so I mean like the information is coming from off-line. So we probably we don't even have to change the utterance document, right, because the whole way, like the whole beauty of the NITE X_M_L_ is that it ties together lots of different files. So we can just create an additional X_M_L_ file which for every utterance like the utterances have I_D_s I presume, some references. So we just we tie uh p just a very short X_M_L_ file, which it's the only information it has that has whatever a number for for the um weight, for the information density, and we just tie that to the existing utterances and tie them to the existing speaker changes. Well otherwise we probably have to go over it and like add some integer that we just increment from top to bottom sort of to every utterance as an as an I_D_ some type. Or un or try to understand how NITE X_M_L_ I_D_s work and maybe there's some special route we have to follow when we use these I_D_s. It's alm hmm? Yeah, the the girl said the utterances themselves are not numbered at the moment. Okay. Okay. Okay. Yeah. So I guess that would be solvable if not. Mm-hmm. Sorry? Okay. Okay. Is that a board marker pen actually? Oh. That's just so like to make a list of all this stuff, or we probably can somebody can do it on paper. All these fancy pens. So what so the stuff we have we have utterances and speakers and weights for utterances. So for for every utterance sort of like the utterance has a speaker and a weight which is coming from outside. Or we just tie it to it. And there is segments, which hmm? Oh, so sorry um. Uh topic s topic segments I meant. Like they are they are a super-unit. So so the utterances are tied to topic segments. And if the time stamps are on a word level, then we b somehow have to extract time stamps for utterances where they start. W what segments now? Okay. Is the uh is that the same as utterances that is that the same as utterances that Mm-hmm. Mm-hmm. What so that's Oh. But that's one o one segment or is that two segments then? Yeah. Okay. Okay. So but but generally utterances is that which we just called uh sorry, segments is that which we just called utterances now. Like it's it's the sa it's sort of like one person's contribution at a time sort of thingy dingy. Okay, so yeah, so we have those, and and then we have some f field somewhere else which has topics. Yeah, and and a topic's basically they are just on the I_D_, probably with a start time or something, and and the utterances referenced to those topics I guess. So the topics don't contain any redundant thing of like showing the whole topic again, but they just sort of say a number and where they start and where they finish. And the utterances then say which topic they belong to. Yeah. No. But I was thinking of the topic segmentation now and and f for that there would only be one, right, because it's sort of like it's just a time window. Yeah. So if this lazy loading works, then this should definitely fit into I mean not memory then because it wouldn't all be in memory at the same time. So if we just have those sort of that information like a long list of all the utterances slash segments and like short or smaller lists which give weight to them. And even though probably if there's a lot of over-head in having two different files, we can probably merge the weights into it off-line. You know what I mean, like if if there's a lot of bureaucracy involved with having two different trees and whether one ties to the other because the one has the weight for the other, then it's probably quicker to just Yeah, I thought that was the whole beauty that like you can just make a new X_M_L_ file and sort of tie that to the other and and it tre Oh yeah. So no, I didn't I didn't mean tree. No. No. I meant just like handling two different files internally. Sort of c I was just thinking you know like if if the overhead for having the same amount of data coming from two d files instead of from one file is massive then it would probably be for us easy to just like off-line put the the weight into into the file that has the segments, uh yeah, segments slash utterances already. But that we can figure out I mean if it's going horrendously wrong. Yeah. Yeah. Yeah, no, we'd we'd be completely using like the whole infrastructure and basically just I mean the main difference really between our project and theirs really is that we load a different part of the data. But otherwise we're doing it the same way that they are doing it. So we just we're sort of running different types of queries on it. We in a sense we I think we are running queries, it's not just about um what we load and what we don't load, but we're l running queries in the sense that we dynamically select by by weights, don't we? That we have to check how fast that is, like to say give us all the ones that whether that works with their query language, whether that's too many results and whether we shou You know, if 'cause if it i let's say I mean if if their query language is strange and if it would return b ten million results and it can't handle it, then we can just write our individual components in the way that they know which what the threshold is. So they still get all the data and just they internally say oh no, this is less than three and I'm not gonna display it or something. Hmm? Yeah. No. I'm just thinking for this whole thing of like a different level, sort of cutting out different different pieces, whether we do that through a query where we say give us everything that's ab above this and this weight, or whether we skip the same infrastructure, but every individual module like the player and the display say like they still get sort of all the different utterances, uh all the different pieces, but they say oh, this piece I leave out, because it's below the current threshold level. When do we need the one for the meet Okay. Yeah, I guess for the so when we have the display, will we display the whole series. Then if we have for the individual topic segments within the meetings if we have ready calculated disp um measures, then we don't have to sort of extract that data from the individual utterances. Yeah, and that's also fairly easy to store along with our segments, isn't it. For the segments, are we extracting some type of title for them that we craft with some fancy algorithm or manually or we're just taking the single most highly valued key-word utterance for the segment heading? Hmm. Hmm. It's probably like in in the end probably it wouldn't be the best thing if it's just the high most highly ranked phrase or key-word because like for example for an introduction that would most definitely not be anything that has any title anywhere similar to introduction or something. Yeah. Also like for this part, maybe if we go over it with named entity in the end, if I mean w if one of the people doing DIL has some named entity code to spare, and just like at least for the for sort of for finding topics, titles for for segments, just take a named entity which has a really high, what's it called, D_F_I_D_F_, whatever. 'Cause you'd probably be quite likely if they're talking about a conference or a person, that that would be a named entity which is very highly fr um frequented in that part. Yeah, he said they're quite sparse. So that basically was don't bother basing too much of your general calculation on it. But like especially if they're sparse, probably individual named entities which describe what a what a segment is about would probably be quite good. Like if there's some name of some conference, they would could probably say that name of the conference quite often, even though he's right that they make indirect references to it. Anyway Sorry? So you're doing that on a on a per word level. Okay. Okay. Okay, cool. I was just wondering where you had the corpus from at the moment. So it it seems that the data structure isn't a big problem and that basically we don't have to have all these massive discussions of how we exactly interact with the data structure because most of our work isn't done with that data structure in memory in the browser, but it's just done off-line and everyone can ha represent it anyway they want as long as they sort of store it in a useful X_M_L_ representation in the end. So like Yeah, that would mean understanding the NITE X_M_L_ X_M_L_ sort of format in a lot more detail. We should I think we should just have a long session in the computer room together and like now that we know a bit more what we want, take a closer look at NITE X_M_L_. Mm-hmm. Mm-hmm. Good. Yeah, I haven't looked at this stuff much at all. Yeah. Yeah. Who's who's sort of doing the the the central coordination of of of the browser application now? Like Hmm? Yeah, or but also like all these elements like like the loading and, yeah, integration and and like handling the data loading and stuff. Nah. I'm sort of like I think I'll take over the display, just because I've started with a bit and found it found it doable. So somebody should sort of be the one person who's who understands most about what's t centrally going on with with the with the project, like with the with the browser as a whole and where the data comes in and Any volunteers? It's also a complicated one. Yeah. I know but uh b I guess we can do it like several people together, it's probably just those people have to work together a lot and very closely and just make sure that they're always f understand what the other one is doing. Yeah, or or ready-made versions of them for that matter and Yeah, but I think actually like at the moment the integration comes first, I mean it's sort of at the moment the building the browser comes first, and then only comes the creating new sophisticated data chunks, because that's sort of the whole thing about having a prototype system which is more or less working on on chunk data. But it at least we have the framework in which we can then test everything and and look at everything. 'Cause before we have that, it's gonna be very difficult for anyone to really see how much the work that they're doing is making sense because you just well I guess you can see something from the data that you have in your individual X_M_L_ s files files that you create, but it would be nice to have some basic system which just displays some stuff. Or just adapt like their like just sort of go from their system and and adapt that piece for piece and see how we could how we could arran like adapt it to our system. Does anyone want to like just sit with me and like play for three hours with NITE X_M_L_ at some point? Uh I wouldn't like to be 'cause I'd like to go to the gym. I'm theoretically free. But if there's any time t hmm? You have nothing no free time on Wednesday. Hmm. Nine 'til twelve and then nothi you have or you Hmm? Anytime Wednesday afternoon I'd be cool, I think. Yo, Forrest Hill, whatever one's easier to discuss stuff, I don't know. I'm not biased. Okay. What time do you wanna do? Okay, so I'll just meet you in in eighteen a in the afternoon. I guess at the moment nobody critically depends on like the NITE X_M_L_ stuff working right now, right? Like at the moment you can all do your stuff and I can do my L_S_A_ stuff. And I can even do the display to a vast degree without actually having their supplying framework working. So it's not that crucial. Yeah, actually I need the raw text as well. Yeah, but I was I was I was more thinking of the sort of the the whole browser framework as a running programme now. Yeah, I think we all need the raw text in different in different flavours, don't we? But number within the X_M_L_ context. Are they spoken numbers? Like do they look like they're utterances numbers? There's the number task, isn't there. That's part of the whole thing. Hmm? Okay. Hmm. Yeah, we have to probably cut that out anyway for our project, I don't know. It's probably gonna screw up a lot of our data otherwise. If Not sure if it what it does to document It would probably make the yeah, if if you have segments for that, probably the Okay. Uh I'm just thinking like it pro it pro probably like the L_S_A_ would perform quite well on it. It would probably find another number task quite easily seeing that it's a constrained vocabulary with a high co-occurrence of the same nine words. So that wou ten word. Hmm? Yeah. I think it's also something that they they said the numbers in order, right? Yeah, I think it's it the it sounded like they wanted to check out how well they were doing with overlapping and stuff, because basically it's like they're reading them at different speeds, but you know in which order they are said. Anyway. ICSI has some reasons for doing it. They must have been pissed off saying like numbers at the end of every meeting. Um Dave, if you would or actually for well, if you're doing I_D_F_s or you whatever you call your your frequencies, I always mix up the name, uh you need some dictionary for that at some point though, like you need to have some representation of a word as not not that specific occurrence of that word token, but of of of a given word form. Because you're making counts for word forms, right? Yeah, so we should work together on that, because I need a dictionary as well. Okay. 'Kay. Okay. Didn't you say that the o the ord Yeah, but for I'm just wondering for the whole thing. Does somebody wo who was it of you two who said that um there's some programme which spits out a dictionary probably with frequencies? Okay. Is anyone of you for the for the document frequency over total frequency, you gonna have total frequencies of words then with that, right? Like over the whole corpus sort of. Or W using which tool are you talking about? Be careful with that. Like my experience with the British National Corpus was that there's far more word types than you ever think because anything that's sort of unusual generally is a new word type. Like any typo or any strange thing where they put two words together. And also any number as a word type of its own. So you can easily end up with hundred thousands of words when you didn't expect them. So generally dictionaries can grow bigger then you think they do. Well you can probably also you can probably pre-filter like with regular expressions even just say if it consists of only dig digits, then skip it, or even if it consists any special characters, then skip it because it's probably something with a dot in between, which is usually not something you wanna have and What I did, for my project I just ignored the hundred most frequent words, because they actually end up all being articles and and everything and stuff. So we need like several of us need a dictionary. Am I the only one who needs it with frequencies? Am I the only one who needs it with frequencies? Or Frequencies. Yeah. Well I guess as soon as we have the raw text, we can probably just start with the Java hash map and like just hash map over it and see how far we get. I mean we can probably on a machine with a few hundred megabyte RAM you can go quite far. You can write it on beefy. So even if it goes wrong and even if it has a million words be Oh yeah, burning it on a like we should be able to burn the whole corpus, just the X_ hmm? Ah I see, I asked support about that two days ago. In the Informatics building there oh sorry, in in Appleton Tower five the ones closest t two machines closest to the support office. So I presume oh wait, I have the exact email. I think he's talking about sort of the ones that Yeah, if you if you enter the big room, in the right-hand corner, I think. Um the thing is like you can only burn from the local file-system. So if it's from s well actually I think if it's mounted, you can directly burn from there, but the problem is I have my data on beefy and so I have to get it into the local temp directory and burn it from there. But you can burn it from there. Uh we looked that up and I for we looked that up and I forgot. Yeah yeah. No, you you we should be able to get it at I don't think it was I don't think it was a gigabyte. Hmm. See I would off I would offer you to to get it on this one, and then um like copy it. But you know what I figured out, I'm quicker down-loading over broad-band into my computer than using this hard disk. There's something strange about the way how they access the hard disk, how they mount it, which is unfortunate. Hmm. What operating system do you have? Okay. Wh what connection do you have at home? Yeah. So if anyone of us gets it, we can then just use an ext hmm? Yeah, burn it to C_D_ or, yeah, put it on on hard disk, whatever. Question is if you're not quicker if you uh because you should get massive compression out of that. Like fifty percent or something with a good algorithm. So if you could compress it and just put it into a temp directory. Like The temp the temps usually have for gigabyte three or two. The temps, yeah. I do like I mean there's not guarantee that anything stays there, but overnight it'll stay. And I think the temps usually have. Ah yeah, but that would have to be the temp directory off the machine you can S_S_H_ into directory of S_S_H_. Yeah, they wou they'd they'd probably hate you for doing it. But They'd probably they'd like you more if you S_S_H_ uh into another computer, compress it there and then sort of copy it into the into the gateway machine. They have um if you S_S_ hey, you know, if you if you S_S_H_ and they have this big warning about doing nothing at all in the gateway machine. Yeah. To your home machine. I haven't I haven't figured out how to tunnel through the gateway into another machine yet. It's not it's not easy definitely. That's why I end up sort of copying stuff into the temp directory at the gateway machine. Sorry if this is boring everybody else. This is just details and how to get stuff home from what we can probably just look at that together when we're meeting. I'm sorry. Mm-hmm. Well yeah. As soon as somebody gives me the raw text of the whole thing, I can probably just implement like a five line Java hash table frequency dictionary builder and see Oh, did you not say frequencies f of words in the whole sorry, did uh So you'd you Yeah, you'd have to count it yourself, yeah. Oh, you don't wanna have different counts for each chunk, but just like sort of for for something from old chunks. Oh yeah, no, that's yeah, so once I write an ar like w if I write like an algorithm which does a hash um table dictionary with frequency from a raw text, then the raw text can be anything. So how far are we g uh how f how far are you getting raw text out of it do you think? Okay, well that's good, because for the dictionary the order doesn't make a difference, does it? So yeah, so um I'll get that from you and I'll write the hash table which goes over that and creates a dictionary file. So for the dictionary, is it okay if I do, whatever, word blank frequency or something? Just p could everybody sort of start from that? I mean I guess we can Yeah, I I need frequency as well. Well I think we might have a lot in common what we calculate because I for my latent semantic analysis need like counts of words within a document, uh within a a segment actually, within a topic segment. Can I convert these probabilities back into frequencies? Okay. Oh, so that's what f Rainbow does, because that's what L_S_A_ builds on. Like it builds a f a document by frequency matrix. So I could probably get that. Even though but I already have I already have my code to build it up myself. No, don't bother. I have my code already. Um Yeah, so Dave, you said you need the frequency counts actually for per document, would you say, not for the whole thing? It more and more appears to me that if we if we scrap the notion of the meeting as an individual thing and sort of ju see meetings as as topic segments and have sort of like hierarchical topic segmentation instead, then it's b like a more coherent framework. Wait, are we are we using this um for the for the for the do for the weighting in the end now, this this measure you're calculating? Because if we're doing Like I think for for the information density we uh we should calculate it on the lowest level, not on the highest. But like 'cause Yeah, but w it don't you have to like go sort of like for in a document versus the whole thing? Isn't that how it works that you c look look at r I don't think that's a good idea because isn't it like that we expect th there to change over i b with the different topic segments more? That they talk about something different in each different topic segment. 'Cause that's what relative term frequency is about, that like in some context they're talking more about a certain word than in general. So that would more be the the topic segments then. I don't know. Yeah. Yeah. Yeah. So I'm just wondering if there's ways to abandon the whole concept of of meetings and sort of but just not really treating separate meetings as too much of a separate entity. But But on algorithmic level, whether we actually whether there's some way to just represent meetings as as topics. Hmm. That's not really what I meant. But I think I have to think more about what I meant. Um g I'm confused about everything. Yeah. I'm I'm not so concerned about the m a meeting plus something else, I'm more talking about like, yeah, the keeping keeping the same algorithm and the same way of handling it and just saying like just this this topic here i uh it happens to be like a whole meeting and it has sort of sub-topics, so just that sort of topics a hierarchical concept where like a topic where there can be super-topics and topics, and the super-topics are in the end what the meetings are, but in general at some level super-topics are treated like like topics. Hmm. Mm I'm not really sure what I want. So sorry, could describe that again, the Mm-hmm. Mm-hmm. Mm-hmm. So that would be the series as a whole. That would be sort of m meetings, yeah. Yeah. I'm a I'm a I'm a bit brain-damaged at the moment, but I think I'll just sit together with you again and and go through it again. Hmm. So so I'll is th it like is this and this structurally then always identical? So that we can that we can treat it with the same algorithm or Yeah, I'm also not sure how we can go from from bottom-up. I have always thought it's like more that oh, whatever, I'm a can't think of it at the moment. Probably this is all too complicated worrying about that at that moment anyway. Now have have we have we decided anything, are we doing anything? S Wednesday we are meeting and looking at their at their implementation in some more detail to actually understand what's going on. We had two things from their stuff just to make sure that we are like understand it, we understand it enough to to m modify it. Yep. How would we do that? By just making like it w read write for everyone. 'Kay, who has most free space on their Same here. Well we alternatively we can probably just make another directory on the beefy scratch space. I mean that's where I'm having gigabytes and gigabytes of stuff at the moment. No. No. Yeah. But I think if he sends to the I think if he sends to the port he'd probably be in a better position. Yeah. Hmm. I think he said yes to that. I think uh that was like in when we were still in the seminar room, I asked that once or like ask is it possible to get it off and nobody said like people were discussing about the technical practicalities, but nobody said anything about al being allowed to or not allowed to. I mean, we have access to it here and I guess it probably means that we we can't give it to anybody else. But but if they give us access to it here o sitting on a DICE machine, then there shouldn't be a reason why we shouldn't be able to use it on our laptop. I personally don't have too many friends who would be too keen on getting it anyway. I have that really excited pirate copied thing. It annotated meeting data. Huh. Wait, wait, wait. Um sorry. Yeah, sorry. What I just realised, we should really t keep different serieses completely separate for virtually all purposes. Just let's be careful about that, because like the the ICSI corpus isn't isn't one meeting series, it's several meeting series with different people meeting for completely different things. For each meeting. Alright. Okay, but like let's just be careful that whatever we sort of we merge together, that like the highest level of merging, it's not the whole ICSI corpus but individual series.. I think we might actually I think That's probably be somewhere like well or something like it. Um I think we might just get away with for the whole project just like looking at only one series and just doing within one series. I mean you can do everything you want in one series. Oh yeah, let's take that. Is the is the data always clearly split up by different series? Uh like is it easy to just pick one Okay. Okay. Okay. Okay. So at at every level everyone has to be careful to really just take even at the highest level, just take stuff from one series and not merge stuff from different series together because they would probably be just majorly messy. Yeah, so so t so like if even if we make one single text file which has the whole corpus, sort of our corpus, that would still be from one series only. Wou but it what you're producing at the moment is like individual text files that sort of have the raw text for a whole a meeting as a whole or Mm-hmm. Yeah. 'Kay. Um so is is anybody creating an uh a real raw text thing at the moment, like which is just the words? Yeah, tha 'cause that's what I'm gonna need as well. But i but if there uh b aren't like so it's it's start and end times just for the file. Like is it just the first and the last line? Or is it for every single thing in So what do you mean by just not print out that? Okay. If you're into it, can you make a text file which just like makes just the words? 'Kay. Do you want it straight flowing, 'cause I would need something that marks the end of uh of uh is is yours segmented by topics then that like is there any information that you have to the topic, to the automated topic topic segmentation? Oh then I need something different later anyway. Okay, but for now, if you c Okay. You're gonna put that as an output of yours, the segmentation. Okay, so for now can you create like sort of just uh a dump which is pure text, just pure text so that I can get a dictionary and you can work on that for your topic segmentation. And Or for for the series. But I can but I can also deal with separate files, I mean I can just write the algorithm that it loads all files in a directory or something. But I mean if you But if you can put it in one single mega-file, that would be quite useful for me. Even though for you, wouldn't it be easier if you had different files because then you sort of know like Yeah. So give m give me different files as long as like it m if you could name them in a way that is easy to enumerate over them, like whatever, one two three four five or something. Or just anything that I can Yeah. Is is it something that's easily enu like to enumerate over? Is it some just some ordered pattern? Okay, cool. Okay, cool. Yeah. In the right order. It's just a wish list. Orders. When do you think you'll have um like a primitive segmentation by some ready-made topic segmentation by some ready-made tool ready? Okay. Okay, cool. 'Cause I'll need that then when it's done. Okay. Mm-hmm. What's what's nine megabyte? The the That sounds quite reasonable. That's nine nine characters over okay. Okay. Okay. That is for are we are we picking one particular series at the moment? Or Yes. Okay. Yeah. Yeah, I guess we can probably process the data for all different series and then check which series is the best for the presentation. It sounds quite reasonable, nine megabyte. I mean if you think if it's r roughly a million words and nine characters per word sounds realisti Yeah. Yes, I'm gonna build a dictionary then from that. Like just a list of the words that maybe a list of the words with the frequencies or a list of the words sorted alphabetically or numerically. What what does anyone want? Does this there any wishes for dictionaries? So I'll create a dictionary. Add add the structure, yeah. And then the actual file we can probably like copy from your home directory or something like it. Yeah yeah, but I'm sa I'm saying for the whole thing in the end. Then like the big thing we probably shouldn't do by email. Yeah. Oh, from the time I get the file I can do that in an afternoon, the next sort of the next morning. Oh, you mean how long processing time it takes. Ah, it's a it's a bog standard algorithm. I've I've sort of I've written it for for DIL just in half an hour or something similar. It's just you put them in a hash table and and say well if it exists already in the hash table then you increase the count by one and I'll probably implement some filter for filtering out numbers or something. Really? How do you do that? Okay, well I don't know any Perl. I mean if anyone wants to do a Perl script for that that does it does it nicely, I uh I've no problem with that. I but I think I have the Java code virtually ready because for DIL I wrote something very similar. Like for DIL I wrote something that counts the the different occurrences of all the tags um Sorry? The hash table? Uh I've never serialized anything. Wouldn't that be absolutely massive though? And then seriali and then write the serialization to a file. So you want like a se like a file which is the serialization of a hash table. Okay. Yeah. I I'll I'll check if I understand how it works. I mean otherwise I can give you the code for loading a dictionary. Give you my my it's just it's it's sort of it's a line break separated file, you know. Yeah. Yeah, I'll see if I understand how to serialize. There's a there's a serialise command so that gives me one mega mother of a s Yeah, but do they automatically write to the file anyway I'll I'll figure that out. We don't have to Yes, is that pretty much pretty much it? So Dave and me look at how NITE X_M_L_ works and we're Hmm. I'll build a dictionary as soon as I get the text. And yeah, so that When do we have to meet again then with this? How are we gonna do a demonstrator next week? My God. No no, not demonstrate, but like didn't you say that uh didn't we sort of agree that it would be useful to have a demonstrator of it, like some primitive thing working next week. That's gotta be very prototype. Mm-hmm. Ah well, let's go. Sorry. I feel like like hanging mid-air and not really like finding a point where you can get your teeth into it and start working properly and so it's all so fuzzy the whole Yeah, but it at the moment but at the moment it's also an implementational level. Like with the data structures, I'm just like over these vague ideas of some trees, I'm f Yeah. It's just we are half-way through the project time table. That's just what freaks me out. Um\\nYeah. Yeah, I mean if we just want to have um some data for the user face, could even be random data. Uh mm mm Yeah, I'm Hmm. Yes. Hmm yes. Hmm. I'm not so sure. I I thought we would just have like um one big summary um with all the uh different importance levels um displayed on it. And depending on what our um zoom level is, we just display a part of it. And we would have one very big thing off-line. And from that we would just select what we are displaying. Yes. So for example you would um give a high value to those um sequences you want to display in the meeting series summary. And you just cut off. That was what I sh I thought, yeah. I thought. But I think the m difference might be that we want just want to have um the words. And that's not so much what he meant with not possibly loading everything was that you m um load all the uh annotation stuff, all the sound files, all In Um I r I I'm getting quite lost um at the moment because um w what's um our difference between the um se um uh the importance measure and the skimming? I mean, do we do both or is it the same thing? Okay. So but when when we talk about summaries you talk about this uh abo about skimming and not about Yeah. Yeah right, isn't that the skimming? Isn't that the skimming? Yeah, but it use the same data. Yeah. A And, yeah, I think we also thought about combining that measure with um the measures I get from um s uh hot-spots and so on. So that would also be on utterance level, I think. I think. Yes, sure. Yes. Yes, right. Oops, it does. So I define baseline and what it loads? For example it loads all the utterances and so on, but it doesn't load um the discourse acts and for example not the and what's what else there? Not the summaries. It only loads those on demand. Y you mean that you um basically split up th the big thing into um different summaries. For example that you have a very um top-level um summary and a separate file for for each level. Mm-hmm. Yes. N Uh no no, it's f for No, you're right. Yeah. It's for Um No, I I think we would just take the segments that are already that were Yeah, there's um this segments file. Um you know, the X_M_L_ segments. Oh. That I don't know. Yeah, that's um Mm-hmm. There there are time stamps um for, well, segments um and for th um segments is for example when when you look at the data, what is displayed in one line. What when when you look at it in the hmm? I think so. Isn't Um for ex um I I compared it with what I did for the pause um duration extraction. Um and basically it's uh words that are uttered in a sequence without pauses. But sometimes um however there are um short pauses in it and they're indicated by square brackets pause or something in the data. Um someti uh but uh the annotators decided what was one segment and what wasn't. I think so. Yeah, but um I think for some annotations um an uttera ca utterance can have several um types. For example for the dialogue acts and so on. Okay. Yeah, that should be for Yeah. Should be, yeah. Yes, but that's Yeah, everything that's a word has a sti time stamp. That's at the end. That's at the end, I think, her time. Yeah, maybe. Didn't have a look at our meetings. Uh I I think it wouldn't as it occurs I mean it would be it occurs in every meeting. So And I think it even has uh its own annotation, like digits or something. So that should be really easy to cut out. Yeah. I'm sure. Ah it's just to test the system, I think. So Mm they have to read numbers from Uh I didn't have a look at that. So They Mm-hmm. Uh th yeah. 'Kay. Um I just um wondered, so who's uh then doing um the frequencies on on the words, because I'm I think I will also um I could also make use of it um for the agreement and disagreement thing. Because I um I in my outline I talked about um using the um discourse acts first, and um then in the chunks of text I found looking for word patterns and so on. So um I would for example need the um most freq um frequent words. So if you cut off all that, I'd won't be use or Yeah, I I but I need it for my chunks then. I would You know? Yeah, but I'd uh I would like to look at the frequency of words in my um in the regions of text I found out to be interesting. So I wouldn't need it. It it would have to be re-calculated only for my segments. Huh? Uh uh mm. I think it would be, you know, l as as big at as the hot-spot annotation things. That's quite small, yeah, that's some utterances. Yes. Yeah, yeah. So I would probably just concatenate all my um text chunks and then let's say m I will run over it. Yes. Yes, definitely. Yeah, right. Ye M Um Jasmine, uh um what is um the text you're extracting uh looking like then at the end? Because um I I think it's actually very similar to what I did for my um speaker um uh extraction and I think I would ch perhaps have to change two lines of codes to get you um for each meeting a file that says fr from um this millisecond to this millisecond there was this sequence of words. And so on. So that's just changing two lines of code. And it would give you that. So Um yeah. So far I extracted um the dura durations. But it's from the words file. So I could just um contatenate concatenate um the words instead of the durations, and it should I mean Should be very straight-forward. I can try to do it and send it to you. Pe and you have a look at it, will it make sense for what you want. Yeah, uh p I mean it I just let it run over all the files. So Yes. I just ordered. Uh I ordered according to the um starting times of the utterances. What do you mean by diffe Yeah, I mean t I I have one what I give you would be one file for each meeting. Yeah, not for each meeting series. I didn't do that yet. Yeah, one group, yeah. Yeah, I mean there's one series that has just one meeting. Yes. Um the you you the data is of the form you have um three identification letter. So B_E_D_ or B_B_D_ or something, and that's always the same group. And then after that there's um a number like O_O_ one, O_O_ two. So, it's a Yeah, but that's that's really quite easy to see because they're named. Yes. But I I mean as um the start uh start times um start for each meeting at zero, you could just probably just um add the um final second time to the next meeting and so on and just put it all together. But then we would have to change um the information about who on which channel it was set, um to by which person it was set. And that is actually stored in another X_M_L_ document. Yeah, I w would then just not print out the um start and end times. No, it's for every single word. Or for every single utterance. Yeah, that depends on what you want. Yeah, but I do it with Perl, it's just string manipulation. So I would I mean I would just Sure. No, I didn't do a sea no. And you would want that all in one file for all the corpus? Or For the series. Yeah, I can directly put it into uh just like So uh only words um per meeting series. Uh-huh. Yes. Yeah, they will just I will just take I would uh take over the names they have anyway. Yeah, yeah. Yeah, one series has the um same three starting letters. So So only words and words and times. And you Yeah, you want it ordered. Okay. Okay, anybody Um ord base dot times. Yeah, and do you want Yeah, sometimes they're contained in one another. So Just after th mm-hmm. 'Kay. Ordered. Only words. Um and I think um for all the corpus, it's just from I know from other times, it's um nine megami byte to have I mean should be should be similar to have the words. Should be. Na um all the words together um for all the meetings. That's what I'm guessing that's, you know, um what I because nine mega-byte is what I got for when I said for every um utterance, this is goes from there to there and takes takes seconds. Oh. Yeah, I mean I'm it doing it for all of it. Doesn't matter. Yeah, I mean I hope it will be the same for the words. It's just what I I Mm-hmm. Mm. So so um I will probably send um just one file of the first meeting um to all those who need it so that you can have a look whether that's what you want. Yeah, I mean if it's just for one meeting, it's really not too big. Yeah. What do we have to demonstrate?\\nThe basic word importance is off-line as well. The combined measure might not be if we want to wait what the user has typed in into the search. Yeah. I'm not quite so what it did you want to do it, i you just wanted to assign Uh I thought about words. Mm. Mm okay. Yeah, but how about those words which don't carry any meaning at all, the um and uhs and something like that. Because if we if we average average over over a whole utterance all the words, and there are quite unimportant words in there, but quite important words as well, I think we should just disregard the the Okay. Alright. Yeah. But there is no I_D_ for an utterance I think. It's just for individual words. So how do we do that then? We for utterances as well. I think it's just for one word. So we have to Yeah. Uh I'm not quite sure, I have only seen that the uh the individual words have got an I_D_. Yeah. You always could have a look at the time stamps and then take the ones that uh belong together to form an utterance. Yeah, if they are already, there's it's easy but it would be possible. Uh yeah. Okay. You s uh you said you are currently in uh implementing the idea. What exactly are you computing? Okay. Okay. Mm-hmm. Mm-hmm. Yeah, I w I w I would need the raw text pretty soon because I have to find out um how I have to put the segments into bins. And then yeah. No, that's not necessary. Yes, I did. But um I've only just got the notes. I have to still have uh to order everything by the time and Yeah, I think it's quite easy after the Yeah. Yeah. So uh Mm-hmm. Yeah, b I uh w that's what I was uh thought. That you just combine them and then order the time stamps accordingly. Okay. Um what I found out was that there are quite a lot of things without without s time stamps in the beginning. Yeah, and uh X_M_L_ files. Yeah, that's just an I_D_ or something. I don't know. Just numbers. Yes, but what are the other things that's uh some kind of number? F maybe the file number or something that is in the beginning. What is that? Do you know? Um I think there are quite a lot of numbers in the beginning where n there is no time stamp for the numbers. It's Think they say um quite a lot of numbers and before that, uh um there's this number. Was it Yeah, there i are numbers in the um the W_ tag, but there are no time stamps. Yeah. Yeah, in the beginning as well sometimes, I think. At least I saw some. Yeah. Yeah. But what it is it actually that numbers? Okay, so but there are no time stamps annotated to that. It's it's quite strange. And also um there are different um combinations of letters. B_R_E_ and something like that. Is it everything ordered are the time stamps global or uh are they local at any point? Okay. Yeah, it's Rainbow. It's um I think it's just the dictionary in the first place. But Um no, I have to bin it up and so I will only have counts for each each bin or something. It's because um Rainbow is a text classification system. And I think it's not possible to have just one class. That's the problem. Maybe we could Yeah sure, you sure, we could do that, but I don't that makes sense. If we need just frequencies, maybe we should just calculate them by using Perl or something. I don't know. Yeah, it's quite easy to just count and s or sort them by um frequency. Just using a Perl script. Is it too big? Yeah. Hmm. I don't know how you how many terms you can handle in Perl. Mm yeah. Uh I can get all the raw text, but it has to be ordered still. So No, it isn't. Um it's in what is implemented in Rainbow is information gain, and I'm not quite sure how they calculate that. Yeah. Uh that's what Rainbow does. I think you j can just get probabilities for a certain words for each document. Certain Um we would have to look at that. Mm-hmm. Oh. Yeah, that's what I thought as well, that you that probably the the topic segment level is the most um informative for the words. Yeah, that's the problem. I don't know. Mm-hmm. So shall we sit together tomorrow then as well? Uh Okay. Um, yeah, w would it be best? At the moment it's it's just lines of Mm-hmm. Um Okay. So um you'd do you extract the words, the raw text, as well? Uh Okay. Mm-hmm. Print out. Okay. Okay, that Okay. So have we already extracted from all the files? Yeah. Did you also order Mm-hmm. Hmm. Hmm. Okay. Uh I don't need the times, I just need the words. But um Yeah, in the right order. Yes. Yeah, that doesn't matter too much, I think. Hmm. Mm-hmm. How long would it take to make the frequency counts with a Java hash table? Yeah. No, how long you would have to program something. Okay. Mm. Because it's quite easy in Perl as well, it's just a line of code for counting all the words and yeah, it's it's by hashes. Yeah. Yeah. 'Kay.\\nI I dry-read it the last time.. Next week. Yeah. Yeah. No. Uh mine's gonna be mostly using the off-line. But the actual stuff it's doing will be on-line. But it won't be very um processor intensive or memory intensive, I don't think. Don't think so. Yeah. Are we still gonna go for dumping it into a database? Are we still gonna dump it into a database? 'Cause if we are, I reckon we should all read our classes out of the database. It'll be so much easier. Well if we're gonna dump the part of it into a database anyway, we might as well dump all the fields we want into the database, calculate everything from there. Then we don't even have to worry that much about the underlying X_M_L_ representation. We can just query it. Well if we're gonna do that, we should try and store everything in in an X_M_L_ format as well. Yeah. Yeah. Well we don't even need to do that, 'cause if we got our information density calculated off-line, so all we do is treat the whole lot as one massive document. I mean they'll it's not gonna be so big that we can't load in a information density for every utterance. And we can just summarise based on that. I think you can do it on-line. I don't think there's really much point in doing like that when it's just gonna feed off in the end the information density measure basically. And that's all calculated off-line. So what you're really doing is sorting a list, is the p computationally hard part of it. Well like the ideas we're calculating are information density all off-line first for every utterance in the whole corpus, right? So what you do is you say if you're looking at a series of meetings, you just say well our whole document comprises of all these stuck together. And then all you have to do is sort them by j information density. Like maybe weighted with the search terms, and then extract them. I don't think it's too slow to do on-line, to be honest. Is that Yeah. Well, on the utterance level I was thinking. So the utterances with the highest like mean information density. Well the trouble with doing it on the word level is if you want the audio to synch up, you've got no way of getting in and extracting just that word. I mean it's impossible. For every single word? Oh, okay. Yeah. I don't think that will do it. We'll have to buffer it. Well the skimming's gonna use the importance. But like at first it's just gonna be I_D_F_. Well mostly skimming, yeah. Yeah. Well the nice thing about that is it will automatically be in sentences. Well more or less. So it will make more sense, and if you get just extract words. Yeah. I see it. But it'll need to be calculated at word level though because otherwise there won't be enough occurrences of the terms to make any meaningful sense. Yeah. Yeah, I reckon you can just mean it over the sentence. I think we should filter them. Maybe we should have like um a cut-off. So it a w word only gets a value if it's above a certain threshold. So anything that has less than say nought point five importance gets assigned to zero. Yeah, that's the other th Yeah. I think we'll have to buffer the audio. But I don't think it will be very hard. I think it would be like an hour or two's work. Like just build an another f wave file essentially. Yeah, I mean I bet there would be packages In memory, yeah. So just like unp there's bound to be like a media wave object or something like that. And just build one in memory. I don't know. I have no idea. But it must have like classes for dealing with files. And if it has classes for concatenating files, you can do it in memory. So Well what I think I might try and build is basically a class that you just feed it a linked list of um different wave-forms, and it will just string them all together with maybe, I don't know, tenth of a second silence in between each one or something like that. Normalise it, yeah. Oh yeah, yeah, we'll need that. We also really wanna be able to search by who's speaking as well. It doesn't matter, 'cause all the calculation's done off-line. That's easy. You just like create a new X_M_L_ document in memory. I don't think it's really that much of a problem because if it's too big, what we can do is just well all the off-line stuff doesn't really matter. And all we can do is just process a bit at a time. Like for summarisation, say we wanted a hundred utterances in the summary, just look at the meeting, take the top one hundred utterances in each other meeting. If it scores higher than the ones already in the summary so far, just replace them. And then you only have to process one meeting at a time. Okay, so maybe we should build a b store a mean measure for the segments and meetings as well? And speaker. Speaker and um topic segmenting we'll need as well. Yeah. Well yeah, and then it'll f preserve the order when it's displayed the Yeah. Yeah. Yeah, I think so. So we should basically make our own X_M_L_ document in memory that everyone's um module changes that, rather than the underlying data. And then have that X_M_L_ uh NITE X_M_L_ document tied to the interface. Well, you can make it in a file if you want. Mm-hmm. They are utterances, aren't they? The segments are utterances, aren't they? Yeah. Alright, okay. Well, that's easy. Well it's close enough, isn't it? It may not be exact every time, but it's a so sort of size we're looking for. Yeah, yeah. Yeah. But why don't we just write it as a new X_M_L_ file? Can NITE handle just loading arbitrary uh new like attributes and stuff? I mean, I would have thought they'd make it able to. Yeah. So why do we need to have two X_M_L_ trees in memory at once? The other thing is that would mean we'd be using their parser as well, which means we wouldn't have to parse anything, which be quite nice. 'Cause their parser is probably much faster than anything we've come up with anyway. Yeah, I mean we can process it in chunks if it gets too big basically. We can just process it all in chunks if it gets too big to load it into memory. I think we probably want to store Sorry. I think we probably want to store um a hierarchical information density as well. So like an informan mation density score for each meeting and each topic segment. 'Cause otherwise we'd be recalculating the same thing over and over and over again. Yeah. And that will obviously make it much easier to display. Well it may not for the whole meeting, but like Yeah, exactly. Yeah. Well, we can start off like that. Well I was gonna start off I've v got sort of half-way through implementing one that does just I_D_F_. And then just I can change that to work on whatever. Yeah. And it should be weighted by stuff like the hot spots and um the key-words in the search and stuff like that. Did he not say something about named entities? So I thought he said there wasn't very many. Yeah. Yeah. It's not T_F_I_D_F_, it's just inverse document frequency. 'Cause it's really easy to do basically. There's just like for a baseline really. Well, I'm half-way through. It's not working yet, but it will do. Um yeah. And then averaging it over the utterances. But it's not like um related to the corpus at all. It's just working on an arbitrary text file at the moment. No. It would be useful to know how everyone's gonna store their things though. Yeah. Yeah. Well I've got like a few hours free. Like after this. It's the most boring task. Yeah. Or at least um simple versions of them. So maybe we should try doing something really simple, like just displaying a whole meeting. And like just being able to scroll through it or something like that. Yeah. Are you free after this? How about Friday then. 'Cause I'm off all Friday. Uh Wednesday I've got a nine 'til twelve. Yeah, nothing in the afternoon. I've got nothing in the afternoon. So Okay. So you ha yeah. Where about, just in Appleton Tower? Uh I'll be in um the Appleton Tower anyway. Um well I'll be there from twelve. I've got some other stuff that needs done on Matlab, so if you're not there at twelve, I can just work on that. So Yeah. Why w Yeah. I'm just building a dictionary. Oh, mine's just gonna use the um hash map one in um Java. 'Cause I'm only gonna do it on small documents. It's just like bef until the information density is up and running. Just something to get give me something to work with. So it's only gonna use quite small documents, you see, to start with. Why does it need to be classified into like different segments? Can we just fill a second class with junk that we don't care about? Like, I don't know, copies of Shakespeare or something. 'Cause if what we're looking for is the um frequency statistics, I don't see how that would be changed by the classification. I the Well there maybe another tool available? Yeah. Um I can't remember who's got it. Might be WordNet. But one of these big corpuses has a list of stop words that you can download and they're just basically lists of really uninteresting boring words that we could filter out before we do that. It's like that's one the papers I read, that's um one things they did right at the beginning is they've got this big s stop-list and they just ignore all of those throughout the experiment. Yeah, I it would be useful for me as well. It uh I think that'd be useful for me as well. Yeah. Yeah. Well all you really wanna do is look into getting some sub-set of the ICSI corpus off the DICE machines. 'Cause I hate working on DICE. It's awful. Like so I can use my home machine. ha has a C_D_ burner though. has a C_D_ burner. Yeah. The right-hand corner, far right. Yeah. How big is it without um the WAV files and stuff? 'Cause I could just say at um going over S_C_P_ one night and just leave it going all night if I had to. It's yeah, I mean the wave data are obviously not gonna get off there completely. Really? Oh right? I'll see if I can S_C_P_ it, I suppose. I've got a Linux box and a Windows box. So Broad-band. Put it on to C_D_. I can if I get down I can put to C_D_. Yeah. I'm not sure if there's enough space. Is how much do we get? Really? Okay. Yeah, but I can do it from that session, can't I? You can compress it from a remote session and S_C_P_ it from the same session? Do you think? Yeah. Oh no no, I was thinking of SSHing just into some machine and then just SCPing it from there. Yeah. I mean it has to go through the gateway. But Can you not do that? Mm, I see. Yeah. So you could just But th first, uh how big are the chunks? How big are the chunks you're looking at? So quite small then. So you could just um you could use just the same thing we used to build the big dictionary. You just do that on-line 'cause that won't take long to build a little dictionary that big, will it. I mean just use the same tool that we use. Yeah. Yeah. It doesn't need ordered, no. Um well that's the t are you using T_F_I_D_F_ for the information density? Alright, okay. Like 'cause frequency would be useful, I think. But um depending on the context, the size, and what we consider a document in the sense of calculating T_F_I_D_F_ is gonna change. Which might need thinking about. I think it would be useful, yeah. Well you need the raw frequency as well. But um you also need how many times things occur within each document. And um what we consider a document's gonna depend on our context, I think. 'Cause if we're looking at the whole lot of meetings, we'll consider each meeting a document in sort of terms of this algorithm. And if we're viewing like say just a small topic segment you might look at even each utterance as a small document. Yeah, but the thing is um It's gonna need some th th thought of how we Actually maybe it doesn't actually matter. Maybe if you just do it once at the highest level, it it will be fine. But I was just thinking it might be difficult to calculate the T_F_I_D_F_ off-line for all the different levels we might want. 'Cause if we're gonna allow disjoint segments for example, then how are we gonna know what's gonna be in context at any given time? But I suppose if you just did it globally, treating a meeting as a document, it'd probably still be work out fine, because you'd only be comparing to ones within the context. Uh I don't know, I thought were you gonna use that in the end? The information density. Oh sorry, that's what I mean. Like um yeah, for each word or whatever, but across the whole lot is what I mean by highest level. Like across the whole corpus. Yeah, but you'd probably look at each meeting as a document. Mm possibly. Are they big enough to get anything meaningful out of? Well yeah, that is not it's not an issue. You just concatenate an X_M_L_ file together. but we still want to have like a notion of meetings for the user. Yeah, sure. Yeah, you just like whatever you want to look at, you just jam together into an X_M_L_ file and that's your meeting, even though bits of it may come from all over the place or whatever. I mean I don't see why that's really a big problem. So basically what you're saying is you can take an arbitrary amount of data and process it with the same algorithm. It doesn't matter conceptually what that data is. It could be a meeting. it could be two utterances. it could be a meeting plus half a meeting from somewhere else. I don't think it's very difficult though. I mean what you do is you just build an X_M_L_ file, and if you want it to get down to the utterances, you'd go to the leaves. And then if you wanted the next level up, you'd go to the parents of those and like just go from like the leaves inwards towards the branch to build up things like um you know, when you click on a segment, it's gonna have like words or whatever that are important. As long as like the algorithms are designed um with it in mind, I don't think it's a very big problem. Well like say you had um like say for a meeting, right, you've got like uh say a hierarchy that looks quite big, like this. And like the utterances come off of here maybe. Then when whatever your algorithm is doing, as long as when you're working with utterances, you go for all the leaves, like then if you need something next up, so like a topic segment, you'd go to here. But if you were looking at say this one, so only went like this. Right, so you it's same, you'd start with the leaves, and you go oh, I want a topic segment. So I go one layer up. See, and then if you're working with just a topic segment in there, it's the only thing you have to worry about. And like each time you want a higher level, you just need to go up the tree. And as long as your algorithm respects that, then we can just process any arbitrary X_M_L_ file with whatever hierarchical structure we want. A meeting, say, and that would be a topic segment. So I think as long as you build an algorithm that respects whatever structure's in the file, rather than imposing its own structure Well no, it doesn't have to be. But I mean it could be as many nodes as you want. Like this one could be deeper maybe, say. So then you'd start with all your utterances here, and when you go up to get topic segments, you go to here here here here here here here. That might be a bit confusing though 'cause you have things on different levels. Well Wednesday. Yeah. Yeah. So we'll see if we can get like a mini-browser just displays two things synched together of some kind. Yeah. Yeah. It'd be useful. I don't know who you see about that though. I d have no idea. I've probably got a reasonable amount because um everything on my DICE account can actually be deleted 'cause I store it all at home as well. Is that guaranteed to stay, the Maybe you should send a support form. Just say we want some web space. Listen to. Yeah. 'Cause that'd be really useful is if we had a big directory. Especially for transferring stuff. Having said that, are we allowed to take a copy of the ICSI corpus? Something we should probably ask before we do it.. Okay. Okay. No, me neither. Might be funny to see what is summarised the whole corpus as anyway. I think it'd be very useful. But We can just change the code. Is that it? That's quite good. Yeah. I could just use it with the frequency, I think, until the information density thing's finished. That would be really useful. If you're doing it in Java, could you um serialize the output as well as writing it to a file? If you're doing it in Java, could you serialize the um dictionary, yeah, as well as writing it to a file? It's really easy. I don't see why it'd be any more massive than the file. Yeah. It just saves you parsing the um file representation of it. And now 'cause I would be using it in Java anyway. So I'd just be building the data structure again. Yeah, but it seems like a bit silly to be parsing it over and over again kinda thing. I would've thought that um I think all the collections and things implement serializable already. I think they might do. Tonight I'll try and um I'll either work some more on uh the T_F_I_D_F_ summarizer or do the audio thing. Yeah. Do we have to demonstrate something next week? Yeah. Yeah, I know. I think it's 'cause we had to specify it ourselves that it's not as um like focus the specification of most um work we have to do. Yeah. Once we start doing it it will all become more or less obvious I think anyway.\\nOkay. Does anyone want to see uh Steve's feedback from the specification? Right. Not really, um just what he's talking about, like duplication of effort and Like duplication of effort and stuff, and um yeah, he was saying that we should maybe uh think about having a prototype for week six, which is next week. Yeah. So we should probably prioritize our packages. Mm. Yeah. Yeah. Hmm. Has has anyone actually looked at the Java code for the, huh? Hmm. Yeah, I think so. Yeah, I I don't know about the search functionality, that might be online. Depends how it's gonna work. Yeah. Mm-hmm. Yeah, that makes sense. Hmm. Hmm. Yeah, you just concatenate them together. Hmm. Yeah. It just means it loads on demand. It only loads when it needs a particular type of file. Like when it's being accessed. Yeah, I think that's the idea, it just loads the particular ones it needs. But if you were doing a search over the whole corpus you'd have to load them all. Hmm. Mm. Hmm. Yeah, we do not want it in to develop a little tree display as well for multiple results. Yeah, but that'd be quite easy to do. You just need to find the time stamp. Yeah. Yeah. Yeah, I think I think those segments for each utterance are split up. Think so. Yeah, I'm pretty sure it's already there. Pretty sure that's already there. The the utterances are numbered. Hmm. Yeah, I think so. Ye that's the impression I get, yeah. Oh. Hmm. Ye Mm. Yeah, uh Right. Okay. Topics, yeah. Yeah, I think that's the right one. Hmm. Hmm. Mm-hmm. Mm. Hmm. Yeah, that'd be much more efficient to do that. Yeah. Hmm. Hmm. Yeah, you're able to do that in Java, yeah? Yeah. Huh. Hmm. Yeah, I've had a b I've had a look at the the topic segments, how it's stored. And then yeah, th those are few per meeting, and it um well, it gives a time stamp and inside each one there's uh the actual like utterance segments. And the list of them that occurred. And they're all numbered. Um so that's where that's stored. Yeah, so I guess um if I'm gonna be segmenting it with a L_C_ seg then that's like same format I'd want to um put it back out in so it'd be equivalent. Well, like the integration. What do you mean, integration? Hmm. I don't know. I don't think anyone's been allocated to do that yet. Yeah, yeah. Yeah, definitely. Hmm, yeah. Yeah, it c could be difficult, yeah. Yeah. Well I guess the important thing is to get the crucial m modules built. Ye yeah. Yeah, and then Yeah, and then we'll maybe have to prioritize somebody into just integrating it. Mm-hmm. Yeah, I think so. Uh yeah. Hmm. Yeah, yeah. Jasmine, I thought you just said that you'd uh looked at extracting the text. Yeah. So you you said you did it in Python, yeah? Yeah, did you use uh b the X_L_ uh X_M_L_ parser in Python? Right. Yeah, sounds pretty good. So um 'cause, yeah, I was having a look in it a look at it as well and I noticed the um the speakers are all in that separate file? So did did you have to combine them all and and then re-order them? Yeah. Ye yeah, c Right. Yeah, so that's approach um well, I was going to do. So yeah, we may as well collaborate. In the word files? I'm not sure I what you mean. Oh right. Hmm. Hmm. Mm I thought they were local to th a particular meeting. Hmm. Mm is there anything else we should discuss? Yeah, should we not have like a group directory or something where we can put all our code in and that kinda thing? Hmm. I've gotten mm hardly any Hmm. Yeah, we can ask Steve if um we can get space. Yeah, uh we could do that. Yeah, I'm sure he had to deal with that last year. Yeah. Hmm. That sounds good. Hmm. Hmm. Yeah, that's what I'm gonna need. Yes. Yeah, it's just mo changing it a bit. Yeah. No, but uh that's what M_L_C_ seg does. It it marks the end of each segment. Yeah. Yeah. Oh. Yeah. Yeah, for me it's better if they're by meeting. Then that'll be really easy to do once they've got the raw text. It's just a case of running the script. Yeah, I mean hopefully this week. Alright. And we could Don't know. Suppose we're just getting on with all our components. So I know. Wa Yeah. Yeah, he suggested that we could have an uh initial prototype. I know, I'd b I'd be surprised if we can get anything working by next week. Alright.\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3U0_21manVl",
        "outputId": "9698c588-b3ca-4c88-9967-b4c8547a2bdb"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(transcript_sum)\n",
        "tokens = [token.text for token in doc]\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'\",\n",
              " 'Kay',\n",
              " '.',\n",
              " 'Gosh',\n",
              " '.',\n",
              " \"'\",\n",
              " 'Kay',\n",
              " '.',\n",
              " 'Is',\n",
              " 'there',\n",
              " 'much',\n",
              " 'more',\n",
              " 'in',\n",
              " 'it',\n",
              " 'than',\n",
              " 'he',\n",
              " 'd',\n",
              " 'Is',\n",
              " 'there',\n",
              " 'much',\n",
              " 'more',\n",
              " 'in',\n",
              " 'it',\n",
              " 'than',\n",
              " 'he',\n",
              " 'said',\n",
              " 'yesterday',\n",
              " '?',\n",
              " 'Mm',\n",
              " '.',\n",
              " 'Hmm',\n",
              " '.',\n",
              " 'Hmm',\n",
              " '?',\n",
              " 'Yeah',\n",
              " ',',\n",
              " 'now',\n",
              " 'I',\n",
              " \"'d\",\n",
              " 'say',\n",
              " 'if',\n",
              " 'for',\n",
              " 'the',\n",
              " 'prototype',\n",
              " 'if',\n",
              " 'we',\n",
              " 'just',\n",
              " 'like',\n",
              " 'wherever',\n",
              " 'possible',\n",
              " 'p',\n",
              " 'chunk',\n",
              " 'in',\n",
              " 'the',\n",
              " 'stuff',\n",
              " 'that',\n",
              " 'we',\n",
              " 'have',\n",
              " 'um',\n",
              " 'pre',\n",
              " '-',\n",
              " 'annotated',\n",
              " 'and',\n",
              " 'stuff',\n",
              " ',',\n",
              " 'and',\n",
              " 'for',\n",
              " 'the',\n",
              " 'stuff',\n",
              " 'that',\n",
              " 'we',\n",
              " 'do',\n",
              " \"n't\",\n",
              " 'have',\n",
              " 'pre',\n",
              " '-',\n",
              " 'annotated',\n",
              " 'write',\n",
              " 'like',\n",
              " 'a',\n",
              " 'stupid',\n",
              " 'baseline',\n",
              " ',',\n",
              " 'then',\n",
              " 'we',\n",
              " 'should',\n",
              " 'probably',\n",
              " 'be',\n",
              " 'able',\n",
              " 'to',\n",
              " 'basically',\n",
              " 'that',\n",
              " 'means',\n",
              " 'we',\n",
              " 'focus',\n",
              " 'on',\n",
              " 'on',\n",
              " 'the',\n",
              " 'interface',\n",
              " 'first',\n",
              " 'sort',\n",
              " 'of',\n",
              " ',',\n",
              " 'so',\n",
              " 'that',\n",
              " 'we',\n",
              " 'we',\n",
              " 'take',\n",
              " 'the',\n",
              " 'the',\n",
              " 'ready',\n",
              " '-',\n",
              " 'made',\n",
              " 'parts',\n",
              " 'and',\n",
              " 'just',\n",
              " 'see',\n",
              " 'how',\n",
              " 'we',\n",
              " 'get',\n",
              " 'them',\n",
              " 'work',\n",
              " 'together',\n",
              " 'in',\n",
              " 'the',\n",
              " 'interface',\n",
              " 'the',\n",
              " 'way',\n",
              " 'we',\n",
              " 'want',\n",
              " 'and',\n",
              " 'and',\n",
              " 'then',\n",
              " 'we',\n",
              " 'have',\n",
              " 'a',\n",
              " 'working',\n",
              " 'prototype',\n",
              " '.',\n",
              " 'And',\n",
              " 'then',\n",
              " 'we',\n",
              " 'can',\n",
              " 'go',\n",
              " 'back',\n",
              " 'and',\n",
              " 'replace',\n",
              " 'pieces',\n",
              " 'either',\n",
              " 'by',\n",
              " 'our',\n",
              " 'own',\n",
              " 'components',\n",
              " 'or',\n",
              " 'by',\n",
              " 'more',\n",
              " 'sophisticated',\n",
              " 'compo',\n",
              " 'po',\n",
              " 'components',\n",
              " 'of',\n",
              " 'our',\n",
              " 'own',\n",
              " '.',\n",
              " 'So',\n",
              " 'it',\n",
              " \"'s\",\n",
              " 'probably',\n",
              " 'feasible',\n",
              " '.',\n",
              " 'The',\n",
              " 'thing',\n",
              " 'is',\n",
              " 'I',\n",
              " \"'m\",\n",
              " 'away',\n",
              " 'this',\n",
              " 'weekend',\n",
              " '.',\n",
              " 'So',\n",
              " 'that',\n",
              " \"'s\",\n",
              " 'for',\n",
              " 'me',\n",
              " 'Oh',\n",
              " 'yeah',\n",
              " ',',\n",
              " 'um',\n",
              " 'yeah',\n",
              " '.',\n",
              " 'No',\n",
              " '.',\n",
              " 'But',\n",
              " 'also',\n",
              " 'I',\n",
              " 'might',\n",
              " 'like',\n",
              " 'the',\n",
              " 'the',\n",
              " 'similarity',\n",
              " 'thing',\n",
              " ',',\n",
              " 'like',\n",
              " 'my',\n",
              " 'just',\n",
              " 'my',\n",
              " 'matrix',\n",
              " 'itself',\n",
              " 'for',\n",
              " 'my',\n",
              " 'stuff',\n",
              " ',',\n",
              " 'I',\n",
              " 'c',\n",
              " 'I',\n",
              " 'I',\n",
              " 'think',\n",
              " 'I',\n",
              " 'can',\n",
              " 'do',\n",
              " 'that',\n",
              " 'fairly',\n",
              " 'quickly',\n",
              " 'because',\n",
              " 'I',\n",
              " 'have',\n",
              " 'the',\n",
              " 'algorithms',\n",
              " '.',\n",
              " 'Yeah',\n",
              " ',',\n",
              " 'I',\n",
              " 'think',\n",
              " 'today',\n",
              " \"'s\",\n",
              " 'meeting',\n",
              " 'is',\n",
              " 'really',\n",
              " 'the',\n",
              " 'one',\n",
              " 'where',\n",
              " 'we',\n",
              " 'where',\n",
              " 'we',\n",
              " 'sort',\n",
              " 'of',\n",
              " 'settle',\n",
              " 'down',\n",
              " 'the',\n",
              " 'data',\n",
              " 'structure',\n",
              " 'and',\n",
              " 'as',\n",
              " 'soon',\n",
              " 'as',\n",
              " 'we',\n",
              " 'have',\n",
              " 'that',\n",
              " ',',\n",
              " 'uh',\n",
              " 'probably',\n",
              " 'like',\n",
              " 'after',\n",
              " 'today',\n",
              " \"'s\",\n",
              " 'meeting',\n",
              " ',',\n",
              " 'we',\n",
              " 'then',\n",
              " 'actually',\n",
              " 'need',\n",
              " 'to',\n",
              " 'well',\n",
              " 'go',\n",
              " 'back',\n",
              " 'first',\n",
              " 'of',\n",
              " 'all',\n",
              " 'and',\n",
              " 'look',\n",
              " 'at',\n",
              " 'NITE',\n",
              " 'X_M_L',\n",
              " '_',\n",
              " 'to',\n",
              " 'see',\n",
              " 'in',\n",
              " 'how',\n",
              " 'far',\n",
              " 'that',\n",
              " 'that',\n",
              " 'which',\n",
              " 'we',\n",
              " 'want',\n",
              " 'is',\n",
              " 'compatible',\n",
              " 'with',\n",
              " 'that',\n",
              " 'which',\n",
              " 'NITE',\n",
              " 'X_M_L',\n",
              " '_',\n",
              " 'offers',\n",
              " 'us',\n",
              " '.',\n",
              " 'And',\n",
              " 'then',\n",
              " 'just',\n",
              " 'sort',\n",
              " 'of',\n",
              " 'everyone',\n",
              " 'make',\n",
              " 'sure',\n",
              " 'everyone',\n",
              " 'understand',\n",
              " 'the',\n",
              " 'interface',\n",
              " '.',\n",
              " 'So',\n",
              " 'I',\n",
              " 'think',\n",
              " 'if',\n",
              " 'today',\n",
              " 'we',\n",
              " 'decide',\n",
              " 'on',\n",
              " 'what',\n",
              " 'data',\n",
              " 'we',\n",
              " 'wanna',\n",
              " 'have',\n",
              " 'now',\n",
              " ',',\n",
              " 'and',\n",
              " 'and',\n",
              " 'later',\n",
              " ',',\n",
              " 'maybe',\n",
              " 'even',\n",
              " 'today',\n",
              " ',',\n",
              " 'we',\n",
              " 'go',\n",
              " 'and',\n",
              " 'look',\n",
              " 'at',\n",
              " 'NITE',\n",
              " 'X_M_L',\n",
              " '_',\n",
              " 'or',\n",
              " 'some',\n",
              " 'of',\n",
              " 'us',\n",
              " 'look',\n",
              " 'at',\n",
              " 'NITE',\n",
              " 'X_M_L',\n",
              " '_',\n",
              " 'in',\n",
              " 'a',\n",
              " 'bit',\n",
              " 'more',\n",
              " 'detail',\n",
              " ',',\n",
              " 'just',\n",
              " 'trying',\n",
              " 'to',\n",
              " 'make',\n",
              " 'some',\n",
              " 'sense',\n",
              " 'of',\n",
              " 'that',\n",
              " 'code',\n",
              " 'and',\n",
              " 'see',\n",
              " 'how',\n",
              " 'does',\n",
              " 'the',\n",
              " 'representation',\n",
              " 'work',\n",
              " 'in',\n",
              " 'their',\n",
              " 'system',\n",
              " '.',\n",
              " 'And',\n",
              " 'then',\n",
              " 'sort',\n",
              " 'of',\n",
              " 'with',\n",
              " 'that',\n",
              " 'knowledge',\n",
              " 'we',\n",
              " 'should',\n",
              " 'be',\n",
              " 'able',\n",
              " 'to',\n",
              " 'then',\n",
              " 'say',\n",
              " 'okay',\n",
              " ',',\n",
              " 'that',\n",
              " 'type',\n",
              " 'of',\n",
              " 'NITE',\n",
              " 'X_M_L',\n",
              " '_',\n",
              " 'data',\n",
              " 'we',\n",
              " 'wanna',\n",
              " 'load',\n",
              " 'into',\n",
              " 'it',\n",
              " ',',\n",
              " 'and',\n",
              " 'this',\n",
              " 'is',\n",
              " 'how',\n",
              " 'everyone',\n",
              " 'can',\n",
              " 'access',\n",
              " 'it',\n",
              " ',',\n",
              " 'and',\n",
              " 'then',\n",
              " 'we',\n",
              " 'should',\n",
              " 'be',\n",
              " 'able',\n",
              " 'to',\n",
              " 'go',\n",
              " 'from',\n",
              " 'there',\n",
              " '.',\n",
              " 'No',\n",
              " '.',\n",
              " 'I',\n",
              " \"'ve\",\n",
              " 'looked',\n",
              " 'looked',\n",
              " 'at',\n",
              " 'the',\n",
              " 'documentation',\n",
              " 'and',\n",
              " 'n',\n",
              " 'like',\n",
              " 'seen',\n",
              " 'enough',\n",
              " 'to',\n",
              " 'make',\n",
              " 'me',\n",
              " 'think',\n",
              " 'that',\n",
              " 'we',\n",
              " 'want',\n",
              " 'to',\n",
              " 'use',\n",
              " 'the',\n",
              " 'NITE',\n",
              " 'X_M_L',\n",
              " '_',\n",
              " 'framework',\n",
              " 'because',\n",
              " 'um',\n",
              " 'they',\n",
              " 'have',\n",
              " 'a',\n",
              " 'good',\n",
              " 'a',\n",
              " 'event',\n",
              " 'model',\n",
              " 'that',\n",
              " 'synchronizes',\n",
              " 'sort',\n",
              " 'of',\n",
              " 'the',\n",
              " 'data',\n",
              " 'and',\n",
              " 'and',\n",
              " 'every',\n",
              " 'display',\n",
              " 'element',\n",
              " '.',\n",
              " 'So',\n",
              " 'that',\n",
              " 'takes',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'work',\n",
              " 'away',\n",
              " 'from',\n",
              " 'us',\n",
              " '.',\n",
              " 'Sort',\n",
              " 'of',\n",
              " 'that',\n",
              " 'would',\n",
              " 'be',\n",
              " 'a',\n",
              " 'reason',\n",
              " 'for',\n",
              " 'staying',\n",
              " 'within',\n",
              " 'their',\n",
              " 'framework',\n",
              " 'and',\n",
              " 'using',\n",
              " 'their',\n",
              " 'general',\n",
              " 'classes',\n",
              " '.',\n",
              " 'But',\n",
              " 'beyond',\n",
              " 'that',\n",
              " 'I',\n",
              " 'have',\n",
              " \"n't\",\n",
              " 'looked',\n",
              " 'at',\n",
              " 'it',\n",
              " 'at',\n",
              " 'all',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'something',\n",
              " 'we',\n",
              " 'should',\n",
              " 'really',\n",
              " 'do',\n",
              " '.',\n",
              " 'Who',\n",
              " 'actually',\n",
              " 'like',\n",
              " 'for',\n",
              " 'this',\n",
              " 'whole',\n",
              " 'discussion',\n",
              " 'I',\n",
              " 'mean',\n",
              " ',',\n",
              " 'who',\n",
              " 'of',\n",
              " 'us',\n",
              " 'is',\n",
              " 'doing',\n",
              " 'stuff',\n",
              " 'that',\n",
              " 'is',\n",
              " 'happening',\n",
              " 'on',\n",
              " '-',\n",
              " 'line',\n",
              " 'and',\n",
              " 'who',\n",
              " 'of',\n",
              " 'us',\n",
              " 'is',\n",
              " 'doing',\n",
              " 'stuff',\n",
              " 'that',\n",
              " \"'s\",\n",
              " 'happening',\n",
              " 'off',\n",
              " '-',\n",
              " 'line',\n",
              " '?',\n",
              " 'Like',\n",
              " 'my',\n",
              " 'data',\n",
              " 'is',\n",
              " 'coming',\n",
              " 'c',\n",
              " 'Hmm',\n",
              " '?',\n",
              " 'Yeah',\n",
              " '.',\n",
              " 'Okay',\n",
              " '.',\n",
              " 'Okay',\n",
              " '.',\n",
              " \"'\",\n",
              " 'Kay',\n",
              " '.',\n",
              " 'So',\n",
              " 'basically',\n",
              " 'apart',\n",
              " 'from',\n",
              " 'the',\n",
              " 'display',\n",
              " 'module',\n",
              " ',',\n",
              " 'the',\n",
              " 'i',\n",
              " 'the',\n",
              " 'display',\n",
              " 'itself',\n",
              " ',',\n",
              " 'we',\n",
              " 'do',\n",
              " \"n't\",\n",
              " 'have',\n",
              " 'an',\n",
              " 'extremely',\n",
              " 'high',\n",
              " 'degree',\n",
              " 'of',\n",
              " 'interaction',\n",
              " 'between',\n",
              " 'sort',\n",
              " 'of',\n",
              " 'our',\n",
              " 'modules',\n",
              " 'that',\n",
              " 'create',\n",
              " 'the',\n",
              " 'stuff',\n",
              " 'and',\n",
              " 'and',\n",
              " 'the',\n",
              " 'interface',\n",
              " ',',\n",
              " 'so',\n",
              " 'the',\n",
              " 'interface',\n",
              " 'is',\n",
              " 'mainly',\n",
              " 'while',\n",
              " 'it',\n",
              " \"'s\",\n",
              " 'running',\n",
              " 'just',\n",
              " 'working',\n",
              " 'on',\n",
              " 'data',\n",
              " 'that',\n",
              " \"'s\",\n",
              " 'just',\n",
              " 'loaded',\n",
              " 'from',\n",
              " 'a',\n",
              " 'file',\n",
              " ',',\n",
              " 'I',\n",
              " 'guess',\n",
              " '.',\n",
              " 'There',\n",
              " 'is',\n",
              " \"n't\",\n",
              " 'Yeah',\n",
              " ',',\n",
              " 'I',\n",
              " 'know',\n",
              " '.',\n",
              " 'Th',\n",
              " 'Yeah',\n",
              " ',',\n",
              " 'the',\n",
              " 'search',\n",
              " 'is',\n",
              " 'I',\n",
              " 'guess',\n",
              " 'the',\n",
              " 'search',\n",
              " 'is',\n",
              " 'sort',\n",
              " 'of',\n",
              " 'a',\n",
              " 'strange',\n",
              " 'beast',\n",
              " 'anyway',\n",
              " 'because',\n",
              " 'for',\n",
              " 'the',\n",
              " 'search',\n",
              " 'we',\n",
              " \"'re\",\n",
              " 'leaving',\n",
              " 'the',\n",
              " 'NITE',\n",
              " 'X_M_L',\n",
              " '_',\n",
              " 'framework',\n",
              " '.',\n",
              " 'Um',\n",
              " 'but',\n",
              " 'that',\n",
              " \"'s\",\n",
              " 'still',\n",
              " 'sort',\n",
              " 'of',\n",
              " 'that',\n",
              " \"'s\",\n",
              " 'good',\n",
              " '.',\n",
              " 'That',\n",
              " 'means',\n",
              " 'that',\n",
              " 'at',\n",
              " 'least',\n",
              " 'like',\n",
              " 'we',\n",
              " 'do',\n",
              " \"n't\",\n",
              " 'have',\n",
              " 'the',\n",
              " 'type',\n",
              " 'of',\n",
              " 'situation',\n",
              " 'where',\n",
              " 'somebody',\n",
              " 'has',\n",
              " 'to',\n",
              " 'do',\n",
              " 'like',\n",
              " 'a',\n",
              " 'billion',\n",
              " 'calculations',\n",
              " 'on',\n",
              " 'on',\n",
              " 'data',\n",
              " 'on',\n",
              " '-',\n",
              " 'line',\n",
              " '.',\n",
              " \"'Cause\",\n",
              " 'that',\n",
              " 'would',\n",
              " 'make',\n",
              " 'it',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'more',\n",
              " 'like',\n",
              " 'that',\n",
              " 'would',\n",
              " 'mean',\n",
              " 'that',\n",
              " 'our',\n",
              " 'interface',\n",
              " 'for',\n",
              " 'the',\n",
              " 'data',\n",
              " 'would',\n",
              " 'have',\n",
              " 'to',\n",
              " 'be',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'more',\n",
              " 'careful',\n",
              " 'about',\n",
              " 'how',\n",
              " 'it',\n",
              " 'performs',\n",
              " 'and',\n",
              " 'and',\n",
              " 'everything',\n",
              " '.',\n",
              " 'And',\n",
              " 'nobody',\n",
              " 'is',\n",
              " 'modifying',\n",
              " 'that',\n",
              " 'data',\n",
              " 'at',\n",
              " 'at',\n",
              " 'on',\n",
              " '-',\n",
              " 'line',\n",
              " 'time',\n",
              " 'at',\n",
              " 'all',\n",
              " 'it',\n",
              " 'seems',\n",
              " '.',\n",
              " 'Nobody',\n",
              " \"'s\",\n",
              " 'making',\n",
              " 'any',\n",
              " 'changes',\n",
              " 'to',\n",
              " 'the',\n",
              " 'actual',\n",
              " 'data',\n",
              " 'on',\n",
              " '-',\n",
              " 'line',\n",
              " '.',\n",
              " 'So',\n",
              " 'that',\n",
              " \"'s\",\n",
              " 'actually',\n",
              " 'making',\n",
              " 'it',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'easier',\n",
              " '.',\n",
              " 'That',\n",
              " 'basically',\n",
              " 'means',\n",
              " 'our',\n",
              " 'browser',\n",
              " 'really',\n",
              " 'is',\n",
              " 'a',\n",
              " 'viewer',\n",
              " 'mostly',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " \"n't\",\n",
              " 'doing',\n",
              " 'much',\n",
              " 'with',\n",
              " 'the',\n",
              " 'data',\n",
              " 'except',\n",
              " 'for',\n",
              " 'sort',\n",
              " 'of',\n",
              " 'selecting',\n",
              " 'a',\n",
              " 'piece',\n",
              " 'piece',\n",
              " 'of',\n",
              " 'it',\n",
              " 'and',\n",
              " 'and',\n",
              " 'displaying',\n",
              " 'it',\n",
              " '.',\n",
              " 'Hmm',\n",
              " '?',\n",
              " 'Well',\n",
              " 'some',\n",
              " 'parts',\n",
              " 'relevant',\n",
              " 'for',\n",
              " 'the',\n",
              " 'search',\n",
              " ',',\n",
              " 'yes',\n",
              " '.',\n",
              " 'I',\n",
              " \"'d\",\n",
              " 'say',\n",
              " 'so',\n",
              " '.',\n",
              " 'Hmm',\n",
              " '?',\n",
              " 'Yeah',\n",
              " ',',\n",
              " 'but',\n",
              " 'nobody',\n",
              " 'of',\n",
              " 'us',\n",
              " 'is',\n",
              " 'doing',\n",
              " 'much',\n",
              " 'of',\n",
              " 'searching',\n",
              " 'from',\n",
              " 'the',\n",
              " 'data',\n",
              " 'in',\n",
              " 'the',\n",
              " 'on',\n",
              " '-',\n",
              " 'line',\n",
              " 'stage',\n",
              " '.',\n",
              " 'And',\n",
              " 'for',\n",
              " 'all',\n",
              " 'together',\n",
              " ',',\n",
              " 'like',\n",
              " 'the',\n",
              " 'display',\n",
              " 'itself',\n",
              " ',',\n",
              " 'I',\n",
              " 'think',\n",
              " 'we',\n",
              " 'are',\n",
              " 'easier',\n",
              " 'if',\n",
              " 'we',\n",
              " 'if',\n",
              " 'it',\n",
              " \"'s\",\n",
              " 'sitting',\n",
              " 'on',\n",
              " 'the',\n",
              " 'X_M_L',\n",
              " '_',\n",
              " 'than',\n",
              " 'if',\n",
              " 'it',\n",
              " \"'s\",\n",
              " 'sitting',\n",
              " 'on',\n",
              " 'the',\n",
              " 'S_Q_L',\n",
              " '_',\n",
              " 'stuff',\n",
              " ',',\n",
              " 'because',\n",
              " 'if',\n",
              " 'it',\n",
              " \"'s\",\n",
              " 'sitting',\n",
              " 'on',\n",
              " 'the',\n",
              " 'X_M_L',\n",
              " '_',\n",
              " ',',\n",
              " 'we',\n",
              " 'have',\n",
              " 'the',\n",
              " 'the',\n",
              " 'NITE',\n",
              " 'X_M_L',\n",
              " '_',\n",
              " 'framework',\n",
              " 'with',\n",
              " 'all',\n",
              " 'its',\n",
              " 'functionality',\n",
              " 'for',\n",
              " 'synchronizing',\n",
              " 'through',\n",
              " 'all',\n",
              " 'the',\n",
              " 'different',\n",
              " 'levels',\n",
              " 'whenever',\n",
              " 'there',\n",
              " \"'s\",\n",
              " 'a',\n",
              " 'change',\n",
              " ',',\n",
              " 'whenever',\n",
              " 'something',\n",
              " \"'s\",\n",
              " 'moving',\n",
              " 'forward',\n",
              " 'and',\n",
              " 'stuff',\n",
              " '.',\n",
              " 'And',\n",
              " 'we',\n",
              " 'can',\n",
              " 'just',\n",
              " 'more',\n",
              " 'or',\n",
              " 'less',\n",
              " 'look',\n",
              " 'at',\n",
              " 'their',\n",
              " 'code',\n",
              " ',',\n",
              " 'like',\n",
              " 'how',\n",
              " 'their',\n",
              " 'player',\n",
              " 'moves',\n",
              " 'forward',\n",
              " ',',\n",
              " 'and',\n",
              " 'how',\n",
              " 'that',\n",
              " 'moving',\n",
              " 'forward',\n",
              " 'is',\n",
              " 'represented',\n",
              " 'in',\n",
              " 'different',\n",
              " 'windows',\n",
              " 'and',\n",
              " 'stuff',\n",
              " '.',\n",
              " 'So',\n",
              " 'I',\n",
              " 'think',\n",
              " 'in',\n",
              " 'the',\n",
              " 'actual',\n",
              " 'browser',\n",
              " 'itself',\n",
              " 'I',\n",
              " 'do',\n",
              " \"n't\",\n",
              " 'wanna',\n",
              " 'sit',\n",
              " 'on',\n",
              " 'the',\n",
              " 'S_Q_L',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m45VrwneanVm"
      },
      "source": [
        "word_freq = {}\n",
        "for word in doc:\n",
        "    if word.text.lower() not in stopwords_english:\n",
        "        if word.text.lower() not in punctuation:\n",
        "            if word.text not in word_freq.keys():\n",
        "                word_freq[word.text] = 1\n",
        "            else:\n",
        "                word_freq[word.text] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RUwfDzLanVm",
        "outputId": "bb6d7c41-09f2-4b46-ea7c-fb0369824a7b"
      },
      "source": [
        "print(word_freq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'much': 26, 'said': 18, 'yesterday': 1, \"'d\": 43, 'say': 38, 'prototype': 6, 'wherever': 1, 'possible': 4, 'p': 7, 'chunk': 4, 'stuff': 45, 'pre': 4, 'annotated': 4, \"n't\": 149, 'write': 11, 'stupid': 1, 'baseline': 3, 'probably': 76, 'able': 10, 'basically': 21, 'means': 11, 'focus': 2, 'interface': 9, 'first': 12, 'sort': 89, 'take': 17, 'ready': 7, 'made': 5, 'parts': 2, 'see': 23, 'get': 36, 'work': 20, 'together': 24, 'way': 17, 'want': 41, 'working': 14, 'go': 30, 'back': 4, 'replace': 2, 'pieces': 4, 'either': 2, 'components': 4, 'sophisticated': 2, 'compo': 1, 'po': 1, \"'s\": 320, 'feasible': 1, 'thing': 50, \"'m\": 58, 'away': 3, 'weekend': 1, 'Oh': 29, 'also': 25, 'might': 18, 'similarity': 1, 'matrix': 2, 'c': 9, 'think': 110, 'fairly': 3, 'quickly': 1, 'algorithms': 2, 'today': 5, 'meeting': 66, 'really': 38, 'one': 73, 'settle': 1, 'data': 47, 'structure': 12, 'soon': 5, 'actually': 27, 'need': 40, 'look': 29, 'NITE': 22, 'X_M_L': 48, 'far': 9, 'compatible': 1, 'offers': 1, 'us': 13, 'everyone': 9, 'make': 31, 'sure': 21, 'understand': 8, 'decide': 2, 'wanna': 8, 'later': 2, 'maybe': 32, 'even': 23, 'bit': 8, 'detail': 3, 'trying': 1, 'sense': 15, 'code': 13, 'representation': 6, 'system': 9, 'knowledge': 1, 'type': 9, 'load': 19, 'access': 4, \"'ve\": 24, 'looked': 8, 'documentation': 1, 'n': 3, 'seen': 3, 'enough': 7, 'use': 20, 'framework': 9, 'good': 12, 'event': 3, 'model': 2, 'synchronizes': 1, 'every': 19, 'display': 29, 'element': 2, 'takes': 4, 'lot': 17, 'Sort': 2, 'would': 94, 'reason': 2, 'staying': 1, 'within': 16, 'using': 13, 'general': 8, 'classes': 4, 'beyond': 1, 'something': 53, 'whole': 68, 'discussion': 1, 'mean': 79, 'happening': 2, 'line': 30, 'coming': 4, 'apart': 2, 'module': 4, 'extremely': 1, 'high': 5, 'degree': 2, 'interaction': 1, 'modules': 2, 'create': 7, 'mainly': 1, 'running': 7, 'loaded': 3, 'file': 43, 'guess': 16, 'know': 57, 'Th': 1, 'search': 13, 'strange': 5, 'beast': 1, 'anyway': 16, \"'re\": 59, 'leaving': 1, 'still': 16, 'least': 5, 'situation': 1, 'somebody': 8, 'billion': 1, 'calculations': 2, \"'Cause\": 20, 'careful': 5, 'performs': 1, 'everything': 17, 'nobody': 5, 'modifying': 1, 'time': 43, 'seems': 4, 'Nobody': 1, 'making': 5, 'changes': 6, 'actual': 5, 'easier': 7, 'browser': 8, 'viewer': 1, 'mostly': 3, 'except': 1, 'selecting': 1, 'piece': 5, 'displaying': 5, 'relevant': 2, 'yes': 3, 'searching': 4, 'stage': 1, 'sitting': 5, 'S_Q_L': 3, 'functionality': 2, 'synchronizing': 1, 'different': 41, 'levels': 11, 'whenever': 2, 'change': 8, 'moving': 2, 'forward': 5, 'less': 10, 'player': 7, 'moves': 2, 'represented': 1, 'windows': 1, 'sit': 5, 'help': 1, 'needs': 7, 'special': 3, 'representations': 1, 'results': 4, 'format': 4, 'stamps': 10, 'easy': 20, 'tie': 6, 'st': 2, 'things': 13, 'multi': 1, 'level': 41, 'idea': 8, 'start': 21, 'series': 41, 'entity': 6, 'individual': 24, 'chunks': 12, 'meetings': 20, 'whereas': 2, 'click': 4, 'segments': 35, 'multiple': 2, 'f': 14, 'discuss': 3, 'find': 7, 'abstraction': 1, 'deals': 1, 'worry': 3, 'whether': 12, 'segment': 20, 'process': 9, 'twice': 1, 'example': 11, 'summary': 9, 'corpus': 18, 'word': 35, 'call': 2, 'topic': 34, 'se': 3, 'compiled': 1, 'separate': 8, 'fine': 3, 'grainedness': 1, 'happen': 1, 'double': 2, 'let': 9, 'single': 12, 'zoom': 4, 'th': 15, 'end': 23, 'position': 6, 'thought': 18, 'could': 36, 'impression': 2, 'got': 18, 'wa': 2, 'worried': 2, 'total': 3, 'memory': 18, 'complexity': 1, 'completely': 5, 'admit': 1, 'took': 1, 'Jonathan': 3, 'loading': 10, 'wrong': 3, 'many': 8, 'utterances': 48, 'w': 16, 'words': 54, 'priority': 3, 'selection': 1, 'summaries': 5, 'automatically': 3, 'feed': 4, 'prioritized': 2, 'indiv': 1, 'utterance': 37, 'refined': 1, 'machine': 11, 'sentences': 2, 'taking': 2, 'highest': 8, 'second': 4, 'u': 1, 'information': 27, 'density': 16, 'calculation': 3, 'times': 10, 'sound': 2, 'crazy': 1, 'point': 7, 'check': 6, 'works': 6, 'merit': 3, 'altogether': 2, 'displays': 4, 'text': 29, 'body': 1, 'latest': 1, 'draft': 1, 'came': 1, 'summarised': 2, 'version': 1, 'graph': 1, 'part': 10, 'Maybe': 5, 'r': 4, 'audio': 11, 'skimming': 12, 'displayed': 4, 'going': 9, 'bother': 3, 'calculate': 7, 'internally': 4, 'store': 12, 'importance': 8, 'rank': 1, 'better': 4, 'required': 1, 'worst': 1, 'case': 2, 'ca': 11, 'anything': 17, 'else': 8, 'W': 4, 'smallest': 1, 'moment': 19, 'thinking': 13, 'assigning': 1, 'measure': 7, 'storing': 1, 'per': 7, 'Dave': 4, 'playing': 1, 'show': 1, 'important': 4, 'makes': 5, 'difference': 5, 'algorithm': 15, \"'cause\": 16, 'put': 14, 'Jasmine': 3, 'ways': 2, 'easily': 4, 'prob': 1, 'filtering': 2, 'problem': 9, 'play': 5, 'unless': 1, 'cut': 6, 'answer': 1, 'specific': 2, 'question': 1, 'deal': 3, 'capable': 1, 'Yes': 22, 'buffering': 1, 'directly': 4, 'feeding': 1, 'stored': 4, 'hard': 6, 'disk': 4, 'stream': 2, 'exists': 2, 'Java': 11, 'binary': 1, 'cutting': 2, 'clearly': 2, 'au': 1, 'phrase': 2, 'addition': 1, 'says': 5, 'minus': 1, 'depends': 4, 'advanced': 1, 'realise': 2, 'massive': 9, 'differences': 1, 'gain': 2, 'simple': 4, 'normalization': 1, 'necessary': 2, 'never': 3, 'accepts': 1, 'input': 1, 'doable': 2, 'quite': 33, 'lucky': 1, 'rankings': 1, 'goes': 5, 'speaking': 3, 'confused': 2, 'document': 24, 'size': 3, 'hand': 4, 'fifty': 3, 'mega': 4, 'byte': 3, 'RAM': 3, 'Actually': 2, 'hundred': 7, 'megabyte': 4, 'big': 27, 'simp': 1, 'error': 1, 'message': 1, 'project': 8, 'hope': 2, 'essentially': 3, 'lazy': 3, 'explain': 1, 'Ah': 6, 'files': 21, 'split': 4, 'three': 7, 'ten': 3, 'chance': 1, 'try': 7, 'fail': 2, 'place': 3, 'ever': 2, 'failed': 1, 'right': 25, 'kit': 1, 'interesting': 2, 'Let': 1, \"'ll\": 38, 'ask': 4, 'alternatively': 2, 'meta': 1, 'representing': 2, 'represents': 1, 'v': 2, 'similar': 7, 'represent': 3, 'always': 8, 'combined': 2, 'instead': 4, 'wi': 1, 'creating': 3, 'virtual': 3, 'treats': 3, 'shift': 1, 'vir': 1, 'two': 20, 'ifs': 2, 'new': 8, 'alternative': 1, 'worrying': 2, 'users': 1, 'view': 1, 'often': 2, 'based': 2, 'granularity': 1, 'seventy': 1, 'hours': 3, 'structurally': 2, 'le': 1, 'score': 2, 'cou': 1, 'course': 1, 'speaker': 5, 'topics': 14, 'sorts': 1, 'puts': 1, 'order': 11, 'set': 5, 'otherwise': 7, 'gon': 35, 'na': 35, 'entertaining': 1, 'leaves': 5, 'database': 6, 'gets': 8, 'marker': 2, 'concerned': 2, 'mark': 1, 'shifts': 1, 'frame': 2, 'alerts': 5, 'updates': 1, 'ju': 2, 'somethi': 1, 'tree': 4, 'clicks': 1, 'stamp': 5, 'central': 3, 'manager': 2, 'skim': 1, 'visual': 1, 'update': 1, 'routines': 1, 'respect': 1, 'current': 2, 'starting': 3, 'mid': 2, 'found': 7, 'middle': 1, 'jump': 1, 'window': 2, 'handling': 4, 'triggered': 1, 'routine': 1, 'push': 1, 'please': 1, 'beauty': 2, 'ties': 2, 'lots': 1, 'additional': 1, 'I_D_s': 3, 'presume': 2, 'references': 2, 'short': 3, 'whatever': 17, 'number': 10, 'weight': 6, 'existing': 2, 'add': 3, 'integer': 1, 'increment': 1, 'top': 3, 'bottom': 2, 'I_D': 5, 'un': 1, 'route': 1, 'follow': 1, 'alm': 1, 'girl': 1, 'numbered': 3, 'solvable': 1, 'Sorry': 6, 'board': 1, 'pen': 1, 'list': 11, 'paper': 1, 'fancy': 2, 'pens': 1, 'speakers': 2, 'weights': 3, 'outside': 1, 'sorry': 9, 'meant': 5, 'super': 4, 'unit': 1, 'tied': 2, 'b': 11, 'somehow': 1, 'extract': 5, 'generally': 3, 'called': 3, 'sa': 2, 'person': 4, 'contribution': 1, 'thingy': 1, 'dingy': 1, 'field': 1, 'somewhere': 3, 'referenced': 1, 'contain': 1, 'redundant': 1, 'showing': 1, 'finish': 1, 'belong': 2, 'segmentation': 7, 'definitely': 5, 'fit': 1, 'long': 12, 'slash': 2, 'smaller': 1, 'lists': 2, 'give': 12, 'though': 13, 'head': 1, 'merge': 3, 'bureaucracy': 1, 'involved': 1, 'trees': 3, 'quicker': 3, 'tre': 1, 'overhead': 1, 'amount': 3, 'already': 12, 'figure': 2, 'horrendously': 1, 'infrastructure': 2, 'main': 1, 'types': 3, 'queries': 3, 'l': 2, 'dynamically': 1, 'select': 2, 'fast': 1, 'ones': 7, 'query': 4, 'language': 2, 'shou': 1, 'return': 1, 'million': 3, 'handle': 3, 'threshold': 3, 'oh': 6, 'ab': 1, 'skip': 3, 'leave': 2, 'meet': 3, 'calculated': 5, 'disp': 1, 'measures': 2, 'along': 1, 'extracting': 4, 'title': 2, 'craft': 1, 'manually': 1, 'highly': 3, 'valued': 1, 'key': 3, 'heading': 1, 'best': 3, 'ranked': 1, 'introduction': 2, 'anywhere': 1, 'Also': 1, 'named': 7, 'people': 5, 'DIL': 4, 'spare': 1, 'finding': 2, 'titles': 1, 'D_F_I_D_F': 1, 'likely': 1, 'talking': 6, 'conference': 3, 'fr': 2, 'frequented': 1, 'sparse': 2, 'basing': 1, 'especially': 1, 'entities': 2, 'describe': 2, 'name': 4, 'indirect': 1, 'Anyway': 2, 'cool': 5, 'wondering': 3, 'discussions': 1, 'exactly': 3, 'interact': 1, 'done': 5, 'ha': 3, 'useful': 12, 'understanding': 1, 'session': 4, 'computer': 3, 'room': 3, 'closer': 1, 'Good': 1, 'coordination': 1, 'application': 1, 'elements': 1, 'integration': 4, 'Nah': 1, 'started': 1, 'understands': 1, 'centrally': 1, 'comes': 4, 'volunteers': 1, 'complicated': 2, 'several': 4, 'closely': 1, 'versions': 2, 'matter': 7, 'building': 4, 'test': 2, 'difficult': 4, 'anyone': 9, 'nice': 3, 'basic': 2, 'adapt': 3, 'arran': 1, 'gym': 1, 'theoretically': 1, 'free': 5, 'nothing': 4, 'Wednesday': 5, 'Nine': 1, 'til': 2, 'twelve': 4, 'nothi': 1, 'Anytime': 1, 'afternoon': 5, 'Yo': 1, 'Forrest': 1, 'Hill': 1, 'biased': 1, 'eighteen': 1, 'critically': 1, 'L_S_A': 3, 'vast': 1, 'without': 5, 'supplying': 1, 'crucial': 2, 'raw': 14, 'programme': 2, 'flavours': 1, 'context': 6, 'spoken': 1, 'numbers': 12, 'task': 3, 'screw': 1, 'pro': 2, 'perform': 1, 'another': 8, 'seeing': 1, 'constrained': 1, 'vocabulary': 1, 'co': 1, 'occurrence': 2, 'nine': 9, 'wou': 2, 'sounded': 1, 'wanted': 4, 'overlapping': 1, 'reading': 1, 'speeds': 1, 'ICSI': 5, 'reasons': 1, 'must': 2, 'pissed': 1, 'saying': 5, 'I_D_F_s': 1, 'frequencies': 10, 'mix': 1, 'dictionary': 19, 'token': 1, 'given': 2, 'form': 4, 'counts': 7, 'forms': 2, 'ord': 2, 'wo': 5, 'spits': 1, 'frequency': 17, 'tool': 4, 'experience': 1, 'British': 1, 'National': 1, 'Corpus': 1, 'unusual': 1, 'typo': 1, 'thousands': 1, 'expect': 2, 'dictionaries': 2, 'grow': 1, 'bigger': 1, 'filter': 4, 'regular': 1, 'expressions': 1, 'consists': 2, 'dig': 1, 'digits': 2, 'characters': 3, 'dot': 2, 'usually': 3, 'ignored': 1, 'frequent': 2, 'articles': 1, 'Frequencies': 1, 'hash': 11, 'map': 3, 'beefy': 3, 'burning': 1, 'burn': 6, 'X': 1, 'asked': 2, 'support': 3, 'days': 1, 'ago': 1, 'Informatics': 1, 'Appleton': 3, 'Tower': 3, 'five': 4, 'closest': 2, 'machines': 2, 'office': 1, 'wait': 4, 'exact': 2, 'email': 2, 'enter': 1, 'corner': 2, 'local': 4, 'mounted': 1, 'temp': 5, 'directory': 10, 'forgot': 1, 'gigabyte': 2, 'See': 2, 'offer': 1, 'copy': 4, 'figured': 2, 'broad': 1, 'band': 2, 'mount': 1, 'unfortunate': 1, 'operating': 1, 'Wh': 1, 'connection': 1, 'home': 6, 'ext': 1, 'C_D': 5, 'Question': 1, 'compression': 1, 'percent': 1, 'compress': 3, 'temps': 3, 'guarantee': 1, 'stays': 1, 'overnight': 1, 'stay': 2, 'S_S_H': 4, 'hate': 2, 'gateway': 5, 'S_S': 1, 'hey': 1, 'warning': 1, 'tunnel': 1, 'yet': 4, 'copying': 1, 'boring': 3, 'everybody': 2, 'details': 1, 'gives': 3, 'implement': 3, 'table': 9, 'builder': 1, 'count': 3, 'old': 1, 'ar': 1, 'g': 2, 'getting': 6, 'creates': 1, 'blank': 1, 'common': 1, 'latent': 1, 'semantic': 1, 'analysis': 1, 'convert': 1, 'probabilities': 2, 'Rainbow': 5, 'builds': 2, 'Even': 2, 'build': 12, 'appears': 1, 'scrap': 1, 'notion': 2, 'hierarchical': 4, 'coherent': 1, 'Wait': 2, 'weighting': 1, 'calculating': 3, 'lowest': 1, 'versus': 1, 'talk': 3, 'relative': 1, 'term': 1, 'certain': 3, 'abandon': 1, 'concept': 2, 'treating': 2, 'algorithmic': 1, 'plus': 2, 'keeping': 2, 'happens': 1, 'sub': 2, 'treated': 1, 'brain': 1, 'damaged': 1, 'identical': 1, 'treat': 2, 'Probably': 1, 'decided': 2, 'looking': 10, 'implementation': 1, 'modify': 1, 'read': 5, 'space': 5, 'scratch': 1, 'gigabytes': 2, 'sends': 2, 'port': 1, 'seminar': 1, 'discussing': 1, 'technical': 1, 'practicalities': 1, 'al': 1, 'allowed': 3, 'anybody': 3, 'DICE': 4, 'laptop': 1, 'personally': 1, 'friends': 1, 'keen': 1, 'excited': 1, 'pirate': 1, 'copied': 1, 'Huh': 3, 'realised': 1, 'keep': 1, 'serieses': 1, 'virtually': 2, 'purposes': 1, 'merging': 1, '..': 3, 'pick': 1, 'majorly': 1, 'messy': 1, 'Wou': 1, 'producing': 1, 'real': 1, 'tha': 1, 'last': 3, 'print': 2, 'straight': 2, 'flowing': 1, 'marks': 2, 'segmented': 1, 'automated': 1, 'output': 2, 'dump': 4, 'pure': 2, 'loads': 7, 'enumerate': 2, 'four': 1, 'enu': 1, 'ordered': 7, 'pattern': 1, 'wish': 1, 'Orders': 1, 'primitive': 2, 'sounds': 5, 'reasonable': 3, 'picking': 1, 'particular': 4, 'presentation': 1, 'roughly': 1, 'realisti': 1, 'sorted': 1, 'alphabetically': 1, 'numerically': 1, 'wishes': 1, 'Add': 1, 'next': 10, 'morning': 1, 'processing': 1, 'bog': 1, 'standard': 1, 'written': 1, 'half': 5, 'hour': 2, 'increase': 1, 'Really': 3, 'Perl': 7, 'wants': 1, 'script': 3, 'nicely': 1, 'wrote': 2, 'occurrences': 2, 'tags': 1, 'serialized': 1, 'Would': 1, 'absolutely': 1, 'seriali': 1, 'serialization': 2, 'Give': 1, 'break': 1, 'separated': 1, 'serialize': 3, 'serialise': 1, 'command': 1, 'mother': 1, 'pretty': 5, 'demonstrator': 2, 'week': 8, 'God': 1, 'demonstrate': 3, 'agree': 1, 'ta': 1, 'feel': 1, 'hanging': 1, 'air': 1, 'teeth': 1, 'properly': 1, 'fuzzy': 1, 'implementational': 1, 'structures': 1, 'vague': 1, 'ideas': 2, 'freaks': 1, 'user': 3, 'face': 1, 'random': 1, 'depending': 2, 'value': 2, 'sequences': 1, 'sh': 1, 'possibly': 2, 'annotation': 3, 'lost': 1, 'abo': 1, 'combining': 1, 'hot': 3, 'spots': 2, 'Oops': 1, 'define': 1, 'discourse': 2, 'acts': 3, 'demand': 2, 'N': 1, 'ex': 1, 'compared': 1, 'pause': 2, 'duration': 1, 'extraction': 2, 'uttered': 1, 'sequence': 2, 'pauses': 2, 'sometimes': 3, 'however': 1, 'indicated': 1, 'square': 1, 'brackets': 1, 'someti': 1, 'annotators': 1, 'annotations': 1, 'uttera': 1, 'dialogue': 1, 'sti': 1, 'occurs': 2, 'wondered': 1, 'agreement': 1, 'disagreement': 1, 'outline': 1, 'talked': 1, 'patterns': 1, 'freq': 1, 'regions': 1, 'spot': 1, 'small': 6, 'concatenate': 4, 'run': 2, 'Ye': 5, 'ch': 1, 'perhaps': 1, 'lines': 3, 'codes': 1, 'millisecond': 2, 'changing': 2, 'extracted': 2, 'dura': 1, 'durations': 2, 'contatenate': 1, 'send': 3, 'Pe': 1, 'according': 1, 'diffe': 1, 'group': 3, 'identification': 1, 'letter': 1, 'B_E_D': 1, 'B_B_D': 1, 'O_O': 2, 'zero': 2, 'final': 1, 'channel': 1, 'string': 2, 'manipulation': 1, 'Sure': 1, 'sea': 1, 'huh': 2, 'names': 1, 'letters': 2, 'base': 1, 'contained': 1, 'Ordered': 1, 'megami': 1, 'Na': 1, 'guessing': 1, 'seconds': 1, 'typed': 1, 'assign': 1, 'carry': 1, 'meaning': 1, 'uhs': 1, 'average': 2, 'unimportant': 1, 'disregard': 1, 'currently': 1, 'implementing': 2, 'computing': 1, 'bins': 1, 'notes': 1, 'combine': 2, 'accordingly': 1, 'beginning': 5, 'kind': 2, 'F': 1, 'Think': 2, 'tag': 1, 'saw': 1, 'combinations': 1, 'B_R_E': 1, 'global': 1, 'bin': 2, 'classification': 2, 'class': 3, 'terms': 4, 'implemented': 1, 'j': 2, 'Certain': 1, 'informative': 1, 'shall': 1, 'tomorrow': 1, 'Print': 1, 'program': 1, 'counting': 1, 'hashes': 1, 'dry': 1, 'Next': 1, 'mine': 2, 'processor': 1, 'intensive': 2, 'dumping': 1, 'reckon': 2, 'fields': 1, 'underlying': 2, 'summarise': 1, 'sorting': 1, 'computationally': 1, 'comprises': 1, 'stuck': 1, 'weighted': 2, 'slow': 1, 'honest': 1, 'trouble': 1, 'synch': 1, 'impossible': 1, 'buffer': 2, 'I_D_F': 2, 'meaningful': 2, 'sentence': 1, 'nought': 1, 'assigned': 1, 'wave': 4, 'bet': 1, 'packages': 2, 'unp': 1, 'bound': 1, 'media': 1, 'object': 1, 'dealing': 1, 'concatenating': 1, 'linked': 1, 'tenth': 1, 'silence': 1, 'Normalise': 1, 'summarisation': 1, 'scores': 1, 'higher': 2, 'Speaker': 1, 'segmenting': 2, 'preserve': 1, 'rather': 2, 'close': 1, 'may': 4, 'arbitrary': 4, 'attributes': 1, 'parser': 3, 'parse': 1, 'faster': 1, 'come': 3, 'informan': 1, 'mation': 1, 'recalculating': 1, 'obviously': 2, 'T_F_I_D_F': 5, 'inverse': 1, 'averaging': 1, 'related': 1, 'scroll': 1, 'Friday': 2, 'Matlab': 1, 'documents': 2, 'bef': 1, 'classified': 1, 'fill': 1, 'junk': 1, 'care': 1, 'copies': 1, 'Shakespeare': 1, 'statistics': 1, 'changed': 1, 'available': 1, 'remember': 1, 'Might': 2, 'WordNet': 1, 'corpuses': 1, 'stop': 2, 'download': 1, 'uninteresting': 1, 'papers': 1, 'ignore': 1, 'throughout': 1, 'experiment': 1, 'awful': 1, 'burner': 2, 'WAV': 1, 'S_C_P': 3, 'night': 2, 'suppose': 2, 'Linux': 1, 'box': 2, 'Windows': 1, 'Broad': 1, 'Put': 1, 'remote': 1, 'SSHing': 1, 'SCPing': 1, 'used': 1, 'little': 2, 'consider': 3, 'occur': 1, 'depend': 1, 'viewing': 1, 'allow': 1, 'disjoint': 1, 'globally': 1, 'comparing': 1, 'across': 2, 'issue': 1, 'jam': 1, 'bits': 1, 'conceptually': 1, 'parents': 1, 'inwards': 1, 'towards': 1, 'branch': 1, 'designed': 1, 'mind': 1, 'hierarchy': 1, 'looks': 1, 'went': 1, 'Right': 5, 'layer': 1, 'respects': 2, 'imposing': 1, 'nodes': 1, 'deeper': 1, 'confusing': 1, 'mini': 1, 'synched': 1, 'account': 1, 'deleted': 1, 'guaranteed': 1, 'web': 1, 'Listen': 1, 'Especially': 1, 'transferring': 1, 'Something': 1, 'neither': 1, 'funny': 1, 'finished': 1, 'writing': 2, 'saves': 1, 'parsing': 2, 'silly': 1, 'kinda': 2, 'collections': 1, 'serializable': 1, 'Tonight': 1, 'summarizer': 1, 'specify': 1, 'specification': 2, 'become': 1, 'obvious': 1, 'Steve': 2, 'feedback': 1, 'duplication': 2, 'effort': 2, 'six': 1, 'prioritize': 2, 'online': 1, 'Depends': 1, 'accessed': 1, 'develop': 1, 'Pretty': 1, 'Topics': 1, 'efficient': 1, 'inside': 1, 'occurred': 1, 'L_C': 1, 'seg': 2, 'equivalent': 1, 'allocated': 1, 'built': 1, 'integrating': 1, 'Python': 2, 'X_L': 1, 'noticed': 1, 'approach': 1, 'collaborate': 1, 'gotten': 1, 'hardly': 1, 'year': 1, 'mo': 1, 'M_L_C': 1, 'hopefully': 1, 'Suppose': 1, 'Wa': 1, 'suggested': 1, 'initial': 1, 'surprised': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io23c0hganVm"
      },
      "source": [
        "max_freq = max(word_freq.values())\n",
        "for word in word_freq.keys():\n",
        "    word_freq[word] = word_freq[word]/max_freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z3UHHZianVm"
      },
      "source": [
        "sentence_tokens = [sent for sent in doc.sents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFAbkqNSanVn",
        "outputId": "f2cf6ec1-b385-41a8-c294-17f6ae57ff28"
      },
      "source": [
        "sentence_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Kay.,\n",
              " Gosh. ',\n",
              " Kay.,\n",
              " Is there much more in it than he d,\n",
              " Is there much more in it than he said yesterday?,\n",
              " Mm.,\n",
              " Hmm.,\n",
              " Hmm?,\n",
              " Yeah, now I'd say if for the prototype if we just like wherever possible p chunk in the stuff that we have um pre-annotated and stuff, and for the stuff that we don't have pre-annotated write like a stupid baseline, then we should probably be able to basically that means we focus on on the interface first sort of, so that we we take the the ready-made parts and just see how we get them work together in the interface the way we want,\n",
              " and,\n",
              " and then we have a working prototype.,\n",
              " And then we can go back and replace pieces either by our own components or by more sophisticated compo po components of our own.,\n",
              " So it's probably feasible.,\n",
              " The thing is I'm away this weekend.,\n",
              " So that's for me,\n",
              " Oh yeah,,\n",
              " um,\n",
              " yeah.,\n",
              " No.,\n",
              " But also I might like the the similarity thing, like my just my matrix itself for my stuff,\n",
              " , I c,\n",
              " I,\n",
              " I think I can do that fairly quickly because I have the algorithms.,\n",
              " Yeah, I think today's meeting is really the one where we where we sort of settle down the data structure and as soon as we have that, uh probably like after today's meeting, we then actually need to well go back first of all and look at NITE X_M_L_ to see in how far that that which we want is compatible with that which NITE X_M_L_ offers us.,\n",
              " And then just sort of everyone make sure everyone understand the interface.,\n",
              " So I think if today we decide on what data we wanna have now,,\n",
              " and and later, maybe even today, we go and look at NITE X_M_L_ or some of us look at NITE X_M_L_ in a bit more detail, just trying to make some sense of that code and see how does the representation work in their system.,\n",
              " And then sort of with that knowledge we should be able to then say okay, that type of NITE X_M_L_ data we wanna load into it, and this is how everyone can access it, and then we should be able to go from there.,\n",
              " No.,\n",
              " I've looked looked at the documentation and n like seen enough to make me think that we want to use the NITE X_M_L_ framework because um they have a good a event model that synchronizes sort of the data,\n",
              " and and every display element.,\n",
              " So that takes a lot of work away from us.,\n",
              " Sort of that would be a reason for staying within their framework and using their general classes.,\n",
              " But beyond that I haven't looked at it at all, which is something we should really do.,\n",
              " Who actually like for this whole discussion I mean, who of us is doing stuff that is happening on-line and who of us is doing stuff that's happening off-line?,\n",
              " Like my data is coming c Hmm?,\n",
              " Yeah.,\n",
              " Okay.,\n",
              " Okay. ',\n",
              " Kay.,\n",
              " So basically apart from the display module, the i the display itself, we don't have an extremely high degree of interaction between sort of our modules that create the stuff,\n",
              " and and the interface, so the interface is mainly while it's running just working on data that's just loaded from a file, I guess.,\n",
              " There isn't Yeah, I know.,\n",
              " Th,\n",
              " Yeah, the search is,\n",
              " I guess the search is sort of a strange beast anyway because for the search we're leaving the NITE X_M_L_ framework.,\n",
              " Um but that's still sort of that's good.,\n",
              " That means that at least like we don't have the type of situation where somebody has to do like a billion calculations on on data on-line.,\n",
              " 'Cause that would make it a lot more like that,\n",
              " would mean that our interface for the data would have to be a lot more careful about how it performs and and everything.,\n",
              " And nobody is modifying that data at at on-line time at all it seems.,\n",
              " Nobody's making any changes to the actual data on-line.,\n",
              " So that's actually making it a lot easier.,\n",
              " That basically means our browser really is a viewer mostly, which isn't doing much with the data except for sort of selecting a piece piece of it and and displaying it.,\n",
              " Hmm?,\n",
              " Well some parts relevant for the search, yes.,\n",
              " I'd say so.,\n",
              " Hmm?,\n",
              " Yeah, but nobody of us is doing much of searching from the data in the on-line stage.,\n",
              " And for all together, like the display itself, I think we are easier if we if it's sitting on the X_M_L_ than if it's sitting on the S_Q_L_ stuff, because if it's sitting on the X_M_L_, we have the the NITE X_M_L_ framework with all its functionality for synchronizing through all the different levels whenever there's a change, whenever something's moving forward and stuff.,\n",
              " And we can just more or less look at their code, like how their player moves forward, and how that moving forward is represented in different windows and stuff.,\n",
              " So I think in the actual browser itself,\n",
              " I don't wanna sit on the S_Q_L_ if we can sit on the X_M_L_ because sitting on the X_M_L_ we have all we have so much help.,\n",
              " And for y for like the p the calculations that we're doing apart from the search, it seems that everyone needs some special representations anyway.,\n",
              " You mean our results?,\n",
              " Yeah, in in the NITE X_M_L_ X_M_L_ format, so with their time stamps and stuff, so that it's easy to to tie together st things.,\n",
              " What I'm like what we have to think about is if we go with this multi-level idea, like this idea that sort of if you start with a whole meeting series as one entity, as one thing that you display, as one whole sort of, that then the individual chunks of the individual meetings, whereas and then you can click on a meeting, and then sort of the meeting is the whole thing and the chunks are the individual segments, that means sort of we have multiple levels of of representation, which we probably If we if we do it this way like we f we have to discuss that if we do it this way, then we should probably find some abstraction model, so that the interface in the sense like deals with it as if it's same so that the interface doesn't really have to worry whether it's a meeting in the whole meeting series or a segment within a meeting,\n",
              " , you know what I mean?,\n",
              " And that's probably stuff that we have to sort of like process twice then.,\n",
              " Like for example that like the summary of a meeting within the whole meeting corpus or meeting series y is meeting series a good word for that?,\n",
              " I don't really know what how to call it.,\n",
              " You know what I mean, like not not the whole corpus, but every meeting that has to do with one topic.,\n",
              " Um so in in the meeting se series so that a summary for a meeting within the meeting series, are sort of compiled off-line by a summary module.,\n",
              " And that is separate from a summary of a segment within a meeting.,\n",
              " 'Cause I don't think we can So are we doing that at all levels?,\n",
              " Are we um And just have different like fine-grainedness levels sort of.,\n",
              " Mm. ',\n",
              " Kay.,\n",
              " So the only thing that,\n",
              " yeah, so,\n",
              " the only thing that would happen basically if I double-click let's say from the whole meeting series on a single meeting, is that the zoom level changes.,\n",
              " Like the th the start and the end position changes and the zoom level changes.,\n",
              " I,\n",
              " I thought we couldn't do that.,\n",
              " Like I was under the impression that we couldn't do that because we couldn't load the data for all that.,\n",
              " But I don't know,\n",
              " , I mean that So I'm s not sure if I got it.,\n",
              " I was Mm-hmm.,\n",
              " Mm-hmm.,\n",
              " Mm-hmm.,\n",
              " Mm-hmm.,\n",
              " Okay.,\n",
              " So,\n",
              " Okay.,\n",
              " I wa,\n",
              " I was just worried about the total memory complexity of it.,\n",
              " But I I completely admit, I mean,,\n",
              " I just sort of like th took that from some thing that Jonathan once said about not loading everything.,\n",
              " But maybe I was just wrong about it.,\n",
              " How many utterances,\n",
              " w,\n",
              " Yeah,,\n",
              " and I w,\n",
              " yeah.,\n",
              " Yeah.,\n",
              " Yeah.,\n",
              " Yeah.,\n",
              " So what we have is we would have a word.,\n",
              " Like we would have words with some priority levels.,\n",
              " And they would basically be because even the selection would would the summaries automatically feed from just how prioritized an individual word or how indiv uh prioritized an individual utterance is?,\n",
              " Or i are the summaries sort of refined from it and made by a machine to make sentences and stuff?,\n",
              " Or are they just sort of taking out the words with the highest priority and,\n",
              " then the words of the second highest priority?,\n",
              " And the u okay.,\n",
              " Are we doing it on th the whole thing on the utterance level?,\n",
              " Or are we doing it on word level, like the information density calculation?,\n",
              " We,\n",
              " I think we have start and end times for words actually, but it's yeah,,\n",
              " but it m,\n",
              " it might,\n",
              " s,\n",
              " but it might sound crazy in the player.,\n",
              " We should really maybe we can do that together at some point today that we check out how the player works.,\n",
              " But there's maybe some merit in altogether doing it on an utterance level in the end.,\n",
              " So Yeah.,\n",
              " Well but also about the displays, I mean the displays in the in the text body, in the in the latest draft that we had sort of we came up with the idea that it isn't displaying utterance for utterance, but it's also displaying uh a summarised version in you know, like below the below the graph, the part.,\n",
              " Maybe Yeah, r Hmm?,\n",
              " Oh yeah,,\n",
              " f,\n",
              " it's just like,\n",
              " there there's like audio skimming,\n",
              " and there's displayed skimming.,\n",
              " Yeah.,\n",
              " Ma,\n",
              " maybe there's some merit of going altogether for utterance level and not even bother to calculate,\n",
              " I mean if you have to do it internally, then you can do it.,\n",
              " But maybe like not even store the importance levels for individual words and just sort of rank utterances as a whole.,\n",
              " Hmm?,\n",
              " Yeah.,\n",
              " 'Cause it,\n",
              " it might be better skimming and less memory required at the same time.,\n",
              " And I mean if you if you know how to do it for individual words, then you can just in the worst case, if you can't find anything else, just sort of make the mean of the words over the utterance.,\n",
              " You know what I mean?,\n",
              " W,\n",
              " it's,\n",
              " it's,\n",
              " Well what's the smallest chunk at the moment you're thinking of of assigning an importance measure to, is it a word or is it an utterance?,\n",
              " So we're thinking of like maybe just storing it on a per utterance level.,\n",
              " Because it's it's less stuff to store probably for Dave in the in the audio playing.,\n",
              " And for in the display it's probably better if you have whole utterances than I don't know, like what it's like,\n",
              " if you just take single words out of utterances.,\n",
              " That probably doesn't make any sense at all, whereas if you just uh show important utterances but the utterance as a whole it makes more sense.,\n",
              " So it doesn't actually make a difference for your algorithm, 'cause it just means that if you're working on a word level, then we just mean it over the utterance.,\n",
              " They are on,\n",
              " Oh so that's good anyway then,,\n",
              " yeah.,\n",
              " Because that makes it a lot easier than to t put it on utterance level.,\n",
              " Oh yeah.,\n",
              " No,\n",
              " but I mean like how how Jasmine does it internally,\n",
              " I don't know,,\n",
              " but it's probably,,\n",
              " yeah, you probably have to work on word levels for importance.,\n",
              " But there should be ways of easily going from a word level to an utterance level.,\n",
              " Okay.,\n",
              " Yeah, prob Hmm.,\n",
              " Well we do a pre-filtering of sort of the whole thing, sort of like,\n",
              " but that, like the problem with that is,\n",
              " it's easy to do in the text level.,\n",
              " But that would mean it would still play the uh in your audio, unless we sort of also store what pieces we cut out for the audio.,\n",
              " Yeah.,\n",
              " I think before we can like answer that specific question how we c deal with that, it's probably good for us to look at what the audio player is capable of doing.,\n",
              " Yes.,\n",
              " So what do you mean by buffering?,\n",
              " Like you think directly feeding,\n",
              " But yeah,,\n",
              " but not but not stored on the hard disk and then loaded in, but loaded in directly from memory.,\n",
              " But it's probably a stream if it exists in Java, it would be probably some binary stream going in of some type.,\n",
              " Okay,,\n",
              " yeah.,\n",
              " Okay.,\n",
              " Okay, so I mean so that means that there's probably,,\n",
              " even if you go on an per utterance level, there's still some merit on within utterances cutting out stuff which clearly isn't relevant at all, and that maybe also for the audio we'd have to do.,\n",
              " So let's say we play the whole au phrase, but then in addition to that, we have some information that says minus that part of something.,\n",
              " That's okay, that we can do.,\n",
              " Yeah, maybe even I mean that's sort of that depends on how how advanced we get.,\n",
              " If maybe if we realise that there's massive differences in in gain or in something, you can probably just make some simple simple normalization, but that really depends on how much time we have and and how much is necessary.,\n",
              " Yeah, if like I d I don't know anything about audio,\n",
              " and I have never seen the player.,\n",
              " So if you find that the player accepts some n input from memory, and if it's easy to do, then I guess that's,\n",
              " that's fairly doable.,\n",
              " So but that means in the general structure we're actually quite lucky, so we we have we load into memory for the whole series of meetings just the utterances and rankings for the utterances and some information probably that says, well, the I guess that goes with the utterance, who's speaking.,\n",
              " Because then we can also do the display about who's speaking.,\n",
              " Yeah.,\n",
              " But I'm I'm still confused 'cause I thought like that's just what Jonathan said we do c that we can't do, like load a massive document of that size.,\n",
              " On the other hand The other hand, I mean it shouldn't be like should be like fifty mega-byte in RAM or something,\n",
              " , it shouldn't be massive, should it?,\n",
              " Actually fifty hundred megabyte is quite big in RAM.,\n",
              " Just thinking, what's the simp,\n",
              " so We do get an error message with the project if we load everything into the project with all the data they load.,\n",
              " So we know that doesn't work.,\n",
              " So our hope is essentially that we load less into it.,\n",
              " What's this lazy loading thing, somebody explain lazy loading to me.,\n",
              " Ah, okay.,\n",
              " So that is that only by type of file.,\n",
              " Like if if if the same thing is in different files, would it then maybe like, you know, if if utterances are split over three or ten or w hundred different files, is then a chance maybe that it doesn't try to load them all into memory at the same time, but just So why does it fail then in the first place?,\n",
              " Then it shouldn't ever fail,,\n",
              " because then it should never Yeah,,\n",
              " but yeah,,\n",
              " but um it uh it it failed right when you load it,,\n",
              " right, the NITE X_M_L_ kit, so that's interesting.,\n",
              " Hmm.,\n",
              " Let's check that out.,\n",
              " Um I'll p,\n",
              " I'll probably ask Jonathan about it.,\n",
              " So alternatively, if we realise we can't do the whole thing in one go, we can probably just process some sort of meta-data,\n",
              " , you know what I mean, like sort of sort of for the whole series chunks representing the individual meetings or some Like something that represents the whole series in in a v in a structure very similar to the structure in which we represent individual um meetings, but with data sort of always combined from the whole series.,\n",
              " so instead of having an single utterance that we display, it would probably be like that would be representing a whole um topic, a segment in a meeting.,\n",
              " And sort of so that wi using the same data st Well, in a sense,\n",
              " Uh I'm I'm thinking of in a sense of like creating a virtual a virtual meeting out of the whole meeting series, sort of.,\n",
              " Yeah, sort of like off-line create a virtual meeting, which which basically treats the meeting series as if it was a meeting, and treats the individual meetings within the series as if they were segments, and treats the individual segments within meetings as if they were um utterances.,\n",
              " You know, so we just sort of we shift it one level up.,\n",
              " And in that way we could probably use the same algorithm and just like make vir like one or two ifs that say okay, if you are on a whole document uh a whole series level and that was a double-click, then don't just go into that um segment, but load a new file or something like it, but in general use the same algorithm.,\n",
              " That would be an alternative if we can't actually load the whole thing and 'Cause also,\n",
              " like even if we maybe this whole like maybe I'm worrying too much about the whole series in one thing display, because actually I mean probably users wouldn't view that one too often.,\n",
              " Yeah,,\n",
              " but I'm,\n",
              " I'm still worried.,\n",
              " Like for example for the display, if you actually if you want a display uh like for the whole series, the information density levels based on and and the f and the only granularity you have is individual utterances, that means you have to through every single utterance in a series of seventy hours of meetings.,\n",
              " Yeah.,\n",
              " Yeah,,\n",
              " and if you make that structurally very similar to the the le like one level down, like the way how we uh store individual utterances and stuff, then maybe we can more or less use the same code and just make a few ifs and stuff.,\n",
              " Yeah,,\n",
              " so so but still so in in general we're having we're having utterances and they have a score.,\n",
              " And that's as much as we really need.,\n",
              " And of cou,\n",
              " and they also have a time a time information of course.,\n",
              " Hmm?,\n",
              " And a and a s and a speaker information,,\n",
              " yeah.,\n",
              " Yeah, so an information which topic they're in,\n",
              " , yeah.,\n",
              " And and probably separate to that an information about the different topics like that,\n",
              " Yeah.,\n",
              " So so the skimming can work on that because the skimming just sort of sorts the utterances and puts as many in as it needs,\n",
              " Yeah.,\n",
              " Yeah, it'll it'll play them in some order in which they were set because otherwise it's gonna be more entertaining.,\n",
              " Um but that that's enough data for the skimming and the the searching, so what the searching does is the searching leaves the whole framework, goes to the S_Q_L_ database and gets like basically in the end gets just a time marker for where that is, like that utterance that we are concerned with.,\n",
              " And then we have to find I'm sure there's some way in in NITE X_M_L_,\n",
              " to just say set position to that time mark.,\n",
              " And then it shifts the whole frame,\n",
              " and it alerts every single element of the display and the display updates.,\n",
              " Yeah,,\n",
              " yeah.,\n",
              " That we can,\n",
              " ju,\n",
              " yeah,,\n",
              " but so so if if somethi so,\n",
              " yeah.,\n",
              " So if in that tree display somebody clicks on something,\n",
              " Yeah,,\n",
              " and then you sort of feed the time stamp to and the NITE X_M_L_ central manager, and that central manager alerts everything that's there,\n",
              " , like alerts the skim like the the audio display, alerts the text display, alerts the visual display and says we have a new time frame,\n",
              " and then they all sort of do their update routines with respect to the current level of zoom.,\n",
              " So how much do they display, and starting position at where the or maybe the mid-position of it, I don't know, like w if start where the thing was found or if that thing wa was found it's in the middle of the part that we display, that I don't know.,\n",
              " But that we can decide about, but a general sort of It's the same thing if like whether you play and it moves forward or whether you jump to a position through search, it's essentially for all the window handling,\n",
              " , it's the same event.,\n",
              " It's only that the event gets triggered by the search routine which sort of push that into NITE X_M_L_ and says please go there now.,\n",
              " Why do we have to do it in memory?,\n",
              " But that stuff's,\n",
              " so I mean like the information is coming from off-line.,\n",
              " So we probably we don't even have to change the utterance document, right, because the whole way, like the whole beauty of the NITE X_M_L_ is that it ties together lots of different files.,\n",
              " So we can just create an additional X_M_L_ file which for every utterance like the utterances have I_D_s,\n",
              " I presume, some references.,\n",
              " So we just we tie uh p just a very short X_M_L_ file, which it's the only information it has,\n",
              " that has whatever a number for for the um weight, for the information density, and we just tie that to the existing utterances and tie them to the existing speaker changes.,\n",
              " Well otherwise we probably have to go over it and like add some integer that we just increment from top to bottom sort of to every utterance as an as an I_D_ some type.,\n",
              " Or un or try to understand how NITE X_M_L_ I_D_s work,\n",
              " and maybe there's some special route we have to follow when we use these I_D_s.,\n",
              " It's alm hmm?,\n",
              " Yeah, the the girl said the utterances themselves are not numbered at the moment.,\n",
              " Okay.,\n",
              " Okay.,\n",
              " Okay.,\n",
              " Yeah.,\n",
              " So I guess that would be solvable if not.,\n",
              " Mm-hmm.,\n",
              " Sorry?,\n",
              " Okay.,\n",
              " Okay.,\n",
              " Is that a board marker pen actually?,\n",
              " Oh.,\n",
              " That's just so like to make a list of all this stuff, or we probably can somebody can do it on paper.,\n",
              " All these fancy pens.,\n",
              " So what so the stuff we have we have utterances and speakers and weights for utterances.,\n",
              " So for for every utterance sort of like the utterance has a speaker and a weight which is coming from outside.,\n",
              " Or we just tie it to it.,\n",
              " And there is segments, which hmm?,\n",
              " Oh, so sorry um.,\n",
              " Uh topic s topic segments I meant.,\n",
              " Like they are they are a super-unit.,\n",
              " So so the utterances are tied to topic segments.,\n",
              " And if the time stamps are on a word level, then we b somehow have to extract time stamps for utterances where they start.,\n",
              " W what segments now?,\n",
              " Okay.,\n",
              " Is the uh is that the same as utterances that is that the same as utterances that Mm-hmm.,\n",
              " Mm-hmm.,\n",
              " What so that's Oh.,\n",
              " But that's one o one segment or is that two segments then?,\n",
              " Yeah.,\n",
              " Okay.,\n",
              " Okay.,\n",
              " So,\n",
              " but but generally utterances is that which we just called uh sorry, segments is that which we just called utterances now.,\n",
              " Like it's it's the sa,\n",
              " it's sort of like one person's contribution at a time sort of thingy dingy.,\n",
              " Okay, so yeah, so we have those, and and then we have some f field somewhere else which has topics.,\n",
              " Yeah,,\n",
              " and and a topic's basically they are just on the I_D_, probably with a start time or something, and and the utterances referenced to those topics I guess.,\n",
              " So the topics don't contain any redundant thing of like showing the whole topic again, but they just sort of say a number and where they start and where they finish.,\n",
              " And the utterances then say which topic they belong to.,\n",
              " Yeah.,\n",
              " No.,\n",
              " But I was thinking of the topic segmentation now and and f for that there would only be one, right, because it's sort of like it's just a time window.,\n",
              " Yeah.,\n",
              " So if this lazy loading works, then this should definitely fit into I mean not memory then because it wouldn't all be in memory at the same time.,\n",
              " So if we just have those sort of that information like a long list of all the utterances slash segments and like short or smaller lists which give weight to them.,\n",
              " And even though probably if there's a lot of over-head in having two different files, we can probably merge the weights into it off-line.,\n",
              " You know what I mean, like if if there's a lot of bureaucracy involved with having two different trees and whether one ties to the other because the one has the weight for the other, then it's probably quicker to,\n",
              " just Yeah, I thought that was the whole beauty that like you can just make a new X_M_L_ file and sort of tie that to the other,\n",
              " and,\n",
              " and it,\n",
              " tre,\n",
              " Oh yeah.,\n",
              " So no, I didn't I didn't mean tree.,\n",
              " No.,\n",
              " No.,\n",
              " I meant just like handling two different files internally.,\n",
              " Sort of c I was just thinking you know like if if the overhead for having the same amount of data coming from two d files instead of from one file is massive then it would probably be for us easy to just like off-line put the the weight into into the file that has the segments, uh yeah, segments slash utterances already.,\n",
              " But that we can figure out I mean if it's going horrendously wrong.,\n",
              " Yeah.,\n",
              " Yeah.,\n",
              " Yeah,,\n",
              " no, we'd we'd be completely using like the whole infrastructure and,\n",
              " basically just I mean the main difference really between our project and theirs really is that we load a different part of the data.,\n",
              " But otherwise we're doing it the same way that they are doing it.,\n",
              " So we just we're sort of running different types of queries on it.,\n",
              " We in a sense we,\n",
              " I think we are running queries, it's not just about um what we load and what we don't load, but we're l running queries in the sense that we dynamically select by by weights,\n",
              " , don't we?,\n",
              " That we have to check how fast that is, like to say give us all the ones that whether that works with their query language, whether that's too many results and whether we shou,\n",
              " You know, if 'cause,\n",
              " if it i let's say I mean if if their query language is strange and if it would return b ten million results and it can't handle it, then we can just write our individual components in the way that they know which what the threshold is.,\n",
              " So they still get all the data and just they internally say oh no, this is less than three,\n",
              " and I'm not gonna display it or something.,\n",
              " Hmm?,\n",
              " Yeah.,\n",
              " No.,\n",
              " I'm just thinking for this whole thing of like a different level, sort of cutting out different different pieces, whether we do that through a query where we say give us everything that's ab above this,\n",
              " and this weight, or whether we skip the same infrastructure, but every individual module like the player and the display say like they still get sort of all the different utterances, uh all the different pieces, but,\n",
              " they say oh, this piece I leave out, because it's below the current threshold level.,\n",
              " When do we need the one for the meet,\n",
              " Okay.,\n",
              " Yeah, I guess for the so when we have the display, will we display the whole series.,\n",
              " Then if we have for the individual topic segments within the meetings if we have ready calculated disp um measures, then we don't have to sort of extract that data from the individual utterances.,\n",
              " Yeah, and that's also fairly easy to store along with our segments, isn't it.,\n",
              " For the segments, are we extracting some type of title for them that we craft with some fancy algorithm or manually or we're just taking the single most highly valued key-word utterance for the segment heading?,\n",
              " Hmm.,\n",
              " Hmm.,\n",
              " It's probably like in in the end,\n",
              " probably it wouldn't be the best thing if it's just the high most highly ranked phrase or key-word because like for example for an introduction that would most definitely not be anything that has any title anywhere similar to introduction or something.,\n",
              " Yeah.,\n",
              " Also like for this part, maybe if we go over it with named entity in the end, if I mean w if one of the people doing DIL has some named entity code to spare, and just like at least for the for sort of for finding topics, titles for for segments, just take a named entity which has a really high, what's it called,,\n",
              " D_F_I_D_F_, whatever.,\n",
              " 'Cause you'd probably be quite likely if they're talking about a conference or a person, that that would be a named entity which is very highly fr um frequented in that part.,\n",
              " Yeah, he said they're quite sparse.,\n",
              " So that basically was don't bother basing too much of your general calculation on it.,\n",
              " But like especially if they're sparse, probably individual named entities which describe what a what a segment is about would probably be quite good.,\n",
              " Like if there's some name of some conference, they would could probably say that name of the conference quite often, even though he's right that they make indirect references to it.,\n",
              " Anyway,\n",
              " Sorry?,\n",
              " So you're doing that on a on a per word level.,\n",
              " Okay.,\n",
              " Okay.,\n",
              " Okay, cool.,\n",
              " I was just wondering where you had the corpus from at the moment.,\n",
              " So it it seems that the data structure isn't a big problem and that basically we don't have to have all these massive discussions of how we exactly interact with the data structure because most of our work isn't done with that data structure in memory in the browser, but it's just done off-line and everyone can ha represent it anyway,\n",
              " they want as long as they sort of store it in a useful X_M_L_ representation in the end.,\n",
              " So like Yeah, that would mean understanding the NITE,\n",
              " X_M_L,\n",
              " _ X_M_L_ sort of format in a lot more detail.,\n",
              " We should I think we should just have a long session in the computer room together and like now that we know a bit more what we want, take a closer look at NITE X_M_L_.,\n",
              " Mm-hmm.,\n",
              " Mm-hmm.,\n",
              " Good.,\n",
              " Yeah, I haven't looked at this stuff much at all.,\n",
              " Yeah.,\n",
              " Yeah.,\n",
              " Who's who's sort of doing the the the central coordination of of of the browser application now?,\n",
              " Like Hmm?,\n",
              " Yeah, or but also like all these elements like like the loading and, yeah, integration and and like handling the data loading and stuff.,\n",
              " Nah.,\n",
              " I'm sort of like,\n",
              " I think I'll take over the display, just because I've started with a bit and found it found it doable.,\n",
              " So somebody should sort of be the one person who's who understands most about what's t centrally going on with with the with the project, like with the with the browser as a whole and where the data comes in and Any volunteers?,\n",
              " It's also a complicated one.,\n",
              " Yeah.,\n",
              " I know,\n",
              " but uh b I guess we can do it like several people together,\n",
              " , it's probably just those people have to work together a lot and very closely and just make sure that they're always f,\n",
              " understand what the other one is doing.,\n",
              " Yeah, or or ready-made versions of them for that matter,\n",
              " and,\n",
              " Yeah,,\n",
              " but I think actually like at the moment the integration comes first,\n",
              " , I mean it's sort of at the moment,\n",
              " the building the browser comes first, and then only comes the creating new sophisticated data chunks, because that's sort of the whole thing about having a prototype system which is more or less working on on chunk data.,\n",
              " But it at least we have the framework in which we can then test everything and and look at everything.,\n",
              " 'Cause before we have that, it's gonna be very difficult for anyone to really see how much the work that they're doing is making sense because you just well I guess you can see something from the data that you have in your individual,\n",
              " X_M_L_ s files files that you create, but it would be nice to have some basic system which just displays some stuff.,\n",
              " Or just adapt like their like just sort of go from their system and and adapt that piece for piece and see how we could how we could arran like adapt it to our system.,\n",
              " Does anyone want to like just sit with me and like play for three hours with NITE X_M_L_ at some point?,\n",
              " Uh I wouldn't like to be 'cause I'd like to go to the gym.,\n",
              " I'm theoretically free.,\n",
              " But if there's any time t hmm?,\n",
              " You have nothing no free time on Wednesday.,\n",
              " Hmm.,\n",
              " Nine ',\n",
              " til twelve and then nothi you have or you Hmm?,\n",
              " Anytime Wednesday afternoon I'd be cool, I think.,\n",
              " Yo, Forrest Hill, whatever one's easier to discuss stuff, I don't know.,\n",
              " I'm not biased.,\n",
              " Okay.,\n",
              " What time do you wanna do?,\n",
              " Okay, so I'll just meet you in in eighteen a in the afternoon.,\n",
              " I guess at the moment nobody critically depends on like the NITE X_M_L_ stuff working right now, right?,\n",
              " Like at the moment you can all do your stuff,\n",
              " and I can do my L_S_A_ stuff.,\n",
              " And I can even do the display to a vast degree without actually having their supplying framework working.,\n",
              " So it's not that crucial.,\n",
              " Yeah, actually I need the raw text as well.,\n",
              " Yeah,,\n",
              " but I was,\n",
              " I was,\n",
              " I was more thinking of the sort of the the whole browser framework as a running programme now.,\n",
              " Yeah, I think we all need the raw text in different in different flavours, don't we?,\n",
              " But number within the X_M_L_ context.,\n",
              " Are they spoken numbers?,\n",
              " Like do they look like they're utterances numbers?,\n",
              " There's the number task, isn't there.,\n",
              " That's part of the whole thing.,\n",
              " Hmm?,\n",
              " Okay.,\n",
              " Hmm.,\n",
              " Yeah, we have to probably cut that out anyway for our project, I don't know.,\n",
              " It's probably gonna screw up a lot of our data otherwise.,\n",
              " If Not sure if it what it does to document,\n",
              " It would probably make the yeah,,\n",
              " if if you have segments for that, probably the Okay.,\n",
              " Uh I'm just thinking like it pro,\n",
              " it pro probably like the L_S_A_ would perform quite well on it.,\n",
              " It would probably find another number task quite easily seeing that it's a constrained vocabulary with a high co-occurrence of the same nine words.,\n",
              " So that wou ten word.,\n",
              " Hmm?,\n",
              " Yeah.,\n",
              " I think it's also something that they they said the numbers in order, right?,\n",
              " Yeah, I think it's it,\n",
              " the,\n",
              " it sounded like they wanted to check out how well they were doing with overlapping and stuff,,\n",
              " because basically it's like they're reading them at different speeds, but you know in which order they are said.,\n",
              " Anyway.,\n",
              " ICSI has some reasons for doing it.,\n",
              " They must have been pissed off saying like numbers at the end of every meeting.,\n",
              " Um Dave, if you would or actually for well, if you're doing I_D_F_s or you whatever you call your your frequencies, I always mix up the name, uh you need some dictionary for that at some point though, like you need to have some representation of a word as not not that specific occurrence of that word token, but of of of a given word form.,\n",
              " Because you're making counts for word forms, right?,\n",
              " Yeah, so we should work together on that, because I need a dictionary as well.,\n",
              " Okay. ',\n",
              " Kay.,\n",
              " Okay.,\n",
              " Didn't you say that the o,\n",
              " the ord,\n",
              " Yeah,,\n",
              " but for I'm just wondering for the whole thing.,\n",
              " Does somebody wo who was it of you two who said that um there's some programme which spits out a dictionary probably with frequencies?,\n",
              " Okay.,\n",
              " Is anyone of you for the for the document frequency over total frequency, you gonna have total frequencies of words then with that, right?,\n",
              " Like over the whole corpus sort of.,\n",
              " Or W using which tool are you talking about?,\n",
              " Be careful with that.,\n",
              " Like my experience with the British National Corpus was that there's far more word types than you ever think because anything that's sort of unusual generally is a new word type.,\n",
              " Like any typo or any strange thing where they put two words together.,\n",
              " And also any number as a word type of its own.,\n",
              " So you can easily end up with hundred thousands of words when you didn't expect them.,\n",
              " So generally dictionaries can grow bigger,\n",
              " then you think they do.,\n",
              " Well you can probably also you can probably pre-filter like with regular expressions even just say if it consists of only dig digits, then skip it, or even if it consists any special characters, then skip it because it's probably something with a dot in between, which is usually not something you wanna have,\n",
              " and What I did, for my project I just ignored the hundred most frequent words, because they actually end up all being articles and and everything and stuff.,\n",
              " So we need like several of us need a dictionary.,\n",
              " Am I the only one who needs it with frequencies?,\n",
              " Am I the only one who needs it with frequencies?,\n",
              " Or Frequencies.,\n",
              " Yeah.,\n",
              " Well I guess as soon as we have the raw text, we can probably just start with the Java hash map and,\n",
              " like just hash map over it and see how far we get.,\n",
              " I mean we can probably on a machine with a few hundred megabyte RAM,\n",
              " you can go quite far.,\n",
              " You can write it on beefy.,\n",
              " So even if it goes wrong and even if it has a million words be,\n",
              " Oh yeah,\n",
              " , burning it on a like we should be able to burn the whole corpus, just the X_ hmm?,\n",
              " Ah I see,\n",
              " , I asked support about that two days ago.,\n",
              " In the Informatics building there,\n",
              " oh sorry, in in Appleton Tower five the ones closest t two machines closest to the support office.,\n",
              " So I presume,\n",
              " oh wait, I have the exact email.,\n",
              " I think he's talking about sort of the ones that Yeah, if you if you enter the big room, in the right-hand corner, I think.,\n",
              " Um the thing is like you can only burn from the local file-system.,\n",
              " So if it's from s,\n",
              " well actually I think if it's mounted, you can directly burn from there, but the problem is I have my data on beefy,\n",
              " and so I have to get it into the local temp directory and burn it from there.,\n",
              " But you can burn it from there.,\n",
              " Uh we looked that up and I for we looked that up,\n",
              " and I forgot.,\n",
              " Yeah,\n",
              " yeah.,\n",
              " No, you you we should be able to get it at,\n",
              " I don't think it was,\n",
              " I don't think it was a gigabyte.,\n",
              " Hmm.,\n",
              " See I would off,\n",
              " I would offer you to to get it on this one, and then um like copy it.,\n",
              " But you know what I figured out, I'm quicker down-loading over broad-band into my computer than using this hard disk.,\n",
              " There's something strange about the way how they access the hard disk, how they mount it, which is unfortunate.,\n",
              " Hmm.,\n",
              " What operating system do you have?,\n",
              " Okay.,\n",
              " Wh what connection do you have at home?,\n",
              " Yeah.,\n",
              " So if anyone of us gets it, we can then just use an ext hmm?,\n",
              " Yeah, burn it to C_D_ or,,\n",
              " yeah, put it on on hard disk, whatever.,\n",
              " Question is if you're not quicker if you uh because you should get massive compression out of that.,\n",
              " Like fifty percent or something with a good algorithm.,\n",
              " So if you could compress it and just put it into a temp directory.,\n",
              " Like The temp the temps usually have for gigabyte three or two.,\n",
              " The temps,,\n",
              " yeah.,\n",
              " I do like,\n",
              " I mean there's not guarantee that anything stays there,,\n",
              " but overnight it'll stay.,\n",
              " And I think the temps usually have.,\n",
              " Ah yeah,,\n",
              " but that would have to be the temp directory off the machine you can S_S_H_ into directory of S_S_H_.,\n",
              " Yeah, they wou they'd,\n",
              " they'd probably hate you for doing it.,\n",
              " But They'd probably they'd like you more if you,\n",
              " S_S_H,\n",
              " _ uh into another computer, compress it there and then sort of copy it into the into the gateway machine.,\n",
              " They have um if you S_S_,\n",
              " hey, you know, if you if you S_S_H_ and,\n",
              " they have this big warning about doing nothing at all in the gateway machine.,\n",
              " Yeah.,\n",
              " To your home machine.,\n",
              " I haven't I haven't figured out how to tunnel through the gateway into another machine yet.,\n",
              " It's not,\n",
              " it's not easy definitely.,\n",
              " That's why I end up sort of copying stuff into the temp directory at the gateway machine.,\n",
              " Sorry if this is boring everybody else.,\n",
              " This is just details and how to get stuff home from what we can probably just look at that together when we're meeting.,\n",
              " I'm sorry.,\n",
              " Mm-hmm.,\n",
              " Well yeah.,\n",
              " As soon as somebody gives me the raw text of the whole thing, I can probably just implement like a five line Java hash table frequency dictionary builder and see,\n",
              " Oh, did you not say frequencies f of words in the whole sorry,\n",
              " , did uh,\n",
              " So you'd you,\n",
              " Yeah, you'd have to count it yourself,,\n",
              " yeah.,\n",
              " Oh, you don't wanna have different counts for each chunk, but,\n",
              " just like sort of for for something from old chunks.,\n",
              " Oh yeah, no,\n",
              " , that's yeah,,\n",
              " so once I write an ar like w if I write like an algorithm which does a hash um table dictionary with frequency from a raw text, then the raw text can be anything.,\n",
              " So how far are we,\n",
              " g,\n",
              " uh how f how far are you getting raw text out of it,\n",
              " do you think?,\n",
              " Okay, well that's good, because for the dictionary the order doesn't make a difference, does it?,\n",
              " So yeah,,\n",
              " so um I'll get that from you,\n",
              " and I'll write the hash table which goes over that and creates a dictionary file.,\n",
              " So for the dictionary, is it okay if I do, whatever, word blank frequency or something?,\n",
              " Just p could everybody sort of start from that?,\n",
              " I mean I guess we can,\n",
              " Yeah, I I need frequency as well.,\n",
              " Well I think we might have a lot in common what we calculate because I for my latent semantic analysis need like counts of words within a document, uh within a a segment actually, within a topic segment.,\n",
              " Can I convert these probabilities back into frequencies?,\n",
              " Okay.,\n",
              " Oh, so that's what f Rainbow does, because that's what L_S_A_ builds on.,\n",
              " Like it builds a f a document by frequency matrix.,\n",
              " So I could probably get that.,\n",
              " Even though but I already have I already have my code to build it up myself.,\n",
              " No, don't bother.,\n",
              " I have my code already.,\n",
              " Um,\n",
              " Yeah, so Dave, you said you need the frequency counts actually for per document, would you say, not for the whole thing?,\n",
              " It more and more appears to me that if we if we scrap the notion of the meeting as an individual thing and sort of ju see meetings as as topic segments and have sort of like hierarchical topic segmentation instead, then it's b like a more coherent framework.,\n",
              " Wait, are we are we using this um for the for the for the do for the weighting in the end now, this this measure you're calculating?,\n",
              " Because if we're doing Like I think for for the information density we uh we should calculate it on the lowest level, not on the highest.,\n",
              " But like 'cause,\n",
              " Yeah,,\n",
              " but w it don't you have to like go sort of like for in a document versus the whole thing?,\n",
              " Isn't that how it works that you c look look at r,\n",
              " I don't think that's a good idea because isn't it like that we expect th there to change over i b with the different topic segments more?,\n",
              " That they talk about something different in each different topic segment.,\n",
              " 'Cause that's what relative term frequency is about, that like in some context they're talking more about a certain word than in general.,\n",
              " So that would more be the the topic segments then.,\n",
              " I don't know.,\n",
              " Yeah.,\n",
              " Yeah.,\n",
              " Yeah.,\n",
              " So I'm just wondering if there's ways to abandon the whole concept of of meetings and sort of but just not really treating separate meetings as too much of a separate entity.,\n",
              " But But on algorithmic level, whether we actually whether there's some way to just represent meetings as as topics.,\n",
              " Hmm.,\n",
              " That's not really what I meant.,\n",
              " But I think I have to think more about what I meant.,\n",
              " Um g I'm confused about everything.,\n",
              " Yeah.,\n",
              " I'm,\n",
              " I'm not so concerned about the m a meeting plus something else, I'm more talking about like,,\n",
              " yeah, the keeping keeping the same algorithm and the same way of handling it and just saying like just this this topic here,\n",
              " i,\n",
              " uh it happens to be like a whole meeting and it has sort of sub-topics, so,\n",
              " just that sort of topics a hierarchical concept where like a topic where there can be super-topics and topics, and,\n",
              " the super-topics are in the end what the meetings are, but in general at some level super-topics are treated like like topics.,\n",
              " Hmm.,\n",
              " Mm,\n",
              " I'm not really sure what I want.,\n",
              " So sorry, could describe that again, the Mm-hmm.,\n",
              " Mm-hmm.,\n",
              " Mm-hmm.,\n",
              " So that would be the series as a whole.,\n",
              " That would be sort of m meetings,,\n",
              " yeah.,\n",
              " Yeah.,\n",
              " I'm a,\n",
              " I'm a,\n",
              " I'm a bit brain-damaged at the moment, but,\n",
              " I think I'll just sit together with you again and and go through it again.,\n",
              " Hmm.,\n",
              " So so I'll is th it like,\n",
              " is this and this structurally then always identical?,\n",
              " So that we can that we can treat it with the same algorithm or,\n",
              " Yeah, I'm also not sure how we can go from from bottom-up.,\n",
              " I have always thought it's like more that oh, whatever, I'm a can't think of it at the moment.,\n",
              " Probably this is all too complicated worrying about that at that moment anyway.,\n",
              " Now have have we have we decided anything, are we doing anything?,\n",
              " S Wednesday we are meeting and looking at their at their implementation in some more detail to actually understand what's going on.,\n",
              " We had two things from their stuff just to make sure that we are like understand it, we understand it enough to to m modify it.,\n",
              " Yep.,\n",
              " How would we do that?,\n",
              " By just making like it w read write for everyone. ',\n",
              " Kay, who has most free space on their Same here.,\n",
              " Well we alternatively we can probably just make another directory on the beefy scratch space.,\n",
              " I mean that's where I'm having gigabytes and gigabytes of stuff at the moment.,\n",
              " No.,\n",
              " No.,\n",
              " Yeah.,\n",
              " But I think if he sends to the I think if he sends to the port he'd probably be in a better position.,\n",
              " Yeah.,\n",
              " Hmm.,\n",
              " I think he said yes to that.,\n",
              " I think uh that was like in,\n",
              " when we were still in the seminar room, I asked that once or like ask is it possible to get it off and,\n",
              " nobody said like people were discussing about the technical practicalities, but nobody said anything about al being allowed to or not allowed to.,\n",
              " I mean, we have access to it here,\n",
              " and I guess it probably means that we we can't give it to anybody else.,\n",
              " But but if they give us access to it here o sitting on a DICE machine, then there shouldn't be a reason why we shouldn't be able to use it on our laptop.,\n",
              " I personally don't have too many friends who would be too keen on getting it anyway.,\n",
              " I have that really excited pirate copied thing.,\n",
              " It annotated meeting data.,\n",
              " Huh.,\n",
              " Wait, wait, wait.,\n",
              " Um sorry.,\n",
              " Yeah, sorry.,\n",
              " What I just realised, we should really t keep different serieses completely separate for virtually all purposes.,\n",
              " Just let's be careful about that,,\n",
              " because like the the ICSI corpus isn't isn't one meeting series, it's several meeting series with different people meeting for completely different things.,\n",
              " For each meeting.,\n",
              " Alright.,\n",
              " Okay, but like let's just be careful that whatever we sort of we merge together,\n",
              " , that like the highest level of merging, it's not the whole ICSI corpus but individual series..,\n",
              " I think we might,\n",
              " actually I think That's probably be somewhere like well,\n",
              " or something like it.,\n",
              " Um I think we might just get away with for the whole project just like looking at only one series and just doing within one series.,\n",
              " I mean you can do everything you want in one series.,\n",
              " Oh yeah, let's take that.,\n",
              " Is the is the data always clearly split up by different series?,\n",
              " Uh like is it easy to just pick one,\n",
              " Okay.,\n",
              " Okay.,\n",
              " Okay.,\n",
              " Okay.,\n",
              " So at at every level everyone has to be careful to really just take even at the highest level, just take stuff from one series and not merge stuff from different series together because they would probably be just majorly messy.,\n",
              " Yeah,,\n",
              " so so t,\n",
              " so like if even if we make one single text file which has the whole corpus, sort of our corpus, that would still be from one series only.,\n",
              " Wou but it what you're producing at the moment is like individual text files that sort of have the raw text for a whole a meeting as a whole or Mm-hmm.,\n",
              " Yeah. ',\n",
              " Kay.,\n",
              " Um so is is anybody creating an uh a real raw text thing at the moment, like which is just the words?,\n",
              " Yeah, tha,\n",
              " 'cause that's what I'm gonna need as well.,\n",
              " But i but if there uh b aren't like,\n",
              " so it's,\n",
              " it's start and end times just for the file.,\n",
              " Like is it just the first and the last line?,\n",
              " Or is it for every single thing in,\n",
              " So what do you mean by just not print out that?,\n",
              " Okay.,\n",
              " If you're into it, can you make a text file which just like makes just the words? ',\n",
              " Kay.,\n",
              " Do you want it straight flowing, 'cause I would need something that marks the end of uh of uh,\n",
              " is is yours segmented by topics,\n",
              " then that like is there any information that you have to the topic, to the automated topic topic segmentation?,\n",
              " Oh then I need something different later anyway.,\n",
              " Okay, but for now, if you c,\n",
              " Okay.,\n",
              " You're gonna put that as an output of yours, the segmentation.,\n",
              " Okay, so for now can you create like sort of just uh a dump which is pure text, just pure text so that I can get a dictionary,\n",
              " and you can work on that for your topic segmentation.,\n",
              " And Or for for the series.,\n",
              " But I can,\n",
              " but I can also deal with separate files,,\n",
              " I mean I can just write the algorithm that it loads all files in a directory or something.,\n",
              " But I mean if you,\n",
              " But if you can put it in one single mega-file, that would be quite useful for me.,\n",
              " Even though for you, wouldn't it be easier if you had different files because then you sort of know like,\n",
              " Yeah.,\n",
              " So give m give me different files as long as,\n",
              " like it m if you could name them in a way that is easy to enumerate over them, like whatever,,\n",
              " one two three four five or something.,\n",
              " Or just anything that I can,\n",
              " Yeah.,\n",
              " Is is it something that's easily enu like to enumerate over?,\n",
              " Is it some just some ordered pattern?,\n",
              " Okay, cool.,\n",
              " Okay, cool.,\n",
              " Yeah.,\n",
              " In the right order.,\n",
              " It's just a wish list.,\n",
              " Orders.,\n",
              " When do you think you'll have um like a primitive segmentation by some ready-made topic segmentation by some ready-made tool ready?,\n",
              " Okay.,\n",
              " Okay, cool.,\n",
              " 'Cause I'll need that then when it's done.,\n",
              " Okay.,\n",
              " Mm-hmm.,\n",
              " What's what's nine megabyte?,\n",
              " The the That sounds quite reasonable.,\n",
              " That's nine nine characters over,\n",
              " okay.,\n",
              " Okay.,\n",
              " Okay.,\n",
              " That is for are we are we picking one particular series at the moment?,\n",
              " Or Yes.,\n",
              " Okay.,\n",
              " Yeah.,\n",
              " Yeah, I guess we can probably process the data for all different series and then check which series is the best for the presentation.,\n",
              " It sounds quite reasonable, nine megabyte.,\n",
              " I mean if you think if it's r roughly a million words and nine characters per word sounds realisti,\n",
              " Yeah.,\n",
              " Yes, I'm gonna build a dictionary then from that.,\n",
              " Like just a list of the words that maybe a list of the words with the frequencies or a list of the words sorted alphabetically or numerically.,\n",
              " What what does anyone want?,\n",
              " Does this there any wishes for dictionaries?,\n",
              " So I'll create a dictionary.,\n",
              " Add add the structure,,\n",
              " yeah.,\n",
              " And then the actual file we can probably like copy from your home directory or something like it.,\n",
              " Yeah,\n",
              " yeah,,\n",
              " but I'm sa,\n",
              " I'm saying for the whole thing in the end.,\n",
              " Then like the big thing we probably shouldn't do by email.,\n",
              " Yeah.,\n",
              " Oh, from the time I get the file I can do that in an afternoon, the next sort of the next morning.,\n",
              " Oh, you mean how long processing time it takes.,\n",
              " Ah, it's a,\n",
              " it's a bog standard algorithm.,\n",
              " I've,\n",
              " I've sort of I've written it for for DIL just in half an hour or something similar.,\n",
              " It's just you put them in a hash table and and,\n",
              " say well if it exists already in the hash table then you increase the count by one,\n",
              " and I'll probably implement some filter for filtering out numbers or something.,\n",
              " Really?,\n",
              " How do you do that?,\n",
              " Okay, well I don't know any Perl.,\n",
              " I mean if anyone wants to do a Perl script for that that does it does it nicely, I uh I've no problem with that.,\n",
              " I,\n",
              " but I think I have the Java code virtually ready because for DIL I wrote something very similar.,\n",
              " Like for DIL I wrote something that counts the the different occurrences of all the tags,\n",
              " um Sorry?,\n",
              " The hash table?,\n",
              " Uh I've never serialized anything.,\n",
              " Wouldn't that be absolutely massive though?,\n",
              " And then seriali and then write the serialization to a file.,\n",
              " So you want like a se like a file which is the serialization of a hash table.,\n",
              " Okay.,\n",
              " Yeah.,\n",
              " I,\n",
              " I'll,\n",
              " I'll check if I understand how it works.,\n",
              " I mean otherwise I can give you the code for loading a dictionary.,\n",
              " Give you my my,\n",
              " it's just,\n",
              " it's,\n",
              " it's,\n",
              " sort of it's a line break separated file, you know.,\n",
              " Yeah.,\n",
              " Yeah, I'll see if I understand how to serialize.,\n",
              " There's a there's a serialise command,\n",
              " so that gives me one mega mother of a s,\n",
              " Yeah, but do they automatically write to the file,\n",
              " anyway,\n",
              " I'll,\n",
              " I'll figure that out.,\n",
              " We don't have to,\n",
              " Yes, is that pretty much pretty much it?,\n",
              " So Dave and me look at how NITE X_M_L_ works,\n",
              " and we're Hmm.,\n",
              " I'll build a dictionary as soon as I get the text.,\n",
              " And yeah,,\n",
              " so that When do we have to meet again then with this?,\n",
              " How are we gonna do a demonstrator next week?,\n",
              " My God.,\n",
              " No no, not demonstrate, but like didn't you say that uh didn't we sort of agree that it would be useful to have a demonstrator of it, like some primitive thing working next week.,\n",
              " That's gotta be very prototype.,\n",
              " Mm-hmm.,\n",
              " Ah well, let's go.,\n",
              " Sorry.,\n",
              " I feel like like hanging mid-air and not really like finding a point where you can get your teeth into it and start working properly,\n",
              " and so it's all so fuzzy the whole,\n",
              " Yeah,,\n",
              " but it at the moment but at the moment it's also an implementational level.,\n",
              " Like with the data structures, I'm just like over these vague ideas of some trees, I'm f,\n",
              " Yeah.,\n",
              " It's just we are half-way through the project time table.,\n",
              " That's just what freaks me out.,\n",
              " Um,\n",
              " Yeah.,\n",
              " Yeah,,\n",
              " I mean if we just want to have um some data for the user face, could even be random data.,\n",
              " Uh mm mm,\n",
              " Yeah, I'm Hmm.,\n",
              " Yes.,\n",
              " Hmm yes.,\n",
              " Hmm.,\n",
              " I'm not so sure.,\n",
              " I,\n",
              " I thought we would just have like um one big summary um with all the uh different importance levels um displayed on it.,\n",
              " And depending on what our um zoom level is, we just display a part of it.,\n",
              " And we would have one very big thing off-line.,\n",
              " And from that we would just select what we are displaying.,\n",
              " Yes.,\n",
              " So for example you would um give a high value to those um sequences you want to display in the meeting series summary.,\n",
              " And you just cut off.,\n",
              " That was what I sh,\n",
              " I thought,,\n",
              " yeah.,\n",
              " I thought.,\n",
              " But I think the m difference might be,\n",
              " that we want just want to have um the words.,\n",
              " And that's not so much what he meant with not possibly loading everything was that you m um load all the uh annotation stuff, all the sound files,,\n",
              " all In Um,\n",
              " I r,\n",
              " I,\n",
              " I'm getting quite lost um at the moment because um w what's um our difference between the um se um uh the importance measure and the skimming?,\n",
              " I mean, do we do both or is it the same thing?,\n",
              " Okay.,\n",
              " So but when when we talk about summaries you talk about this uh abo about skimming and not about Yeah.,\n",
              " Yeah right, isn't that the skimming?,\n",
              " Isn't that the skimming?,\n",
              " Yeah,,\n",
              " but it use the same data.,\n",
              " Yeah.,\n",
              " A,\n",
              " And, yeah,,\n",
              " I think we also thought about combining that measure with um the measures I get from um s uh hot-spots and so on.,\n",
              " So that would also be on utterance level, I think.,\n",
              " I think.,\n",
              " Yes, sure.,\n",
              " Yes.,\n",
              " Yes, right.,\n",
              " Oops, it does.,\n",
              " So I define baseline and what it loads?,\n",
              " For example it loads all the utterances and so on, but it doesn't load um the discourse acts and for example not the and what's what else there?,\n",
              " Not the summaries.,\n",
              " It only loads those on demand.,\n",
              " Y you mean that you um basically split up th the big thing into um different summaries.,\n",
              " For example that you have a very um top-level um summary and a separate file for for each level.,\n",
              " Mm-hmm.,\n",
              " Yes.,\n",
              " N,\n",
              " Uh no no, it's f for No,\n",
              " , you're right.,\n",
              " Yeah.,\n",
              " It's for Um,\n",
              " No, I I think we would just take the segments that are already that were,\n",
              " Yeah, there's um this segments file.,\n",
              " Um you know, the X_M_L_ segments.,\n",
              " Oh.,\n",
              " That I don't know.,\n",
              " Yeah, that's um Mm-hmm.,\n",
              " There there are time stamps um for, well, segments um and for th um segments is for example when when you look at the data, what is displayed in one line.,\n",
              " What when when you look at it in the hmm?,\n",
              " I think so.,\n",
              " Isn't Um for ex,\n",
              " um I I compared it with what I did for the pause um duration extraction.,\n",
              " Um and basically it's uh words that are uttered in a sequence without pauses.,\n",
              " But sometimes um however there are um short pauses in it,\n",
              " and they're indicated by square brackets pause or something in the data.,\n",
              " Um someti uh but uh the annotators decided what was one segment and what wasn't.,\n",
              " I think so.,\n",
              " Yeah,,\n",
              " but um I think for some annotations um an uttera ca utterance can have several um types.,\n",
              " For example for the dialogue acts and so on.,\n",
              " Okay.,\n",
              " Yeah, that should be for Yeah.,\n",
              " Should be,,\n",
              " yeah.,\n",
              " Yes,,\n",
              " but that's,\n",
              " Yeah, everything that's a word has a sti time stamp.,\n",
              " That's at the end.,\n",
              " That's at the end, I think, her time.,\n",
              " Yeah, maybe.,\n",
              " Didn't have a look at our meetings.,\n",
              " Uh I I think it wouldn't as it occurs,\n",
              " I mean it would be,\n",
              " it occurs in every meeting.,\n",
              " So,\n",
              " And I think it even has uh its own annotation, like digits or something.,\n",
              " So that should be really easy to cut out.,\n",
              " Yeah.,\n",
              " I'm sure.,\n",
              " Ah it's just to test the system, I think.,\n",
              " So Mm they have to read numbers from,\n",
              " Uh I didn't have a look at that.,\n",
              " So They Mm-hmm.,\n",
              " Uh th yeah. ',\n",
              " Kay.,\n",
              " Um I just um wondered, so who's uh then doing um the frequencies on on the words, because I'm,\n",
              " I think I will also um I could also make use of it um for the agreement and disagreement thing.,\n",
              " Because I um I in my outline,\n",
              " I talked about um using the um discourse acts first, and um then in the chunks of text I found looking for word patterns and so on.,\n",
              " So um I would for example need the um most freq um frequent words.,\n",
              " So if you cut off all that, I'd won't be use,\n",
              " or,\n",
              " Yeah, I I,\n",
              " but I need it for my chunks then.,\n",
              " I would You know?,\n",
              " Yeah,,\n",
              " but I'd uh I would like to look at the frequency of words in my um in the regions of text,\n",
              " I found out to be interesting.,\n",
              " So I wouldn't need it.,\n",
              " It it would have to be re-calculated only for my segments.,\n",
              " Huh?,\n",
              " Uh uh mm.,\n",
              " I think it would be, you know, l as as big at as the hot-spot annotation things.,\n",
              " That's quite small,,\n",
              " yeah, that's some utterances.,\n",
              " Yes.,\n",
              " Yeah,,\n",
              " yeah.,\n",
              " So I would probably just concatenate all my um text chunks and then let's say m I will run over it.,\n",
              " Yes.,\n",
              " Yes, definitely.,\n",
              " Yeah, right.,\n",
              " Ye M Um Jasmine, uh um what is um the text you're extracting uh,\n",
              " looking like then at the end?,\n",
              " Because um I I think it's actually very similar to what I did for my um speaker um uh extraction,\n",
              " and I think I would ch perhaps have to change two lines of codes to get you um for each meeting a file that says fr from um this millisecond to this millisecond there was this sequence of words.,\n",
              " And so on.,\n",
              " So that's just changing two lines of code.,\n",
              " And it would give you that.,\n",
              " So Um yeah.,\n",
              " So far I extracted um the dura durations.,\n",
              " But it's from the words file.,\n",
              " So I could just um contatenate concatenate um the words instead of the durations, and,\n",
              " it should I mean Should be very straight-forward.,\n",
              " I can try to do it and send it to you.,\n",
              " Pe and you have a look at it, will it make sense for what you want.,\n",
              " Yeah, uh p,\n",
              " I mean it,\n",
              " I just let it run over all the files.,\n",
              " So Yes.,\n",
              " I just ordered.,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdQAyAkranVn"
      },
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "    for word in sent:\n",
        "        if word.text.lower() in word_freq.keys():\n",
        "            if sent not in sentence_scores.keys():\n",
        "                sentence_scores[sent] = word_freq[word.text.lower()]\n",
        "            else:\n",
        "                sentence_scores[sent] += word_freq[word.text.lower()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyfKaZCFanVn",
        "outputId": "1269f145-e658-4906-f55d-a57e40c5cb8a"
      },
      "source": [
        "print(sentence_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{Is there much more in it than he d: 0.08125, Is there much more in it than he said yesterday?: 0.140625, Yeah, now I'd say if for the prototype if we just like wherever possible p chunk in the stuff that we have um pre-annotated and stuff, and for the stuff that we don't have pre-annotated write like a stupid baseline, then we should probably be able to basically that means we focus on on the interface first sort of, so that we we take the the ready-made parts and just see how we get them work together in the interface the way we want: 2.6531250000000006, and then we have a working prototype.: 0.0625, And then we can go back and replace pieces either by our own components or by more sophisticated compo po components of our own.: 0.16875, So it's probably feasible.: 1.240625, The thing is I'm away this weekend.: 0.35000000000000003, So that's for me: 1.0, Oh yeah,: 0.01875, But also I might like the the similarity thing, like my just my matrix itself for my stuff: 0.44062499999999993, , I c: 0.028125, I think I can do that fairly quickly because I have the algorithms.: 0.3625, Yeah, I think today's meeting is really the one where we where we sort of settle down the data structure and as soon as we have that, uh probably like after today's meeting, we then actually need to well go back first of all and look at NITE X_M_L_ to see in how far that that which we want is compatible with that which NITE X_M_L_ offers us.: 4.571875, And then just sort of everyone make sure everyone understand the interface.: 0.5499999999999999, So I think if today we decide on what data we wanna have now,: 0.5375, and and later, maybe even today, we go and look at NITE X_M_L_ or some of us look at NITE X_M_L_ in a bit more detail, just trying to make some sense of that code and see how does the representation work in their system.: 0.9125000000000002, And then sort of with that knowledge we should be able to then say okay, that type of NITE X_M_L_ data we wanna load into it, and this is how everyone can access it, and then we should be able to go from there.: 0.85625, I've looked looked at the documentation and n like seen enough to make me think that we want to use the NITE X_M_L_ framework because um they have a good a event model that synchronizes sort of the data: 1.3093750000000002, and and every display element.: 0.15625, So that takes a lot of work away from us.: 0.17812499999999998, Sort of that would be a reason for staying within their framework and using their general classes.: 0.7375, But beyond that I haven't looked at it at all, which is something we should really do.: 0.7781250000000001, Who actually like for this whole discussion I mean, who of us is doing stuff that is happening on-line and who of us is doing stuff that's happening off-line?: 2.109375, Like my data is coming c Hmm?: 0.18750000000000003, So basically apart from the display module, the i the display itself, we don't have an extremely high degree of interaction between sort of our modules that create the stuff: 1.2062500000000003, and and the interface, so the interface is mainly while it's running just working on data that's just loaded from a file, I guess.: 2.4656249999999997, There isn't Yeah, I know.: 0.64375, Th: 0.046875, Yeah, the search is: 0.040625, I guess the search is sort of a strange beast anyway because for the search we're leaving the NITE X_M_L_ framework.: 0.6937500000000001, Um but that's still sort of that's good.: 2.365625, That means that at least like we don't have the type of situation where somebody has to do like a billion calculations on on data on-line.: 0.821875, 'Cause that would make it a lot more like that: 0.49374999999999997, would mean that our interface for the data would have to be a lot more careful about how it performs and and everything.: 1.1343750000000001, And nobody is modifying that data at at on-line time at all it seems.: 0.40625000000000006, Nobody's making any changes to the actual data on-line.: 1.3062500000000001, So that's actually making it a lot easier.: 1.1750000000000003, That basically means our browser really is a viewer mostly, which isn't doing much with the data except for sort of selecting a piece piece of it and and displaying it.: 1.2812500000000002, Well some parts relevant for the search, yes.: 0.0625, I'd say so.: 0.253125, Yeah, but nobody of us is doing much of searching from the data in the on-line stage.: 0.39375, And for all together, like the display itself, I think we are easier if we if it's sitting on the X_M_L_ than if it's sitting on the S_Q_L_ stuff, because if it's sitting on the X_M_L_, we have the the NITE X_M_L_ framework with all its functionality for synchronizing through all the different levels whenever there's a change, whenever something's moving forward and stuff.: 6.284375, And we can just more or less look at their code, like how their player moves forward, and how that moving forward is represented in different windows and stuff.: 0.503125, So I think in the actual browser itself: 0.384375, I don't wanna sit on the S_Q_L_ if we can sit on the X_M_L_ because sitting on the X_M_L_ we have all we have so much help.: 0.6218750000000002, And for y for like the p the calculations that we're doing apart from the search, it seems that everyone needs some special representations anyway.: 0.384375, You mean our results?: 0.259375, Yeah, in in the NITE X_M_L_ X_M_L_ format, so with their time stamps and stuff, so that it's easy to to tie together st things.: 1.521875, What I'm like what we have to think about is if we go with this multi-level idea, like this idea that sort of if you start with a whole meeting series as one entity, as one thing that you display, as one whole sort of, that then the individual chunks of the individual meetings, whereas and then you can click on a meeting, and then sort of the meeting is the whole thing and the chunks are the individual segments, that means sort of we have multiple levels of of representation, which we probably If we if we do it this way like we f we have to discuss that if we do it this way, then we should probably find some abstraction model, so that the interface in the sense like deals with it as if it's same so that the interface doesn't really have to worry whether it's a meeting in the whole meeting series or a segment within a meeting: 9.528125000000005, , you know what I mean?: 0.42500000000000004, And that's probably stuff that we have to sort of like process twice then.: 1.6875, Like for example that like the summary of a meeting within the whole meeting corpus or meeting series y is meeting series a good word for that?: 1.6093750000000002, I don't really know what how to call it.: 0.7687499999999999, You know what I mean, like not not the whole corpus, but every meeting that has to do with one topic.: 1.29375, Um so in in the meeting se series so that a summary for a meeting within the meeting series, are sort of compiled off-line by a summary module.: 1.378125, And that is separate from a summary of a segment within a meeting.: 0.371875, 'Cause I don't think we can So are we doing that at all levels?: 0.89375, Are we um And just have different like fine-grainedness levels sort of.: 0.453125, So the only thing that: 0.15625, the only thing that would happen basically if I double-click let's say from the whole meeting series on a single meeting, is that the zoom level changes.: 2.634374999999999, Like the th the start and the end position changes and the zoom level changes.: 0.38125, I thought we couldn't do that.: 0.634375, Like I was under the impression that we couldn't do that because we couldn't load the data for all that.: 1.3687500000000001, But I don't know: 0.64375, , I mean that So I'm s not sure if I got it.: 0.5499999999999999, I wa: 0.00625, I was just worried about the total memory complexity of it.: 0.075, But I I completely admit, I mean,: 0.265625, I just sort of like th took that from some thing that Jonathan once said about not loading everything.: 0.625, But maybe I was just wrong about it.: 0.109375, How many utterances: 0.175, w: 0.05, and I w: 0.05, So what we have is we would have a word.: 0.403125, Like we would have words with some priority levels.: 0.5062500000000001, And they would basically be because even the selection would would the summaries automatically feed from just how prioritized an individual word or how indiv uh prioritized an individual utterance is?: 1.4500000000000002, Or i are the summaries sort of refined from it and made by a machine to make sentences and stuff?: 0.590625, Or are they just sort of taking out the words with the highest priority and: 0.48750000000000004, then the words of the second highest priority?: 0.215625, And the u okay.: 0.003125, Are we doing it on th the whole thing on the utterance level?: 0.659375, Or are we doing it on word level, like the information density calculation?: 0.38125000000000003, I think we have start and end times for words actually, but it's yeah,: 1.765625, it might: 0.05625, but it might sound crazy in the player.: 0.0875, We should really maybe we can do that together at some point today that we check out how the player works.: 0.39062499999999994, But there's maybe some merit in altogether doing it on an utterance level in the end.: 1.4312500000000001, Well but also about the displays, I mean the displays in the in the text body, in the in the latest draft that we had sort of we came up with the idea that it isn't displaying utterance for utterance, but it's also displaying uh a summarised version in you know, like below the below the graph, the part.: 2.784375, Maybe Yeah, r Hmm?: 0.1125, Oh yeah,: 0.01875, f: 0.04375, it's just like: 1.0, there there's like audio skimming: 1.0718750000000001, and there's displayed skimming.: 1.05, maybe there's some merit of going altogether for utterance level and not even bother to calculate: 1.490625, I mean if you have to do it internally, then you can do it.: 0.259375, But maybe like not even store the importance levels for individual words and just sort of rank utterances as a whole.: 1.15625, 'Cause it: 0.05, it might be better skimming and less memory required at the same time.: 0.33125, And I mean if you if you know how to do it for individual words, then you can just in the worst case, if you can't find anything else, just sort of make the mean of the words over the utterance.: 2.184375, You know what I mean?: 0.42500000000000004, W: 0.05, it's: 1.0, it's: 1.0, Well what's the smallest chunk at the moment you're thinking of of assigning an importance measure to, is it a word or is it an utterance?: 1.575, So we're thinking of like maybe just storing it on a per utterance level.: 0.59375, Because it's it's less stuff to store probably for Dave in the in the audio playing.: 2.4843749999999996, And for in the display it's probably better if you have whole utterances than I don't know, like what it's like: 3.346875, if you just take single words out of utterances.: 0.40937500000000004, That probably doesn't make any sense at all, whereas if you just uh show important utterances but the utterance as a whole it makes more sense.: 1.409375, So it doesn't actually make a difference for your algorithm, 'cause it just means that if you're working on a word level, then we just mean it over the utterance.: 1.6218750000000002, Oh so that's good anyway then,: 1.1062500000000002, Because that makes it a lot easier than to t put it on utterance level.: 0.378125, Oh yeah.: 0.01875, but I mean like how how Jasmine does it internally: 0.259375, I don't know,: 0.64375, but it's probably,: 1.2375, yeah, you probably have to work on word levels for importance.: 0.46875, But there should be ways of easily going from a word level to an utterance level.: 0.528125, Yeah, prob Hmm.: 0.003125, Well we do a pre-filtering of sort of the whole thing, sort of like: 0.9437500000000001, but that, like the problem with that is: 0.028125, it's easy to do in the text level.: 1.28125, But that would mean it would still play the uh in your audio, unless we sort of also store what pieces we cut out for the audio.: 1.3968750000000003, I think before we can like answer that specific question how we c deal with that, it's probably good for us to look at what the audio player is capable of doing.: 1.8593750000000002, Yes.: 0.009375, So what do you mean by buffering?: 0.25, Like you think directly feeding: 0.359375, but not but not stored on the hard disk and then loaded in, but loaded in directly from memory.: 0.13125, But it's probably a stream if it exists in Java, it would be probably some binary stream going in of some type.: 1.8468750000000003, Okay, so I mean so that means that there's probably,: 1.51875, even if you go on an per utterance level, there's still some merit on within utterances cutting out stuff which clearly isn't relevant at all, and that maybe also for the audio we'd have to do.: 2.6625, So let's say we play the whole au phrase, but then in addition to that, we have some information that says minus that part of something.: 1.6875, That's okay, that we can do.: 1.0, Yeah, maybe even I mean that's sort of that depends on how how advanced we get.: 1.825, If maybe if we realise that there's massive differences in in gain or in something, you can probably just make some simple simple normalization, but that really depends on how much time we have and and how much is necessary.: 2.1062499999999997, Yeah, if like I d I don't know anything about audio: 0.7312500000000001, and I have never seen the player.: 0.040624999999999994, So if you find that the player accepts some n input from memory, and if it's easy to do, then I guess that's: 2.2281250000000004, that's fairly doable.: 1.015625, So but that means in the general structure we're actually quite lucky, so we we have we load into memory for the whole series of meetings just the utterances and rankings for the utterances and some information probably that says, well, the I guess that goes with the utterance, who's speaking.: 2.821875, Because then we can also do the display about who's speaking.: 1.1781249999999999, But I'm I'm still confused 'cause I thought like that's just what Jonathan said we do c that we can't do, like load a massive document of that size.: 2.2812500000000004, On the other hand The other hand, I mean it shouldn't be like should be like fifty mega-byte in RAM or something: 0.9343750000000001, , it shouldn't be massive, should it?: 0.49375, Actually fifty hundred megabyte is quite big in RAM.: 0.31562500000000004, Just thinking, what's the simp: 1.04375, so We do get an error message with the project if we load everything into the project with all the data they load.: 0.4875, So we know that doesn't work.: 0.70625, So our hope is essentially that we load less into it.: 0.10625, What's this lazy loading thing, somebody explain lazy loading to me.: 1.2656249999999998, So that is that only by type of file.: 0.1625, Like if if if the same thing is in different files, would it then maybe like, you know, if if utterances are split over three or ten or w hundred different files, is then a chance maybe that it doesn't try to load them all into memory at the same time, but just So why does it fail then in the first place?: 2.2750000000000004, Then it shouldn't ever fail,: 0.47812499999999997, because then it should never Yeah,: 0.009375, but um it uh it it failed right when you load it,: 0.140625, right, the NITE X_M_L_ kit, so that's interesting.: 1.0875000000000001, Let's check that out.: 1.046875, Um I'll p: 0.140625, I'll probably ask Jonathan about it.: 0.36874999999999997, So alternatively, if we realise we can't do the whole thing in one go, we can probably just process some sort of meta-data: 1.896875, , you know what I mean, like sort of sort of for the whole series chunks representing the individual meetings or some Like something that represents the whole series in in a v in a structure very similar to the structure in which we represent individual um meetings, but with data sort of always combined from the whole series.: 3.059375, so instead of having an single utterance that we display, it would probably be like that would be representing a whole um topic, a segment in a meeting.: 1.675, And sort of so that wi using the same data st Well, in a sense: 0.521875, Uh I'm I'm thinking of in a sense of like creating a virtual a virtual meeting out of the whole meeting series, sort of.: 1.5093750000000001, Yeah, sort of like off-line create a virtual meeting, which which basically treats the meeting series as if it was a meeting, and treats the individual meetings within the series as if they were segments, and treats the individual segments within meetings as if they were um utterances.: 2.115625, You know, so we just sort of we shift it one level up.: 0.815625, And in that way we could probably use the same algorithm and just like make vir like one or two ifs that say okay, if you are on a whole document uh a whole series level and that was a double-click, then don't just go into that um segment, but load a new file or something like it, but in general use the same algorithm.: 2.9437499999999996, That would be an alternative if we can't actually load the whole thing and 'Cause also: 1.4375, like even if we maybe this whole like maybe I'm worrying too much about the whole series in one thing display, because actually I mean probably users wouldn't view that one too often.: 3.1374999999999997, but I'm: 0.18125, I'm still worried.: 0.23750000000000002, Like for example for the display, if you actually if you want a display uh like for the whole series, the information density levels based on and and the f and the only granularity you have is individual utterances, that means you have to through every single utterance in a series of seventy hours of meetings.: 1.6656250000000001, and if you make that structurally very similar to the the le like one level down, like the way how we uh store individual utterances and stuff, then maybe we can more or less use the same code and just make a few ifs and stuff.: 1.41875, so so but still so in in general we're having we're having utterances and they have a score.: 0.6, And that's as much as we really need.: 1.325, And of cou: 0.003125, and they also have a time a time information of course.: 0.434375, And a and a s and a speaker information,: 0.1, Yeah, so an information which topic they're in: 0.375, And and probably separate to that an information about the different topics like that: 0.51875, So so the skimming can work on that because the skimming just sort of sorts the utterances and puts as many in as it needs: 0.61875, Yeah, it'll it'll play them in some order in which they were set because otherwise it's gonna be more entertaining.: 1.546875, Um but that that's enough data for the skimming and the the searching, so what the searching does is the searching leaves the whole framework, goes to the S_Q_L_ database and gets like basically in the end gets just a time marker for where that is, like that utterance that we are concerned with.: 1.984375, And then we have to find I'm sure there's some way in in NITE X_M_L_: 1.3218750000000001, to just say set position to that time mark.: 0.29062499999999997, And then it shifts the whole frame: 0.221875, and it alerts every single element of the display and the display updates.: 0.303125, ju: 0.00625, but so so if if somethi so: 0.003125, So if in that tree display somebody clicks on something: 0.296875, and then you sort of feed the time stamp to and the NITE X_M_L_ central manager, and that central manager alerts everything that's there: 1.540625, , like alerts the skim like the the audio display, alerts the text display, alerts the visual display and says we have a new time frame: 0.63125, and then they all sort of do their update routines with respect to the current level of zoom.: 0.43437499999999996, So how much do they display, and starting position at where the or maybe the mid-position of it, I don't know, like w if start where the thing was found or if that thing wa was found it's in the middle of the part that we display, that I don't know.: 3.2156250000000006, But that we can decide about, but a general sort of It's the same thing if like whether you play and it moves forward or whether you jump to a position through search, it's essentially for all the window handling: 2.66875, , it's the same event.: 1.009375, It's only that the event gets triggered by the search routine which sort of push that into NITE X_M_L_ and says please go there now.: 1.4749999999999999, Why do we have to do it in memory?: 0.05625, But that stuff's: 1.140625, so I mean like the information is coming from off-line.: 0.43750000000000006, So we probably we don't even have to change the utterance document, right, because the whole way, like the whole beauty of the NITE X_M_L_ is that it ties together lots of different files.: 1.8312500000000003, So we can just create an additional X_M_L_ file which for every utterance like the utterances have I_D_s: 0.484375, I presume, some references.: 0.0125, So we just we tie uh p just a very short X_M_L_ file, which it's the only information it has: 1.26875, that has whatever a number for for the um weight, for the information density, and we just tie that to the existing utterances and tie them to the existing speaker changes.: 0.47187499999999993, Well otherwise we probably have to go over it and like add some integer that we just increment from top to bottom sort of to every utterance as an as an I_D_ some type.: 0.8656249999999999, Or un or try to understand how NITE X_M_L_ I_D_s work: 0.1125, and maybe there's some special route we have to follow when we use these I_D_s.: 1.178125, It's alm hmm?: 1.003125, Yeah, the the girl said the utterances themselves are not numbered at the moment.: 0.278125, So I guess that would be solvable if not.: 0.346875, Sorry?: 0.028125, Is that a board marker pen actually?: 0.096875, Oh.: 0.01875, That's just so like to make a list of all this stuff, or we probably can somebody can do it on paper.: 1.5375, All these fancy pens.: 0.009375000000000001, So what so the stuff we have we have utterances and speakers and weights for utterances.: 0.45625000000000004, So for for every utterance sort of like the utterance has a speaker and a weight which is coming from outside.: 0.61875, Or we just tie it to it.: 0.01875, And there is segments, which hmm?: 0.109375, Oh, so sorry um.: 0.046875, Uh topic s topic segments I meant.: 0.3375, Like they are they are a super-unit.: 0.015625, So so the utterances are tied to topic segments.: 0.371875, And if the time stamps are on a word level, then we b somehow have to extract time stamps for utterances where they start.: 0.8375, W what segments now?: 0.159375, Is the uh is that the same as utterances that is that the same as utterances that Mm-hmm.: 0.3, What so that's Oh.: 1.01875, But that's one o one segment or is that two segments then?: 1.6906249999999998, but but generally utterances is that which we just called uh sorry, segments is that which we just called utterances now.: 0.46562500000000007, Like it's it's the sa: 2.00625, it's sort of like one person's contribution at a time sort of thingy dingy.: 2.9406249999999994, Okay, so yeah, so we have those, and and then we have some f field somewhere else which has topics.: 0.125, and and a topic's basically they are just on the I_D_, probably with a start time or something, and and the utterances referenced to those topics I guess.: 2.0218749999999996, So the topics don't contain any redundant thing of like showing the whole topic again, but they just sort of say a number and where they start and where they finish.: 1.490625, And the utterances then say which topic they belong to.: 0.38125, But I was thinking of the topic segmentation now and and f for that there would only be one, right, because it's sort of like it's just a time window.: 3.23125, So if this lazy loading works, then this should definitely fit into I mean not memory then because it wouldn't all be in memory at the same time.: 1.3312499999999998, So if we just have those sort of that information like a long list of all the utterances slash segments and like short or smaller lists which give weight to them.: 0.775, And even though probably if there's a lot of over-head in having two different files, we can probably merge the weights into it off-line.: 2.0125, You know what I mean, like if if there's a lot of bureaucracy involved with having two different trees and whether one ties to the other because the one has the weight for the other, then it's probably quicker to: 3.4499999999999997, just Yeah, I thought that was the whole beauty that like you can just make a new X_M_L_ file and sort of tie that to the other: 0.828125, tre: 0.003125, Oh yeah.: 0.01875, So no, I didn't I didn't mean tree.: 1.190625, I meant just like handling two different files internally.: 0.296875, Sort of c I was just thinking you know like if if the overhead for having the same amount of data coming from two d files instead of from one file is massive then it would probably be for us easy to just like off-line put the the weight into into the file that has the segments, uh yeah, segments slash utterances already.: 2.565625, But that we can figure out I mean if it's going horrendously wrong.: 1.29375, no, we'd we'd be completely using like the whole infrastructure and: 0.54375, basically just I mean the main difference really between our project and theirs really is that we load a different part of the data.: 0.959375, But otherwise we're doing it the same way that they are doing it.: 0.259375, So we just we're sort of running different types of queries on it.: 0.6312500000000001, We in a sense we: 0.046875, I think we are running queries, it's not just about um what we load and what we don't load, but we're l running queries in the sense that we dynamically select by by weights: 2.2468749999999997, , don't we?: 0.465625, That we have to check how fast that is, like to say give us all the ones that whether that works with their query language, whether that's too many results and whether we shou: 1.43125, You know, if 'cause: 0.22812500000000002, if it i let's say I mean if if their query language is strange and if it would return b ten million results and it can't handle it, then we can just write our individual components in the way that they know which what the threshold is.: 2.6625, So they still get all the data and just they internally say oh no, this is less than three: 0.5125000000000001, and I'm not gonna display it or something.: 0.65625, I'm just thinking for this whole thing of like a different level, sort of cutting out different different pieces, whether we do that through a query where we say give us everything that's ab above this: 2.703125, and this weight, or whether we skip the same infrastructure, but every individual module like the player and the display say like they still get sort of all the different utterances, uh all the different pieces, but: 1.309375, they say oh, this piece I leave out, because it's below the current threshold level.: 1.303125, When do we need the one for the meet: 0.36250000000000004, Yeah, I guess for the so when we have the display, will we display the whole series.: 0.5718749999999999, Then if we have for the individual topic segments within the meetings if we have ready calculated disp um measures, then we don't have to sort of extract that data from the individual utterances.: 1.5812499999999998, Yeah, and that's also fairly easy to store along with our segments, isn't it.: 1.765625, For the segments, are we extracting some type of title for them that we craft with some fancy algorithm or manually or we're just taking the single most highly valued key-word utterance for the segment heading?: 0.75625, It's probably like in in the end: 1.309375, probably it wouldn't be the best thing if it's just the high most highly ranked phrase or key-word because like for example for an introduction that would most definitely not be anything that has any title anywhere similar to introduction or something.: 2.9218749999999996, Also like for this part, maybe if we go over it with named entity in the end, if I mean w if one of the people doing DIL has some named entity code to spare, and just like at least for the for sort of for finding topics, titles for for segments, just take a named entity which has a really high, what's it called,: 2.734375, D_F_I_D_F_, whatever.: 0.053125, 'Cause you'd probably be quite likely if they're talking about a conference or a person, that that would be a named entity which is very highly fr um frequented in that part.: 1.1375000000000004, Yeah, he said they're quite sparse.: 0.35, So that basically was don't bother basing too much of your general calculation on it.: 0.6593750000000002, But like especially if they're sparse, probably individual named entities which describe what a what a segment is about would probably be quite good.: 1.275, Like if there's some name of some conference, they would could probably say that name of the conference quite often, even though he's right that they make indirect references to it.: 3.2124999999999995, Anyway: 0.05, Sorry?: 0.028125, So you're doing that on a on a per word level.: 0.44375000000000003, Okay, cool.: 0.015625, I was just wondering where you had the corpus from at the moment.: 0.125, So it it seems that the data structure isn't a big problem and that basically we don't have to have all these massive discussions of how we exactly interact with the data structure because most of our work isn't done with that data structure in memory in the browser, but it's just done off-line and everyone can ha represent it anyway: 3.5500000000000003, they want as long as they sort of store it in a useful X_M_L_ representation in the end.: 0.609375, So like Yeah, that would mean understanding the NITE: 0.5437500000000001, _ X_M_L_ sort of format in a lot more detail.: 0.353125, We should I think we should just have a long session in the computer room together and like now that we know a bit more what we want, take a closer look at NITE X_M_L_.: 0.9656250000000001, Good.: 0.0375, Yeah, I haven't looked at this stuff much at all.: 0.7125000000000001, Who's who's sort of doing the the the central coordination of of of the browser application now?: 2.3187499999999996, Yeah, or but also like all these elements like like the loading and, yeah, integration and and like handling the data loading and stuff.: 0.45625000000000004, I'm sort of like: 0.459375, I think I'll take over the display, just because I've started with a bit and found it found it doable.: 0.7593749999999999, So somebody should sort of be the one person who's who understands most about what's t centrally going on with with the with the project, like with the with the browser as a whole and where the data comes in and Any volunteers?: 3.003125, It's also a complicated one.: 1.3125, I know: 0.178125, but uh b I guess we can do it like several people together: 0.1875, , it's probably just those people have to work together a lot and very closely and just make sure that they're always f: 1.8625, understand what the other one is doing.: 0.253125, Yeah, or or ready-made versions of them for that matter: 0.06562499999999999, but I think actually like at the moment the integration comes first: 0.5499999999999999, , I mean it's sort of at the moment: 1.5843749999999999, the building the browser comes first, and then only comes the creating new sophisticated data chunks, because that's sort of the whole thing about having a prototype system which is more or less working on on chunk data.: 2.2531250000000003, But it at least we have the framework in which we can then test everything and and look at everything.: 0.24687499999999998, 'Cause before we have that, it's gonna be very difficult for anyone to really see how much the work that they're doing is making sense because you just well I guess you can see something from the data that you have in your individual: 2.4, X_M_L_ s files files that you create, but it would be nice to have some basic system which just displays some stuff.: 0.64375, Or just adapt like their like just sort of go from their system and and adapt that piece for piece and see how we could how we could arran like adapt it to our system.: 0.7875000000000002, Does anyone want to like just sit with me and like play for three hours with NITE X_M_L_ at some point?: 0.240625, Uh I wouldn't like to be 'cause I'd like to go to the gym.: 1.0406250000000001, I'm theoretically free.: 0.19999999999999998, But if there's any time t hmm?: 1.134375, You have nothing no free time on Wednesday.: 0.1625, Nine ': 0.028125, til twelve and then nothi you have or you Hmm?: 0.021875000000000002, Anytime Wednesday afternoon I'd be cool, I think.: 0.509375, Yo, Forrest Hill, whatever one's easier to discuss stuff, I don't know.: 2.096875, I'm not biased.: 0.18437499999999998, What time do you wanna do?: 0.159375, Okay, so I'll just meet you in in eighteen a in the afternoon.: 0.14687499999999998, I guess at the moment nobody critically depends on like the NITE X_M_L_ stuff working right now, right?: 0.48125, Like at the moment you can all do your stuff: 0.2, and I can do my L_S_A_ stuff.: 0.140625, And I can even do the display to a vast degree without actually having their supplying framework working.: 0.346875, So it's not that crucial.: 1.00625, Yeah, actually I need the raw text as well.: 0.34375, I was more thinking of the sort of the the whole browser framework as a running programme now.: 0.6124999999999999, Yeah, I think we all need the raw text in different in different flavours, don't we?: 1.328125, But number within the X_M_L_ context.: 0.1, Are they spoken numbers?: 0.040625, Like do they look like they're utterances numbers?: 0.4625, There's the number task, isn't there.: 1.5062499999999999, That's part of the whole thing.: 1.4, Yeah, we have to probably cut that out anyway for our project, I don't know.: 0.975, It's probably gonna screw up a lot of our data otherwise.: 1.6812500000000004, If Not sure if it what it does to document: 0.140625, It would probably make the yeah,: 0.628125, if if you have segments for that, probably the Okay.: 0.346875, Uh I'm just thinking like it pro: 0.228125, it pro probably like the L_S_A_ would perform quite well on it.: 0.64375, It would probably find another number task quite easily seeing that it's a constrained vocabulary with a high co-occurrence of the same nine words.: 1.9656250000000002, So that wou ten word.: 0.125, I think it's also something that they they said the numbers in order, right?: 1.79375, Yeah, I think it's it: 1.34375, it sounded like they wanted to check out how well they were doing with overlapping and stuff,: 0.178125, because basically it's like they're reading them at different speeds, but you know in which order they are said.: 1.6531250000000002, Anyway.: 0.05, ICSI has some reasons for doing it.: 0.003125, They must have been pissed off saying like numbers at the end of every meeting.: 0.39999999999999997, Um Dave, if you would or actually for well, if you're doing I_D_F_s or you whatever you call your your frequencies, I always mix up the name, uh you need some dictionary for that at some point though, like you need to have some representation of a word as not not that specific occurrence of that word token, but of of of a given word form.: 1.4468750000000001, Because you're making counts for word forms, right?: 0.415625, Yeah, so we should work together on that, because I need a dictionary as well.: 0.321875, Didn't you say that the o: 0.584375, the ord: 0.00625, but for I'm just wondering for the whole thing.: 0.559375, Does somebody wo who was it of you two who said that um there's some programme which spits out a dictionary probably with frequencies?: 1.4968750000000002, Is anyone of you for the for the document frequency over total frequency, you gonna have total frequencies of words then with that, right?: 0.7250000000000001, Like over the whole corpus sort of.: 0.546875, Or W using which tool are you talking about?: 0.12187500000000001, Be careful with that.: 0.015625, Like my experience with the British National Corpus was that there's far more word types than you ever think because anything that's sort of unusual generally is a new word type.: 3.0625, Like any typo or any strange thing where they put two words together.: 0.525, And also any number as a word type of its own.: 0.246875, So you can easily end up with hundred thousands of words when you didn't expect them.: 0.75, So generally dictionaries can grow bigger: 0.021875, then you think they do.: 0.34375, Well you can probably also you can probably pre-filter like with regular expressions even just say if it consists of only dig digits, then skip it, or even if it consists any special characters, then skip it because it's probably something with a dot in between, which is usually not something you wanna have: 2.5156249999999996, and What I did, for my project I just ignored the hundred most frequent words, because they actually end up all being articles and and everything and stuff.: 0.578125, So we need like several of us need a dictionary.: 0.3625, Am I the only one who needs it with frequencies?: 0.28125, Am I the only one who needs it with frequencies?: 0.28125, Or Frequencies.: 0.03125, Well I guess as soon as we have the raw text, we can probably just start with the Java hash map and: 0.5468750000000001, like just hash map over it and see how far we get.: 0.25625000000000003, I mean we can probably on a machine with a few hundred megabyte RAM: 0.553125, you can go quite far.: 0.225, You can write it on beefy.: 0.043750000000000004, So even if it goes wrong and even if it has a million words be: 0.346875, Oh yeah: 0.01875, , burning it on a like we should be able to burn the whole corpus, just the X_ hmm?: 0.321875, Ah I see: 0.071875, , I asked support about that two days ago.: 0.084375, In the Informatics building there: 0.0125, oh sorry, in in Appleton Tower five the ones closest t two machines closest to the support office.: 0.175, So I presume: 0.00625, oh wait, I have the exact email.: 0.04375, I think he's talking about sort of the ones that Yeah, if you if you enter the big room, in the right-hand corner, I think.: 2.2, Um the thing is like you can only burn from the local file-system.: 0.35000000000000003, So if it's from s: 1.0, well actually I think if it's mounted, you can directly burn from there, but the problem is I have my data on beefy: 1.646875, and so I have to get it into the local temp directory and burn it from there.: 0.190625, But you can burn it from there.: 0.01875, Uh we looked that up and I for we looked that up: 0.05, and I forgot.: 0.003125, No, you you we should be able to get it at: 0.14375, I don't think it was: 0.809375, I don't think it was a gigabyte.: 0.8156249999999999, See I would off: 0.365625, I would offer you to to get it on this one, and then um like copy it.: 0.6499999999999999, But you know what I figured out, I'm quicker down-loading over broad-band into my computer than using this hard disk.: 0.496875, There's something strange about the way how they access the hard disk, how they mount it, which is unfortunate.: 1.284375, What operating system do you have?: 0.03125, Wh what connection do you have at home?: 0.021875, So if anyone of us gets it, we can then just use an ext hmm?: 0.159375, Yeah, burn it to C_D_ or,: 0.01875, yeah, put it on on hard disk, whatever.: 0.128125, Question is if you're not quicker if you uh because you should get massive compression out of that.: 0.340625, Like fifty percent or something with a good algorithm.: 0.2625, So if you could compress it and just put it into a temp directory.: 0.2125, Like The temp the temps usually have for gigabyte three or two.: 0.125, The temps,: 0.009375, I mean there's not guarantee that anything stays there,: 1.3062500000000001, but overnight it'll stay.: 0.128125, And I think the temps usually have.: 0.36250000000000004, but that would have to be the temp directory off the machine you can S_S_H_ into directory of S_S_H_.: 0.40625, Yeah, they wou they'd: 0.140625, they'd probably hate you for doing it.: 0.37812499999999993, But They'd probably they'd like you more if you: 0.50625, _ uh into another computer, compress it there and then sort of copy it into the into the gateway machine.: 0.384375, hey, you know, if you if you S_S_H_ and: 0.18125, they have this big warning about doing nothing at all in the gateway machine.: 0.15000000000000002, To your home machine.: 0.053125000000000006, I haven't I haven't figured out how to tunnel through the gateway into another machine yet.: 1.028125, It's not: 1.0, it's not easy definitely.: 1.078125, That's why I end up sort of copying stuff into the temp directory at the gateway machine.: 1.590625, Sorry if this is boring everybody else.: 0.06875, This is just details and how to get stuff home from what we can probably just look at that together when we're meeting.: 1.0687499999999999, I'm sorry.: 0.209375, As soon as somebody gives me the raw text of the whole thing, I can probably just implement like a five line Java hash table frequency dictionary builder and see: 1.1562499999999998, Oh, did you not say frequencies f of words in the whole sorry: 0.621875, So you'd you: 0.134375, Yeah, you'd have to count it yourself,: 0.14375, Oh, you don't wanna have different counts for each chunk, but: 0.6718749999999999, just like sort of for for something from old chunks.: 0.48437499999999994, Oh yeah, no: 0.01875, , that's yeah,: 1.0, so once I write an ar like w if I write like an algorithm which does a hash um table dictionary with frequency from a raw text, then the raw text can be anything.: 0.6656249999999999, So how far are we: 0.028125, g: 0.00625, uh how f how far are you getting raw text out of it: 0.22499999999999998, do you think?: 0.34375, Okay, well that's good, because for the dictionary the order doesn't make a difference, does it?: 1.709375, so um I'll get that from you: 0.23125, and I'll write the hash table which goes over that and creates a dictionary file.: 0.428125, So for the dictionary, is it okay if I do, whatever, word blank frequency or something?: 0.44375, Just p could everybody sort of start from that?: 0.484375, I mean I guess we can: 0.296875, Yeah, I I need frequency as well.: 0.178125, Well I think we might have a lot in common what we calculate because I for my latent semantic analysis need like counts of words within a document, uh within a a segment actually, within a topic segment.: 1.34375, Can I convert these probabilities back into frequencies?: 0.053125000000000006, Oh, so that's what f Rainbow does, because that's what L_S_A_ builds on.: 2.06875, Like it builds a f a document by frequency matrix.: 0.184375, So I could probably get that.: 0.46249999999999997, Even though but I already have I already have my code to build it up myself.: 0.265625, No, don't bother.: 0.47500000000000003, I have my code already.: 0.078125, Yeah, so Dave, you said you need the frequency counts actually for per document, would you say, not for the whole thing?: 1.21875, It more and more appears to me that if we if we scrap the notion of the meeting as an individual thing and sort of ju see meetings as as topic segments and have sort of like hierarchical topic segmentation instead, then it's b like a more coherent framework.: 2.58125, Wait, are we are we using this um for the for the for the do for the weighting in the end now, this this measure you're calculating?: 0.34375, Because if we're doing Like I think for for the information density we uh we should calculate it on the lowest level, not on the highest.: 0.8406250000000001, But like 'cause: 0.05, but w it don't you have to like go sort of like for in a document versus the whole thing?: 1.3343749999999999, Isn't that how it works that you c look look at r: 0.7062499999999998, I don't think that's a good idea because isn't it like that we expect th there to change over i b with the different topic segments more?: 2.7937499999999997, That they talk about something different in each different topic segment.: 0.6, 'Cause that's what relative term frequency is about, that like in some context they're talking more about a certain word than in general.: 1.475, So that would more be the the topic segments then.: 0.509375, I don't know.: 0.64375, So I'm just wondering if there's ways to abandon the whole concept of of meetings and sort of but just not really treating separate meetings as too much of a separate entity.: 2.096875, But But on algorithmic level, whether we actually whether there's some way to just represent meetings as as topics.: 1.4593749999999999, That's not really what I meant.: 1.134375, But I think I have to think more about what I meant.: 0.703125, Um g I'm confused about everything.: 0.246875, I'm: 0.18125, I'm not so concerned about the m a meeting plus something else, I'm more talking about like,: 0.790625, yeah, the keeping keeping the same algorithm and the same way of handling it and just saying like just this this topic here: 0.246875, uh it happens to be like a whole meeting and it has sort of sub-topics, so: 0.7499999999999999, just that sort of topics a hierarchical concept where like a topic where there can be super-topics and topics, and: 0.546875, the super-topics are in the end what the meetings are, but in general at some level super-topics are treated like like topics.: 0.446875, I'm not really sure what I want.: 0.49374999999999997, So sorry, could describe that again, the Mm-hmm.: 0.146875, So that would be the series as a whole.: 0.634375, That would be sort of m meetings,: 0.634375, I'm a: 0.18125, I'm a: 0.18125, I'm a bit brain-damaged at the moment, but: 0.271875, I think I'll just sit together with you again and and go through it again.: 0.646875, So so I'll is th it like: 0.165625, is this and this structurally then always identical?: 0.034375, So that we can that we can treat it with the same algorithm or: 0.053125, Yeah, I'm also not sure how we can go from from bottom-up.: 0.425, I have always thought it's like more that oh, whatever, I'm a can't think of it at the moment.: 2.2375000000000003, Probably this is all too complicated worrying about that at that moment anyway.: 0.359375, Now have have we have we decided anything, are we doing anything?: 0.11249999999999999, S Wednesday we are meeting and looking at their at their implementation in some more detail to actually understand what's going on.: 1.3875, We had two things from their stuff just to make sure that we are like understand it, we understand it enough to to m modify it.: 0.48125, How would we do that?: 0.29375, By just making like it w read write for everyone. ': 0.14375000000000002, Kay, who has most free space on their Same here.: 0.03125, Well we alternatively we can probably just make another directory on the beefy scratch space.: 0.42500000000000004, I mean that's where I'm having gigabytes and gigabytes of stuff at the moment.: 1.640625, But I think if he sends to the I think if he sends to the port he'd probably be in a better position.: 1.10625, I think he said yes to that.: 0.40937500000000004, I think uh that was like in: 0.34375, when we were still in the seminar room, I asked that once or like ask is it possible to get it off and: 0.20625, nobody said like people were discussing about the technical practicalities, but nobody said anything about al being allowed to or not allowed to.: 0.24375, I mean, we have access to it here: 0.259375, and I guess it probably means that we we can't give it to anybody else.: 0.8937499999999999, But but if they give us access to it here o sitting on a DICE machine, then there shouldn't be a reason why we shouldn't be able to use it on our laptop.: 1.175, I personally don't have too many friends who would be too keen on getting it anyway.: 0.8625000000000002, I have that really excited pirate copied thing.: 0.284375, It annotated meeting data.: 0.365625, Huh.: 0.00625, Wait, wait, wait.: 0.037500000000000006, Um sorry.: 0.028125, Yeah, sorry.: 0.028125, What I just realised, we should really t keep different serieses completely separate for virtually all purposes.: 0.30624999999999997, Just let's be careful about that,: 1.04375, because like the the ICSI corpus isn't isn't one meeting series, it's several meeting series with different people meeting for completely different things.: 3.431249999999999, For each meeting.: 0.20625, Okay, but like let's just be careful that whatever we sort of we merge together: 1.4593749999999999, , that like the highest level of merging, it's not the whole ICSI corpus but individual series..: 1.6374999999999997, I think we might: 0.4, actually I think That's probably be somewhere like well: 1.675, or something like it.: 0.165625, Um I think we might just get away with for the whole project just like looking at only one series and just doing within one series.: 1.553125, I mean you can do everything you want in one series.: 0.784375, Oh yeah, let's take that.: 1.1, Is the is the data always clearly split up by different series?: 0.44687499999999997, Uh like is it easy to just pick one: 0.29375, So at at every level everyone has to be careful to really just take even at the highest level, just take stuff from one series and not merge stuff from different series together because they would probably be just majorly messy.: 2.1968749999999995, so like if even if we make one single text file which has the whole corpus, sort of our corpus, that would still be from one series only.: 1.9625, Wou but it what you're producing at the moment is like individual text files that sort of have the raw text for a whole a meeting as a whole or Mm-hmm.: 1.5281249999999997, Um so is is anybody creating an uh a real raw text thing at the moment, like which is just the words?: 0.540625, Yeah, tha: 0.003125, 'cause that's what I'm gonna need as well.: 1.575, But i but if there uh b aren't like: 0.5, so it's: 1.0, it's start and end times just for the file.: 1.3031249999999999, Like is it just the first and the last line?: 0.140625, Or is it for every single thing in: 0.253125, So what do you mean by just not print out that?: 0.253125, If you're into it, can you make a text file which just like makes just the words? ': 0.690625, Do you want it straight flowing, 'cause I would need something that marks the end of uh of uh: 0.85, is is yours segmented by topics: 0.046875, then that like is there any information that you have to the topic, to the automated topic topic segmentation?: 0.428125, Oh then I need something different later anyway.: 0.4937499999999999, Okay, but for now, if you c: 0.028125, You're gonna put that as an output of yours, the segmentation.: 0.475, Okay, so for now can you create like sort of just uh a dump which is pure text, just pure text so that I can get a dictionary: 0.678125, and you can work on that for your topic segmentation.: 0.19062500000000002, And Or for for the series.: 0.128125, but I can also deal with separate files,: 0.17812499999999998, I mean I can just write the algorithm that it loads all files in a directory or something.: 0.6124999999999999, But I mean if you: 0.246875, But if you can put it in one single mega-file, that would be quite useful for me.: 0.890625, Even though for you, wouldn't it be easier if you had different files because then you sort of know like: 1.54375, So give m give me different files as long as: 0.30624999999999997, like it m if you could name them in a way that is easy to enumerate over them, like whatever,: 0.3, one two three four five or something.: 0.49375, Or just anything that I can: 0.053125, Is is it something that's easily enu like to enumerate over?: 1.1875, Is it some just some ordered pattern?: 0.024999999999999998, Okay, cool.: 0.015625, Okay, cool.: 0.015625, In the right order.: 0.1125, It's just a wish list.: 1.0375, When do you think you'll have um like a primitive segmentation by some ready-made topic segmentation by some ready-made tool ready?: 0.7281249999999998, Okay, cool.: 0.015625, 'Cause I'll need that then when it's done.: 1.309375, What's what's nine megabyte?: 2.0406250000000004, The the That sounds quite reasonable.: 0.128125, That's nine nine characters over: 1.0656249999999998, That is for are we are we picking one particular series at the moment?: 0.43124999999999997, Or Yes.: 0.009375, Yeah, I guess we can probably process the data for all different series and then check which series is the best for the presentation.: 0.8781250000000002, It sounds quite reasonable, nine megabyte.: 0.16875, I mean if you think if it's r roughly a million words and nine characters per word sounds realisti: 1.9718749999999998, Yes, I'm gonna build a dictionary then from that.: 0.50625, Like just a list of the words that maybe a list of the words with the frequencies or a list of the words sorted alphabetically or numerically.: 0.7500000000000002, What what does anyone want?: 0.15625, Does this there any wishes for dictionaries?: 0.009375000000000001, So I'll create a dictionary.: 0.2, Add add the structure,: 0.056249999999999994, And then the actual file we can probably like copy from your home directory or something like it.: 0.615625, but I'm sa: 0.1875, I'm saying for the whole thing in the end.: 0.6375000000000001, Then like the big thing we probably shouldn't do by email.: 0.9500000000000001, Oh, from the time I get the file I can do that in an afternoon, the next sort of the next morning.: 0.7593750000000001, Oh, you mean how long processing time it takes.: 0.45312499999999994, Ah, it's a: 1.0, it's a bog standard algorithm.: 1.053125, I've: 0.075, I've sort of I've written it for for DIL just in half an hour or something similar.: 0.640625, It's just you put them in a hash table and and: 1.10625, say well if it exists already in the hash table then you increase the count by one: 0.465625, and I'll probably implement some filter for filtering out numbers or something.: 0.5874999999999999, Really?: 0.11875, Okay, well I don't know any Perl.: 0.64375, I mean if anyone wants to do a Perl script for that that does it does it nicely, I uh I've no problem with that.: 0.39375000000000004, but I think I have the Java code virtually ready because for DIL I wrote something very similar.: 0.60625, Like for DIL I wrote something that counts the the different occurrences of all the tags: 0.33125, um Sorry?: 0.028125, The hash table?: 0.0625, Uh I've never serialized anything.: 0.140625, Wouldn't that be absolutely massive though?: 0.83125, And then seriali and then write the serialization to a file.: 0.178125, So you want like a se like a file which is the serialization of a hash table.: 0.34062499999999996, I'll: 0.11875, I'll check if I understand how it works.: 0.18124999999999997, I mean otherwise I can give you the code for loading a dictionary.: 0.4375, Give you my my: 0.0375, it's just: 1.0, it's: 1.0, it's: 1.0, sort of it's a line break separated file, you know.: 1.690625, Yeah, I'll see if I understand how to serialize.: 0.22499999999999998, There's a there's a serialise command: 2.0062499999999996, so that gives me one mega mother of a s: 0.253125, Yeah, but do they automatically write to the file: 0.178125, anyway: 0.05, I'll: 0.11875, I'll figure that out.: 0.125, We don't have to: 0.465625, Yes, is that pretty much pretty much it?: 0.203125, So Dave and me look at how NITE X_M_L_ works: 0.109375, and we're Hmm.: 0.184375, I'll build a dictionary as soon as I get the text.: 0.434375, so that When do we have to meet again then with this?: 0.009375, How are we gonna do a demonstrator next week?: 0.28125, No no, not demonstrate, but like didn't you say that uh didn't we sort of agree that it would be useful to have a demonstrator of it, like some primitive thing working next week.: 1.940625, That's gotta be very prototype.: 1.078125, Ah well, let's go.: 1.121875, Sorry.: 0.028125, I feel like like hanging mid-air and not really like finding a point where you can get your teeth into it and start working properly: 0.390625, and so it's all so fuzzy the whole: 1.215625, but it at the moment but at the moment it's also an implementational level.: 1.328125, Like with the data structures, I'm just like over these vague ideas of some trees, I'm f: 0.575, It's just we are half-way through the project time table.: 1.2562499999999999, That's just what freaks me out.: 1.003125, I mean if we just want to have um some data for the user face, could even be random data.: 0.8687500000000001, Yeah, I'm Hmm.: 0.18125, Yes.: 0.009375, Hmm yes.: 0.009375, I'm not so sure.: 0.246875, I thought we would just have like um one big summary um with all the uh different importance levels um displayed on it.: 0.8906249999999999, And depending on what our um zoom level is, we just display a part of it.: 0.26875, And we would have one very big thing off-line.: 0.85625, And from that we would just select what we are displaying.: 0.315625, Yes.: 0.009375, So for example you would um give a high value to those um sequences you want to display in the meeting series summary.: 0.9718749999999998, And you just cut off.: 0.01875, That was what I sh: 0.003125, I thought,: 0.05625, I thought.: 0.05625, But I think the m difference might be: 0.415625, that we want just want to have um the words.: 0.425, And that's not so much what he meant with not possibly loading everything was that you m um load all the uh annotation stuff, all the sound files,: 1.4687500000000002, I r: 0.0125, I'm getting quite lost um at the moment because um w what's um our difference between the um se um uh the importance measure and the skimming?: 1.525, I mean, do we do both or is it the same thing?: 0.403125, So but when when we talk about summaries you talk about this uh abo about skimming and not about Yeah.: 0.07500000000000001, Yeah right, isn't that the skimming?: 0.5812499999999999, Isn't that the skimming?: 0.503125, but it use the same data.: 0.209375, I think we also thought about combining that measure with um the measures I get from um s uh hot-spots and so on.: 0.6375000000000001, So that would also be on utterance level, I think.: 0.9593750000000001, I think.: 0.34375, Yes, sure.: 0.075, Yes.: 0.009375, Yes, right.: 0.0875, So I define baseline and what it loads?: 0.034375, For example it loads all the utterances and so on, but it doesn't load um the discourse acts and for example not the and what's what else there?: 1.80625, Not the summaries.: 0.015625, It only loads those on demand.: 0.028124999999999997, Y you mean that you um basically split up th the big thing into um different summaries.: 0.7562500000000001, For example that you have a very um top-level um summary and a separate file for for each level.: 0.4875, Yes.: 0.009375, N: 0.009375, Uh no no, it's f for No: 1.04375, , you're right.: 0.2625, It's for Um: 1.0, No, I I think we would just take the segments that are already that were: 0.8374999999999999, Yeah, there's um this segments file.: 1.24375, Um you know, the X_M_L_ segments.: 0.2875, Oh.: 0.01875, That I don't know.: 0.64375, Yeah, that's um Mm-hmm.: 1.0, There there are time stamps um for, well, segments um and for th um segments is for example when when you look at the data, what is displayed in one line.: 1.0375, What when when you look at it in the hmm?: 0.090625, I think so.: 0.34375, Isn't Um for ex: 0.46875, um I I compared it with what I did for the pause um duration extraction.: 0.018750000000000003, Um and basically it's uh words that are uttered in a sequence without pauses.: 1.2656250000000002, But sometimes um however there are um short pauses in it: 0.028124999999999997, and they're indicated by square brackets pause or something in the data.: 0.5125, Um someti uh but uh the annotators decided what was one segment and what wasn't.: 0.76875, I think so.: 0.34375, but um I think for some annotations um an uttera ca utterance can have several um types.: 0.521875, For example for the dialogue acts and so on.: 0.04687500000000001, Yes,: 0.009375, but that's: 1.0, Yeah, everything that's a word has a sti time stamp.: 1.315625, That's at the end.: 1.071875, That's at the end, I think, her time.: 1.5499999999999998, Yeah, maybe.: 0.1, Didn't have a look at our meetings.: 0.61875, Uh I I think it wouldn't as it occurs: 1.109375, I mean it would be: 0.540625, it occurs in every meeting.: 0.271875, And I think it even has uh its own annotation, like digits or something.: 0.596875, So that should be really easy to cut out.: 0.19999999999999998, I'm sure.: 0.246875, Ah it's just to test the system, I think.: 1.378125, So Mm they have to read numbers from: 0.053125, Uh I didn't have a look at that.: 0.55625, Uh th yeah. ': 0.046875, Um I just um wondered, so who's uh then doing um the frequencies on on the words, because I'm: 1.384375, I think I will also um I could also make use of it um for the agreement and disagreement thing.: 0.9343750000000002, Because I um I in my outline: 0.003125, I talked about um using the um discourse acts first, and um then in the chunks of text I found looking for word patterns and so on.: 0.39062499999999994, So um I would for example need the um most freq um frequent words.: 0.63125, So if you cut off all that, I'd won't be use: 0.696875, but I need it for my chunks then.: 0.1625, I would You know?: 0.47187500000000004, but I'd uh I would like to look at the frequency of words in my um in the regions of text: 0.8343749999999999, I found out to be interesting.: 0.028124999999999997, So I wouldn't need it.: 0.884375, It it would have to be re-calculated only for my segments.: 0.41875, Huh?: 0.00625, I think it would be, you know, l as as big at as the hot-spot annotation things.: 0.96875, That's quite small,: 1.121875, yeah, that's some utterances.: 1.15, Yes.: 0.009375, So I would probably just concatenate all my um text chunks and then let's say m I will run over it.: 1.8249999999999997, Yes.: 0.009375, Yes, definitely.: 0.025, Yeah, right.: 0.078125, Ye M Um Jasmine, uh um what is um the text you're extracting uh: 0.28750000000000003, looking like then at the end?: 0.103125, Because um I I think it's actually very similar to what I did for my um speaker um uh extraction: 1.4718750000000003, and I think I would ch perhaps have to change two lines of codes to get you um for each meeting a file that says fr from um this millisecond to this millisecond there was this sequence of words.: 1.4062500000000004, So that's just changing two lines of code.: 1.11875, And it would give you that.: 0.33125, So far I extracted um the dura durations.: 0.043750000000000004, But it's from the words file.: 1.3031249999999999, So I could just um contatenate concatenate um the words instead of the durations, and: 0.315625, it should I mean Should be very straight-forward.: 0.26875, I can try to do it and send it to you.: 0.03125, Pe and you have a look at it, will it make sense for what you want.: 0.3625, Yeah, uh p: 0.021875, I mean it: 0.246875, I just let it run over all the files.: 0.1, So Yes.: 0.009375, I just ordered.: 0.021875, Uh I ordered according to the um starting times of the utterances.: 0.21562499999999998, What do you mean by diffe: 0.25, I mean t: 0.246875, I have one what I give you: 0.265625, would be one file for each meeting.: 0.8625, Yeah, not for each meeting series.: 0.334375, I didn't do that yet.: 0.478125, Yeah, one group,: 0.2375, Yeah, I mean there's one series that has just one meeting.: 2.0374999999999996, Yes.: 0.009375, Um the you you the data is of the form you have um three identification letter.: 0.1875, So B_E_D_ or B_B_D_ or something, and that's always the same group.: 1.1999999999999997, And then after that there's um a number like O_O_ one, O_O_ two.: 1.321875, So, it's a Yeah, but that's: 2.0, that's really quite easy to see because they're named.: 1.5624999999999998, Yes.: 0.009375, But I I mean as um the start uh start times um start for each meeting at zero, you could just probably just um add the um final second time to the next meeting and so on and just put it all together.: 1.5531249999999996, But then we would have to change um the information about who on which channel it was set, um to by which person it was set.: 0.45000000000000007, And that is actually stored in another X_M_L_ document.: 0.19687500000000002, Yeah, I w would then just not print out the um start and end times.: 0.5187499999999999, No, it's for every single word.: 1.20625, Or for every single utterance.: 0.2125, Yeah, that depends on what you want.: 0.140625, but I do it with Perl, it's just string manipulation.: 1.0093750000000001, So I would I mean I would just Sure.: 0.9000000000000001, No, I didn't do a sea no.: 0.46875, And you would want that all in one file for all the corpus?: 0.8406250000000001, Or For the series.: 0.128125, Yeah, I can directly put it into uh just like: 0.056249999999999994, So uh only words um per meeting series.: 0.5249999999999999, Uh-huh.: 0.00625, Yes.: 0.009375, just I will just take I would uh take over the names they have anyway.: 0.45312499999999994, Yeah, one series has the um same three starting letters.: 0.39374999999999993, So So only words and words and times.: 0.36875, Yeah, you want it ordered.: 0.15, Okay, anybody Um ord base dot times.: 0.05625, Yeah, and do you want: 0.128125, Yeah, sometimes they're contained in one another.: 0.45, So Just after th mm-hmm. ': 0.046875, Ordered.: 0.021875, Only words.: 0.16875, Um and I think um for all the corpus, it's just from: 1.4, I know from other times: 0.209375, , it's um nine megami byte to have: 1.040625, I mean should be should be similar to have the words.: 0.4375, Na um all the words together um for all the meetings.: 0.415625, That's what I'm guessing that's, you know, um: 2.3625000000000003, what I because nine mega-byte is what I got for when I said for every um utterance, this is goes from there to there and takes takes seconds.: 0.38125000000000003, Oh.: 0.01875, Yeah, I mean I'm it doing it for all of it.: 0.428125, Doesn't matter.: 0.4875, Yeah, I mean I hope it will be the same for the words.: 0.421875, It's just what I: 1.0, So so um I will probably send um just one file of the first meeting um to all those who need it so that you can have a look whether that's what you want.: 2.234375, Yeah, I mean if it's just for one meeting, it's really not too big.: 2.884375, What do we have to demonstrate?\n",
            ": 0.009375, The basic word importance is off-line as well.: 0.234375, The combined measure might not be if we want to wait what the user has typed in into the search.: 0.278125, I'm not quite so what it did you want to do it: 0.4125, , i you just wanted to assign: 0.015625, Uh I thought about words.: 0.225, but how about those words which don't carry any meaning at all, the um and uhs and something like that.: 0.8093750000000002, Because if we if we average average over over a whole utterance all the words, and there are quite unimportant words in there, but quite important words as well: 1.06875, , I think we should just disregard the the: 0.346875, But there is no I_D_ for an utterance I think.: 0.459375, It's just for individual words.: 1.24375, We for utterances as well.: 0.15, I think it's just for one word.: 1.68125, Uh I'm not quite sure: 0.35, , I have only seen that the uh the individual words have got an I_D_.: 0.309375, You always could have a look at the time stamps and then take the ones that uh belong together to form an utterance.: 0.6781249999999999, Yeah, if they are already, there's: 1.0375, it's easy: 1.0625, but it would be possible.: 0.30625, uh you said you are currently in uh implementing the idea.: 0.09062500000000001, What exactly are you computing?: 0.0125, Yeah, I w: 0.05, w: 0.05, I would need the raw text pretty soon because I have to find out um how I have to put the segments into bins.: 0.7625, No, that's not necessary.: 1.00625, Yes, I did.: 0.009375, But um I've only just got the notes.: 0.134375, I have to still have uh to order everything by the time: 0.271875, Yeah, I think it's quite easy after the: 1.509375, b: 0.034375, w: 0.05, that's what I was uh thought.: 1.05625, That you just combine them and then order the time stamps accordingly.: 0.20937499999999998, Um what I found out was that there are quite a lot of things without without s time stamps in the beginning.: 0.43125, Yeah, and uh X_M_L_ files.: 0.065625, Yeah, that's just an I_D_ or something.: 1.165625, I don't know.: 0.64375, Just numbers.: 0.0375, Yes, but what are the other things that's uh some kind of number?: 1.0875000000000001, F maybe the file number or something that is in the beginning.: 0.490625, Do you know?: 0.178125, Um I think there are quite a lot of numbers in the beginning where n there is no time stamp for the numbers.: 0.75, It's Think they say um quite a lot of numbers and before that, uh um there's this number.: 2.6875, Yeah, there i are numbers in the um the W_ tag, but there are no time stamps.: 0.25625, Yeah, in the beginning as well sometimes, I think.: 0.36875, At least I saw some.: 0.01875, But what it is it actually that numbers?: 0.12187500000000001, but there are no time stamps annotated to that.: 0.178125, It's it's quite strange.: 2.11875, And also um there are different um combinations of letters.: 0.21562499999999998, B_R_E_ and something like that.: 0.165625, Is it everything ordered are the time stamps global or uh are they local at any point?: 0.27812499999999996, Yeah, it's Rainbow.: 1.0, It's um I think it's just the dictionary in the first place.: 2.45, But Um no, I have to bin it up: 0.00625, and so I will only have counts for each each bin or something.: 0.19374999999999998, It's because um Rainbow is a text classification system.: 1.125, And I think it's not possible to have just one class.: 1.5937499999999998, That's the problem.: 1.028125, Maybe we could: 0.21250000000000002, Yeah sure, you sure, we could do that, but I don't that makes sense.: 0.7718750000000001, If we need just frequencies, maybe we should just calculate them by using Perl or something.: 0.484375, I don't know.: 0.64375, Yeah, it's quite easy to just count and s or sort them by um frequency.: 1.5062499999999999, Just using a Perl script.: 0.05, Is it too big?: 0.084375, I don't know how you how many terms you can handle in Perl.: 0.690625, Uh I can get all the raw text, but it has to be ordered still.: 0.31875, So No, it isn't.: 0.465625, Um it's in what is implemented in Rainbow: 1.003125, is information gain, and I'm not quite sure how they calculate that.: 0.46249999999999997, Uh that's what Rainbow does.: 1.0, I think you j can just get probabilities for a certain words for each document.: 0.7218749999999999, Certain: 0.009375, Um we would have to look at that.: 0.384375, Oh.: 0.01875, Yeah, that's what I thought as well,: 1.05625, that you that probably the the topic segment level is the most um informative for the words.: 0.70625, Yeah, that's the problem.: 1.028125, I don't know.: 0.64375, So shall we sit together tomorrow then as well?: 0.096875, w would it be best?: 0.353125, At the moment it's it's just lines of Mm-hmm.: 2.06875, So um you'd do you extract the words, the raw text, as well?: 0.453125, Print out.: 0.00625, So have we already extracted from all the files?: 0.109375, Did you also order Mm-hmm.: 0.1125, Uh I don't need the times, I just need the words.: 0.9156249999999999, But um Yeah, in the right order.: 0.1125, Yes.: 0.009375, Yeah, that doesn't matter too much, I think.: 0.9125, How long would it take to make the frequency counts with a Java hash table?: 0.6187499999999999, No, how long you would have to program something.: 0.5, Because it's quite easy in Perl as well: 1.165625, , it's just a line of code for counting all the words: 1.30625, , it's it's by hashes.: 2.003125, I dry-read it the last time..: 0.17187499999999997, Next week.: 0.05625, Uh mine's gonna be mostly using the off-line.: 1.36875, But the actual stuff it's doing will be on-line.: 1.25, But it won't be very um processor intensive or memory intensive, I don't think.: 1.3625, Don't think so.: 0.809375, Are we still gonna go for dumping it into a database?: 0.38437499999999997, Are we still gonna dump it into a database?: 0.3, 'Cause: 0.05, if we are, I reckon we should all read our classes out of the database.: 0.053125000000000006, It'll be so much easier.: 0.22187500000000002, Well if we're gonna dump the part of it into a database anyway, we might as well dump all the fields we want into the database, calculate everything from there.: 0.809375, Then we don't even have to worry that much about the underlying X_M_L_ representation.: 0.6531250000000001, We can just query it.: 0.0125, Well if we're gonna do that, we should try and store everything in in an X_M_L_ format as well.: 0.528125, Well we don't even need to do that, 'cause if we got our information density calculated off-line, so all we do is treat the whole lot as one massive document.: 1.615625, I mean they'll: 0.365625, it's not gonna be so big that we can't load in a information density for every utterance.: 2.1718750000000004, And we can just summarise based on that.: 0.009375000000000001, I think you can do it on-line.: 0.4375, I don't think there's really much point in doing like that when it's just gonna feed off in the end the information density measure basically.: 3.55625, And that's all calculated off-line.: 1.109375, So what you're really doing is sorting a list, is the p computationally hard part of it.: 0.4156249999999999, Well like the ideas we're calculating are information density all off-line first for every utterance in the whole corpus, right?: 0.9875, So what you do is you say: 0.11875, if you're looking at a series of meetings, you just say well our whole document comprises of all these stuck together.: 0.89375, And then all you have to do is sort them by j information density.: 0.41875, Like maybe weighted with the search terms, and then extract them.: 0.17500000000000002, I don't think it's too slow to do on-line, to be honest.: 1.909375, Well, on the utterance level I was thinking.: 0.284375, So the utterances with the highest like mean information density.: 0.55625, Well the trouble with doing it on the word level is if you want the audio to synch up, you've got no way of getting in and extracting just that word.: 0.73125, I mean it's impossible.: 1.25, For every single word?: 0.20625, Oh, okay.: 0.01875, I don't think that will do it.: 0.809375, We'll have to buffer it.: 0.125, Well the skimming's gonna use the importance.: 1.34375, But like at first it's just gonna be I_D_F_.: 1.25625, Well mostly skimming,: 0.046875, Well the nice thing about that is: 0.165625, it will automatically be in sentences.: 0.015625, Well more or less.: 0.03125, So it will make more sense, and if you get just extract words.: 0.440625, I see it.: 0.071875, But it'll need to be calculated at word level though because otherwise there won't be enough occurrences of the terms to make any meaningful sense.: 1.2312500000000002, Yeah, I reckon you can just mean it over the sentence.: 0.25625, I think we should filter them.: 0.35625, Maybe we should have like um a cut-off.: 0.11875000000000001, So it a w word only gets a value if it's above a certain threshold.: 1.2093749999999999, So anything that has less than say nought point five importance gets assigned to zero.: 0.3, Yeah, that's the other th: 1.046875, I think we'll have to buffer the audio.: 0.503125, But I don't think it will be very hard.: 0.828125, I think it would be like an hour or two's work.: 1.7687499999999998, Like just build an another f wave file essentially.: 0.2625, Yeah, I mean I bet there would be packages In memory,: 0.60625, So just like unp there's bound to be like a media wave object or something like that.: 1.190625, And just build one in memory.: 0.321875, I don't know.: 0.64375, I have no idea.: 0.025, But it must have like classes for dealing with files.: 0.08750000000000001, And if it has classes for concatenating files, you can do it in memory.: 0.1375, So Well what I think I might try and build is basically a class that you just feed it a linked list of um different wave-forms, and it will just string them all together with maybe, I don't know, tenth of a second silence in between each one or something like that.: 1.96875, Oh yeah,: 0.01875, yeah, we'll need that.: 0.24375, We also really wanna be able to search by who's speaking as well.: 1.3031249999999999, It doesn't matter, 'cause all the calculation's done off-line.: 1.65625, That's easy.: 1.0625, You just like create a new X_M_L_ document in memory.: 0.178125, I don't think it's really that much of a problem because if it's too big, what we can do is just well all the off-line stuff doesn't really matter.: 3.9625000000000004, And all we can do is just process a bit at a time.: 0.1875, Like for summarisation, say we wanted a hundred utterances in the summary, just look at the meeting, take the top one hundred utterances in each other meeting.: 1.3, If it scores higher than the ones already in the summary so far, just replace them.: 0.13125, And then you only have to process one meeting at a time.: 0.5968749999999999, Okay, so maybe we should build a b store a mean measure for the segments and meetings as well?: 0.65, And speaker.: 0.015625, Speaker and um topic segmenting we'll need as well.: 0.37187499999999996, and then it'll f preserve the order when it's displayed: 1.2125, Yeah, I think so.: 0.34375, So we should basically make our own X_M_L_ document in memory that everyone's um module changes that, rather than the underlying data.: 1.5125000000000002, _ document tied to the interface.: 0.109375, Well, you can make it in a file if you want.: 0.359375, They are utterances, aren't they?: 0.615625, The segments are utterances, aren't they?: 0.7250000000000001, Well, that's easy.: 1.0625, Well it's close enough, isn't it?: 1.490625, It may not be exact every time, but it's a so sort of size we're looking for.: 1.7156249999999997, But why don't we just write it as a new X_M_L_ file?: 0.659375, Can NITE handle just loading arbitrary uh new like attributes and stuff?: 0.221875, I mean, I would have thought they'd make it able to.: 0.8593750000000001, So why do we need to have two X_M_L_ trees in memory at once?: 0.253125, The other thing is that would mean we'd be using their parser as well, which means we wouldn't have to parse anything, which be quite nice.: 1.84375, 'Cause their parser is probably much faster than anything we've come up with anyway.: 0.56875, Yeah, I mean we can process it in chunks if it gets too big basically.: 0.4875, We can just process it all in chunks if it gets too big to load it into memory.: 0.290625, I think we probably want to store Sorry.: 0.775, I think we probably want to store um a hierarchical information density as well.: 0.89375, So like an informan mation density score for each meeting and each topic segment.: 0.4375, 'Cause otherwise we'd be recalculating the same thing over and over and over again.: 0.365625, And that will obviously make it much easier to display.: 0.296875, Well it may not for the whole meeting, but like Yeah, exactly.: 0.44062500000000004, Well, we can start off like that.: 0.065625, Well I was gonna start off: 0.284375, I've: 0.075, v got sort of half-way through implementing one that does just I_D_F_.: 0.6437499999999999, And then just I can change that to work on whatever.: 0.140625, And it should be weighted by stuff like the hot spots and um the key-words in the search and stuff like that.: 0.5218750000000001, Did he not say something about named entities?: 0.31249999999999994, So I thought he said there wasn't very many.: 0.603125, It's not T_F_I_D_F_: 1.0, , it's just inverse document frequency.: 1.13125, 'Cause it's really easy to do basically.: 1.296875, There's just like for a baseline really.: 1.1281249999999998, Well, I'm half-way through.: 0.25, It's not working yet, but it will do.: 1.05625, And then averaging it over the utterances.: 0.15312499999999998, But it's not like um related to the corpus at all.: 1.059375, It's just working on an arbitrary text file at the moment.: 1.3406249999999997, It would be useful to know how everyone's gonna store their things though.: 1.875, Well I've got like a few hours free.: 0.15625, It's the most boring task.: 1.0187499999999998, Or at least um simple versions of them.: 0.034375, So maybe we should try doing something really simple, like just displaying a whole meeting.: 0.8531249999999999, And like just being able to scroll through it or something like that.: 0.2, Are you free after this?: 0.015625, 'Cause I'm off all Friday.: 0.23125, Uh Wednesday I've got a nine 'til twelve.: 0.17812500000000003, Yeah, nothing in the afternoon.: 0.028125, I've got nothing in the afternoon.: 0.15937500000000002, So you ha: 0.009375, Uh I'll be in um the Appleton Tower anyway.: 0.16875, Um well I'll be there from twelve.: 0.13125, I've got some other stuff that needs done on Matlab, so if you're not there at twelve, I can just work on that.: 0.56875, Why w: 0.05, I'm just building a dictionary.: 0.253125, Oh, mine's just gonna use the um hash map one in um Java.: 1.5781249999999998, 'Cause I'm only gonna do it on small documents.: 0.475, It's just like bef until the information density is up and running.: 1.1593750000000003, Just something to get give me something to work with.: 0.54375, So it's only gonna use quite small documents, you see, to start with.: 1.546875, Why does it need to be classified into like different segments?: 0.365625, Can we just fill a second class with junk that we don't care about?: 0.496875, Like, I don't know, copies of Shakespeare or something.: 0.8125000000000001, 'Cause if what we're looking for is the um frequency statistics, I don't see how that would be changed by the classification.: 1.1625, Well there maybe another tool available?: 0.140625, Um I can't remember who's got it.: 1.559375, Might be WordNet.: 0.05625, But one of these big corpuses has a list of stop words that you can download and: 0.528125, they're just basically lists of really uninteresting boring words that we could filter out before we do that.: 0.68125, It's like: 1.0, that's one: 1.228125, the papers I read: 0.01875, , that's um one things they did right at the beginning: 1.3624999999999998, is they've got this big s stop-list: 0.25625000000000003, and they just ignore all of those throughout the experiment.: 0.009375000000000001, Yeah, I it would be useful for me as well.: 0.33125, uh I think that'd be useful for me as well.: 0.515625, Well all you really wanna do is look into getting some sub-set of the ICSI corpus off the DICE machines.: 0.33749999999999997, 'Cause I hate working on DICE.: 0.1, It's awful.: 1.003125, Like so I can use my home machine.: 0.115625, ha has a C_D_ burner though.: 0.05625, has a C_D_ burner.: 0.00625, The right-hand corner, far right.: 0.203125, How big is it without um the WAV files and stuff?: 0.30625, 'Cause: 0.05, I could just say at um going over S_C_P_ one night and just leave it going all night: 0.5343749999999999, It's yeah,: 1.0, I mean the wave data are obviously not gonna get off there completely.: 0.759375, Really?: 0.11875, Oh right?: 0.096875, I'll see if I can S_C_P: 0.190625, _ it, I suppose.: 0.00625, I've got a Linux box and a Windows box.: 0.146875, So Broad-band.: 0.009375000000000001, Put it on to C_D_.: 0.04375, I can if I get down I can put to C_D_.: 0.15625, I'm not sure if there's enough space.: 1.284375, Is how much do we get?: 0.19375, Really?: 0.11875, but I can do it from that session, can't I?: 0.5125, You can compress it from a remote session and S_C_P: 0.025, _ it from the same session?: 0.0125, Do you think?: 0.34375, Oh no no, I was thinking of SSHing just into some machine and then just SCPing it from there.: 0.09375, I mean it has to go through the gateway.: 0.35625, Mm, I see.: 0.071875, So you could: 0.1125, just But th first, uh how big are the chunks?: 0.20625000000000002, How big are the chunks you're looking at?: 0.3375, So quite small then.: 0.121875, So you could just um you could use just the same thing we used to build the big dictionary.: 0.6281249999999999, You just do that on-line 'cause that won't take long to build a little dictionary that big, will it.: 0.9031249999999998, I mean just use the same tool that we use.: 0.384375, It doesn't need ordered, no.: 0.6124999999999999, Um well that's the t are you using T_F_I_D_F_ for the information density?: 1.175, Like 'cause frequency would be useful, I think.: 0.778125, But um depending on the context, the size, and what we consider a document in the sense of calculating T_F_I_D_F_ is gonna change.: 0.41875, Which might need thinking about.: 0.221875, I think it would be useful,: 0.6749999999999999, Well you need the raw frequency as well.: 0.22187500000000002, But um you also need how many times things occur within each document.: 0.42812500000000003, And um what we consider a document's gonna depend on our context, I think.: 1.6687500000000002, 'Cause if we're looking at the whole lot of meetings, we'll consider each meeting a document in sort of terms of this algorithm.: 1.340625, And if we're viewing like say just a small topic segment you might look at even each utterance as a small document.: 0.921875, Yeah, but the thing is um It's gonna need some th th thought of how we Actually maybe it doesn't actually matter.: 2.4062500000000004, Maybe if you just do it once at the highest level, it it will be fine.: 0.2625, But I was just thinking it might be difficult to calculate the T_F_I_D_F_ off-line for all the different levels we might want.: 0.571875, 'Cause if we're gonna allow disjoint segments for example, then how are we gonna know what's gonna be in context at any given time?: 2.378125, But I suppose if you just did it globally, treating a meeting as a document, it'd probably still be work out fine, because you'd only be comparing to ones within the context.: 1.01875, Uh I don't know, I thought were you gonna use that in the end?: 1.053125, The information density.: 0.13437500000000002, Oh sorry, that's what I mean.: 1.29375, Like um yeah, for each word or whatever, but across the whole lot is what I mean by highest level.: 0.8343749999999999, Like across the whole corpus.: 0.275, Yeah, but you'd probably look at each meeting as a document.: 0.7437499999999999, Mm possibly.: 0.00625, Are they big enough to get anything meaningful out of?: 0.27812499999999996, it's not an issue.: 1.003125, You just concatenate an X_M_L_ file together.: 0.221875, but we still want to have like a notion of meetings for the user.: 0.25625, Yeah, sure.: 0.065625, Yeah, you just like whatever you want to look at, you just jam together into an X_M_L_ file and that's your meeting, even though bits of it may come from all over the place or whatever.: 1.8906249999999998, I mean I don't see why that's really a big problem.: 2.015625, So basically what you're saying is you can take an arbitrary amount of data and process it with the same algorithm.: 0.5625, It doesn't matter conceptually what that data is.: 0.6375, It could be a meeting.: 0.31875, it could be two utterances.: 0.32499999999999996, it could be a meeting plus half a meeting from somewhere else.: 0.58125, I don't think it's very difficult though.: 1.8624999999999998, I mean what you do is you just build an X_M_L_ file, and if you want it to get down to the utterances, you'd go to the leaves.: 1.053125, And then if you wanted the next level up, you'd go to the parents of those and like just go from like the leaves inwards towards the branch to build up things like um you know,: 0.7781250000000001, when you click on a segment, it's gonna have like words or whatever that are important.: 1.528125, As long as like the algorithms are designed um with it in mind, I don't think it's a very big problem.: 1.971875, Well like say you had um like say for a meeting,: 0.44375, right, you've got like uh say a hierarchy that looks quite big, like this.: 0.521875, And like the utterances come off of here maybe.: 0.259375, Then when whatever your algorithm is doing, as long as when you're working with utterances, you go for all the leaves, like then if you need something next up, so like a topic segment, you'd go to here.: 1.34375, But if you were looking at say this one, so only went like this.: 0.38125, Right, so you: 0.078125, it's same, you'd start with the leaves, and you go: 1.309375, oh, I want a topic segment.: 0.315625, So I go one layer up.: 0.325, See, and then if you're working with just a topic segment in there, it's the only thing you have to worry about.: 1.634375, And like each time you want a higher level, you just need to go up the tree.: 0.6281249999999998, And as long as your algorithm respects that, then we can just process any arbitrary X_M_L_ file with whatever hierarchical structure we want.: 0.49687499999999996, A meeting, say, and that would be a topic segment.: 0.7874999999999999, So I think as long as you build an algorithm that respects whatever structure's in the file, rather than imposing its own structure: 1.7437500000000001, Well no, it doesn't have to be.: 0.465625, But I mean it could be as many nodes as you want.: 0.515625, Like this one could be deeper maybe, say.: 0.5625, So then you'd start with all your utterances here, and when you go up to get topic segments, you go to here here here here here here here.: 0.865625, That might be a bit confusing though 'cause you have things on different levels.: 0.378125, So we'll see if we can get like a mini-browser just displays two things synched together of some kind.: 0.53125, It'd be useful.: 0.171875, I don't know who you see about that though.: 0.7562500000000001, I d have no idea.: 0.025, I've probably got a reasonable amount because um everything on my DICE account can actually be deleted 'cause I store it all at home as well.: 0.6375000000000002, Is that guaranteed to stay, the Maybe you should send a support form.: 0.140625, Just say we want some web space.: 0.265625, 'Cause: 0.05, that'd be really useful: 0.29062499999999997, is if we had a big directory.: 0.115625, Especially for transferring stuff.: 0.146875, Having said that, are we allowed to take a copy of the ICSI corpus?: 0.1875, Something we should probably ask before we do it..: 0.425, No, me neither.: 0.003125, Might be funny to see what is summarised the whole corpus as anyway.: 0.45625, I think it'd be very useful.: 0.515625, But We can just change the code.: 0.065625, That's quite good.: 1.140625, I could just use it with the frequency, I think, until the information density thing's finished.: 1.865625, That would be really useful.: 0.44999999999999996, If you're doing it in Java, could you um serialize the output as well as writing it to a file?: 0.453125, If you're doing it in Java, could you serialize the um dictionary,: 0.36562500000000003, yeah, as well as writing it to a file?: 0.140625, It's really easy.: 1.18125, I don't see why it'd be any more massive than the file.: 0.834375, It just saves you parsing the um file representation of it.: 0.16249999999999998, And now 'cause I would be using it in Java anyway.: 0.434375, So I'd just be building the data structure again.: 0.33125, Yeah, but it seems like a bit silly to be parsing it over and over again kinda thing.: 0.209375, I would've thought that um I think all the collections and things implement serializable already.: 0.8625000000000002, I think they might do.: 0.4, Tonight I'll try and um I'll either work some more on uh the T_F_I_D_F_ summarizer or do the audio thing.: 0.521875, Do we have to demonstrate something next week?: 0.23124999999999998, Yeah, I know.: 0.178125, I think it's 'cause we had to specify it ourselves that it's not as um like focus the specification of most um work we have to do.: 2.4718750000000003, Once we start doing it it will all become more or less obvious: 0.10312500000000001, I think anyway.\n",
            ": 0.39375, Does anyone want to see uh Steve's feedback from the specification?: 1.2375, Right.: 0.078125, Not really, um just what he's talking about, like duplication of effort and Like duplication of effort and stuff, and: 1.3031250000000003, um yeah, he was saying that we should maybe uh think about having a prototype for week six, which is next week.: 0.5625, So we should probably prioritize our packages.: 0.25, Has has anyone actually looked at the Java code for the: 0.178125, , huh? Hmm.: 0.00625, Yeah, I think so.: 0.34375, Yeah, I I don't know about the search functionality, that might be online.: 0.7500000000000001, Depends how it's gonna work.: 1.29375, Yeah, that makes sense.: 0.0625, Yeah, you just concatenate them together.: 0.0875, It just means it loads on demand.: 0.0625, It only loads when it needs a particular type of file.: 0.21875, Like when it's being accessed.: 1.003125, Yeah, I think that's the idea, it just loads the particular ones it needs.: 1.4468750000000001, But if you were doing a search over the whole corpus you'd have to load them all.: 0.5031249999999999, Yeah, we do not want it in to develop a little tree display as well for multiple results.: 0.25937499999999997, Yeah, but that'd be quite easy to do.: 0.3, You just need to find the time stamp.: 0.296875, Yeah, I think I think those segments for each utterance are split up.: 0.9249999999999999, Think so.: 0.34375, Yeah, I'm pretty sure it's already there.: 1.3, Pretty sure that's already there.: 1.1187500000000001, The the utterances are numbered.: 0.159375, Yeah, I think so.: 0.34375, Ye that's the impression I get,: 1.1187500000000001, Oh.: 0.01875, Yeah, uh Right.: 0.078125, Topics,: 0.04375, Yeah, I think that's the right one.: 1.65, Yeah, that'd be much more efficient to do that.: 0.21875, Yeah, you're able to do that in Java,: 0.215625, Huh.: 0.00625, Yeah, I've had a b I've had a look at the the topic segments, how it's stored.: 1.503125, And then yeah, th those are few per meeting, and it um: 0.275, well, it gives a time stamp and inside each one there's uh the actual like utterance segments.: 1.63125, And the list of them that occurred.: 0.037500000000000006, And they're all numbered.: 0.19375, Um so that's where that's stored.: 2.0125, Yeah, so I guess um if I'm gonna be segmenting it with a L_C_ seg: 0.46249999999999997, then that's like same format: 1.0125, I'd want to um put it back out in so it'd be equivalent.: 0.45625, Well, like the integration.: 0.0125, What do you mean, integration?: 0.259375, I don't know.: 0.64375, I don't think anyone's been allocated to do that yet.: 1.853125, Yeah, definitely.: 0.015625, Yeah, it c could be difficult,: 0.153125, Well I guess the important thing is to get the crucial m modules built.: 0.34687499999999993, and then we'll maybe have to prioritize somebody into just integrating it.: 0.253125, Yeah, I think so.: 0.34375, Jasmine, I thought you just said that you'd uh looked at extracting the text.: 0.37500000000000006, So you you said you did it in Python,: 0.05625, Yeah, did you use uh b the X_L_ uh X_M_L_ parser in Python?: 0.10625, Right.: 0.078125, Yeah, sounds pretty good.: 0.06875, So um 'cause, yeah, I was having a look in it a look at it as well: 0.23125, and I noticed the um the speakers are all in that separate file?: 0.16875, So did did you have to combine them all and and then re-order them?: 0.040625, c: 0.028125, Right.: 0.078125, Yeah, so that's approach um: 1.003125, well, I was going to do.: 0.028125, So yeah, we may as well collaborate.: 0.015625, In the word files?: 0.175, I'm not sure I what you mean.: 0.49375, Oh right.: 0.096875, Mm I thought they were local to th a particular meeting.: 0.334375, Mm is there anything else we should discuss?: 0.0875, Yeah, should we not have like a group directory or something where we can put all our code in and that kinda thing?: 0.453125, I've gotten mm hardly any Hmm.: 0.08125, Yeah, we can ask Steve if um we can get space.: 0.140625, Yeah, uh we could do that.: 0.1125, Yeah, I'm sure he had to deal with that last year.: 0.26875000000000004, That sounds good.: 0.053125, Yeah, that's what I'm gonna need.: 1.525, Yes.: 0.009375, Yeah, it's just mo changing it a bit.: 1.034375, but uh that's what M_L_C_ seg does.: 1.00625, it marks the end of each segment.: 0.140625, Oh.: 0.01875, Yeah, for me it's better if they're by meeting.: 1.403125, Then that'll be really easy to do once they've got the raw text.: 0.565625, It's just a case of running the script.: 1.0375, Yeah, I mean hopefully this week.: 0.275, And we could Don't know.: 0.75625, Suppose we're just getting on with all our components.: 0.22187500000000002, So I know.: 0.178125, Wa: 0.00625, Yeah, he suggested that we could have an uh initial prototype.: 0.1375, I know, I'd b: 0.346875, I'd be surprised if we can get anything working by next week.: 0.403125}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7hxcNtTanVo"
      },
      "source": [
        "select_length = int(len(sentence_tokens) * 0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0k63z6vanVo",
        "outputId": "8ddd5c30-7b31-4649-9cb9-72699ec7b85e"
      },
      "source": [
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
        "summary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[What I'm like what we have to think about is if we go with this multi-level idea, like this idea that sort of if you start with a whole meeting series as one entity, as one thing that you display, as one whole sort of, that then the individual chunks of the individual meetings, whereas and then you can click on a meeting, and then sort of the meeting is the whole thing and the chunks are the individual segments, that means sort of we have multiple levels of of representation, which we probably If we if we do it this way like we f we have to discuss that if we do it this way, then we should probably find some abstraction model, so that the interface in the sense like deals with it as if it's same so that the interface doesn't really have to worry whether it's a meeting in the whole meeting series or a segment within a meeting,\n",
              " And for all together, like the display itself, I think we are easier if we if it's sitting on the X_M_L_ than if it's sitting on the S_Q_L_ stuff, because if it's sitting on the X_M_L_, we have the the NITE X_M_L_ framework with all its functionality for synchronizing through all the different levels whenever there's a change, whenever something's moving forward and stuff.,\n",
              " Yeah, I think today's meeting is really the one where we where we sort of settle down the data structure and as soon as we have that, uh probably like after today's meeting, we then actually need to well go back first of all and look at NITE X_M_L_ to see in how far that that which we want is compatible with that which NITE X_M_L_ offers us.,\n",
              " I don't think it's really that much of a problem because if it's too big, what we can do is just well all the off-line stuff doesn't really matter.,\n",
              " I don't think there's really much point in doing like that when it's just gonna feed off in the end the information density measure basically.,\n",
              " So it it seems that the data structure isn't a big problem and that basically we don't have to have all these massive discussions of how we exactly interact with the data structure because most of our work isn't done with that data structure in memory in the browser, but it's just done off-line and everyone can ha represent it anyway,\n",
              " You know what I mean, like if if there's a lot of bureaucracy involved with having two different trees and whether one ties to the other because the one has the weight for the other, then it's probably quicker to,\n",
              " because like the the ICSI corpus isn't isn't one meeting series, it's several meeting series with different people meeting for completely different things.,\n",
              " And for in the display it's probably better if you have whole utterances than I don't know, like what it's like,\n",
              " But I was thinking of the topic segmentation now and and f for that there would only be one, right, because it's sort of like it's just a time window.,\n",
              " So how much do they display, and starting position at where the or maybe the mid-position of it, I don't know, like w if start where the thing was found or if that thing wa was found it's in the middle of the part that we display, that I don't know.,\n",
              " Like if there's some name of some conference, they would could probably say that name of the conference quite often, even though he's right that they make indirect references to it.,\n",
              " like even if we maybe this whole like maybe I'm worrying too much about the whole series in one thing display, because actually I mean probably users wouldn't view that one too often.,\n",
              " Like my experience with the British National Corpus was that there's far more word types than you ever think because anything that's sort of unusual generally is a new word type.,\n",
              " , you know what I mean, like sort of sort of for the whole series chunks representing the individual meetings or some Like something that represents the whole series in in a v in a structure very similar to the structure in which we represent individual um meetings, but with data sort of always combined from the whole series.,\n",
              " So somebody should sort of be the one person who's who understands most about what's t centrally going on with with the with the project, like with the with the browser as a whole and where the data comes in and Any volunteers?,\n",
              " And in that way we could probably use the same algorithm and just like make vir like one or two ifs that say okay, if you are on a whole document uh a whole series level and that was a double-click, then don't just go into that um segment, but load a new file or something like it, but in general use the same algorithm.,\n",
              " it's sort of like one person's contribution at a time sort of thingy dingy.,\n",
              " probably it wouldn't be the best thing if it's just the high most highly ranked phrase or key-word because like for example for an introduction that would most definitely not be anything that has any title anywhere similar to introduction or something.,\n",
              " Yeah, I mean if it's just for one meeting, it's really not too big.,\n",
              " So but that means in the general structure we're actually quite lucky, so we we have we load into memory for the whole series of meetings just the utterances and rankings for the utterances and some information probably that says, well, the I guess that goes with the utterance, who's speaking.,\n",
              " I don't think that's a good idea because isn't it like that we expect th there to change over i b with the different topic segments more?,\n",
              " Well but also about the displays, I mean the displays in the in the text body, in the in the latest draft that we had sort of we came up with the idea that it isn't displaying utterance for utterance, but it's also displaying uh a summarised version in you know, like below the below the graph, the part.,\n",
              " Also like for this part, maybe if we go over it with named entity in the end, if I mean w if one of the people doing DIL has some named entity code to spare, and just like at least for the for sort of for finding topics, titles for for segments, just take a named entity which has a really high, what's it called,,\n",
              " I'm just thinking for this whole thing of like a different level, sort of cutting out different different pieces, whether we do that through a query where we say give us everything that's ab above this,\n",
              " It's Think they say um quite a lot of numbers and before that, uh um there's this number.,\n",
              " But that we can decide about, but a general sort of It's the same thing if like whether you play and it moves forward or whether you jump to a position through search, it's essentially for all the window handling,\n",
              " even if you go on an per utterance level, there's still some merit on within utterances cutting out stuff which clearly isn't relevant at all, and that maybe also for the audio we'd have to do.,\n",
              " if it i let's say I mean if if their query language is strange and if it would return b ten million results and it can't handle it, then we can just write our individual components in the way that they know which what the threshold is.,\n",
              " Yeah, now I'd say if for the prototype if we just like wherever possible p chunk in the stuff that we have um pre-annotated and stuff, and for the stuff that we don't have pre-annotated write like a stupid baseline, then we should probably be able to basically that means we focus on on the interface first sort of, so that we we take the the ready-made parts and just see how we get them work together in the interface the way we want,\n",
              " the only thing that would happen basically if I double-click let's say from the whole meeting series on a single meeting, is that the zoom level changes.,\n",
              " It more and more appears to me that if we if we scrap the notion of the meeting as an individual thing and sort of ju see meetings as as topic segments and have sort of like hierarchical topic segmentation instead, then it's b like a more coherent framework.,\n",
              " Sort of c I was just thinking you know like if if the overhead for having the same amount of data coming from two d files instead of from one file is massive then it would probably be for us easy to just like off-line put the the weight into into the file that has the segments, uh yeah, segments slash utterances already.,\n",
              " Well you can probably also you can probably pre-filter like with regular expressions even just say if it consists of only dig digits, then skip it, or even if it consists any special characters, then skip it because it's probably something with a dot in between, which is usually not something you wanna have,\n",
              " Because it's it's less stuff to store probably for Dave in the in the audio playing.,\n",
              " I think it's 'cause we had to specify it ourselves that it's not as um like focus the specification of most um work we have to do.,\n",
              " and and the interface, so the interface is mainly while it's running just working on data that's just loaded from a file, I guess.,\n",
              " It's um I think it's just the dictionary in the first place.,\n",
              " Yeah, but the thing is um It's gonna need some th th thought of how we Actually maybe it doesn't actually matter.,\n",
              " 'Cause before we have that, it's gonna be very difficult for anyone to really see how much the work that they're doing is making sense because you just well I guess you can see something from the data that you have in your individual,\n",
              " 'Cause if we're gonna allow disjoint segments for example, then how are we gonna know what's gonna be in context at any given time?,\n",
              " Um but that's still sort of that's good.,\n",
              " That's what I'm guessing that's, you know, um,\n",
              " Who's who's sort of doing the the the central coordination of of of the browser application now?,\n",
              " But I'm I'm still confused 'cause I thought like that's just what Jonathan said we do c that we can't do, like load a massive document of that size.,\n",
              " Like if if if the same thing is in different files, would it then maybe like, you know, if if utterances are split over three or ten or w hundred different files, is then a chance maybe that it doesn't try to load them all into memory at the same time, but just So why does it fail then in the first place?,\n",
              " the building the browser comes first, and then only comes the creating new sophisticated data chunks, because that's sort of the whole thing about having a prototype system which is more or less working on on chunk data.,\n",
              " I think we are running queries, it's not just about um what we load and what we don't load, but we're l running queries in the sense that we dynamically select by by weights,\n",
              " I have always thought it's like more that oh, whatever, I'm a can't think of it at the moment.,\n",
              " So so um I will probably send um just one file of the first meeting um to all those who need it so that you can have a look whether that's what you want.,\n",
              " So if you find that the player accepts some n input from memory, and if it's easy to do, then I guess that's,\n",
              " I think he's talking about sort of the ones that Yeah, if you if you enter the big room, in the right-hand corner, I think.,\n",
              " So at at every level everyone has to be careful to really just take even at the highest level, just take stuff from one series and not merge stuff from different series together because they would probably be just majorly messy.,\n",
              " And I mean if you if you know how to do it for individual words, then you can just in the worst case, if you can't find anything else, just sort of make the mean of the words over the utterance.,\n",
              " it's not gonna be so big that we can't load in a information density for every utterance.,\n",
              " It's it's quite strange.,\n",
              " Yeah, sort of like off-line create a virtual meeting, which which basically treats the meeting series as if it was a meeting, and treats the individual meetings within the series as if they were segments, and treats the individual segments within meetings as if they were um utterances.,\n",
              " Who actually like for this whole discussion I mean, who of us is doing stuff that is happening on-line and who of us is doing stuff that's happening off-line?,\n",
              " If maybe if we realise that there's massive differences in in gain or in something, you can probably just make some simple simple normalization, but that really depends on how much time we have and and how much is necessary.,\n",
              " Yo, Forrest Hill, whatever one's easier to discuss stuff, I don't know.,\n",
              " So I'm just wondering if there's ways to abandon the whole concept of of meetings and sort of but just not really treating separate meetings as too much of a separate entity.,\n",
              " Oh, so that's what f Rainbow does, because that's what L_S_A_ builds on.,\n",
              " At the moment it's it's just lines of Mm-hmm.,\n",
              " What's what's nine megabyte?,\n",
              " Yeah, I mean there's one series that has just one meeting.,\n",
              " and and a topic's basically they are just on the I_D_, probably with a start time or something, and and the utterances referenced to those topics I guess.,\n",
              " I mean I don't see why that's really a big problem.,\n",
              " And even though probably if there's a lot of over-head in having two different files, we can probably merge the weights into it off-line.,\n",
              " Um so that's where that's stored.,\n",
              " Like it's it's the sa,\n",
              " There's a there's a serialise command,\n",
              " , it's it's by hashes.,\n",
              " So, it's a Yeah, but that's,\n",
              " Um but that that's enough data for the skimming and the the searching, so what the searching does is the searching leaves the whole framework, goes to the S_Q_L_ database and gets like basically in the end gets just a time marker for where that is, like that utterance that we are concerned with.,\n",
              " As long as like the algorithms are designed um with it in mind, I don't think it's a very big problem.,\n",
              " I mean if you think if it's r roughly a million words and nine characters per word sounds realisti,\n",
              " So Well what I think I might try and build is basically a class that you just feed it a linked list of um different wave-forms, and it will just string them all together with maybe, I don't know, tenth of a second silence in between each one or something like that.,\n",
              " It would probably find another number task quite easily seeing that it's a constrained vocabulary with a high co-occurrence of the same nine words.,\n",
              " so like if even if we make one single text file which has the whole corpus, sort of our corpus, that would still be from one series only.,\n",
              " No no, not demonstrate, but like didn't you say that uh didn't we sort of agree that it would be useful to have a demonstrator of it, like some primitive thing working next week.,\n",
              " I don't think it's too slow to do on-line, to be honest.,\n",
              " So alternatively, if we realise we can't do the whole thing in one go, we can probably just process some sort of meta-data,\n",
              " Yeah, you just like whatever you want to look at, you just jam together into an X_M_L_ file and that's your meeting, even though bits of it may come from all over the place or whatever.,\n",
              " It would be useful to know how everyone's gonna store their things though.,\n",
              " I could just use it with the frequency, I think, until the information density thing's finished.,\n",
              " , it's probably just those people have to work together a lot and very closely and just make sure that they're always f,\n",
              " I don't think it's very difficult though.,\n",
              " I think before we can like answer that specific question how we c deal with that, it's probably good for us to look at what the audio player is capable of doing.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYYE-cn1anVo",
        "outputId": "215859b7-7aaa-4029-de4e-b9eb080b5811"
      },
      "source": [
        "summary_luhn =[]\n",
        "for i in summary:\n",
        "    summary_luhn.append(str(i))\n",
        "final_luhn = ' '.join(summary_luhn)\n",
        "final_luhn "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"What I'm like what we have to think about is if we go with this multi-level idea, like this idea that sort of if you start with a whole meeting series as one entity, as one thing that you display, as one whole sort of, that then the individual chunks of the individual meetings, whereas and then you can click on a meeting, and then sort of the meeting is the whole thing and the chunks are the individual segments, that means sort of we have multiple levels of of representation, which we probably If we if we do it this way like we f we have to discuss that if we do it this way, then we should probably find some abstraction model, so that the interface in the sense like deals with it as if it's same so that the interface doesn't really have to worry whether it's a meeting in the whole meeting series or a segment within a meeting And for all together, like the display itself, I think we are easier if we if it's sitting on the X_M_L_ than if it's sitting on the S_Q_L_ stuff, because if it's sitting on the X_M_L_, we have the the NITE X_M_L_ framework with all its functionality for synchronizing through all the different levels whenever there's a change, whenever something's moving forward and stuff. Yeah, I think today's meeting is really the one where we where we sort of settle down the data structure and as soon as we have that, uh probably like after today's meeting, we then actually need to well go back first of all and look at NITE X_M_L_ to see in how far that that which we want is compatible with that which NITE X_M_L_ offers us. I don't think it's really that much of a problem because if it's too big, what we can do is just well all the off-line stuff doesn't really matter. I don't think there's really much point in doing like that when it's just gonna feed off in the end the information density measure basically. So it it seems that the data structure isn't a big problem and that basically we don't have to have all these massive discussions of how we exactly interact with the data structure because most of our work isn't done with that data structure in memory in the browser, but it's just done off-line and everyone can ha represent it anyway You know what I mean, like if if there's a lot of bureaucracy involved with having two different trees and whether one ties to the other because the one has the weight for the other, then it's probably quicker to because like the the ICSI corpus isn't isn't one meeting series, it's several meeting series with different people meeting for completely different things. And for in the display it's probably better if you have whole utterances than I don't know, like what it's like But I was thinking of the topic segmentation now and and f for that there would only be one, right, because it's sort of like it's just a time window. So how much do they display, and starting position at where the or maybe the mid-position of it, I don't know, like w if start where the thing was found or if that thing wa was found it's in the middle of the part that we display, that I don't know. Like if there's some name of some conference, they would could probably say that name of the conference quite often, even though he's right that they make indirect references to it. like even if we maybe this whole like maybe I'm worrying too much about the whole series in one thing display, because actually I mean probably users wouldn't view that one too often. Like my experience with the British National Corpus was that there's far more word types than you ever think because anything that's sort of unusual generally is a new word type. , you know what I mean, like sort of sort of for the whole series chunks representing the individual meetings or some Like something that represents the whole series in in a v in a structure very similar to the structure in which we represent individual um meetings, but with data sort of always combined from the whole series. So somebody should sort of be the one person who's who understands most about what's t centrally going on with with the with the project, like with the with the browser as a whole and where the data comes in and Any volunteers? And in that way we could probably use the same algorithm and just like make vir like one or two ifs that say okay, if you are on a whole document uh a whole series level and that was a double-click, then don't just go into that um segment, but load a new file or something like it, but in general use the same algorithm. it's sort of like one person's contribution at a time sort of thingy dingy. probably it wouldn't be the best thing if it's just the high most highly ranked phrase or key-word because like for example for an introduction that would most definitely not be anything that has any title anywhere similar to introduction or something. Yeah, I mean if it's just for one meeting, it's really not too big. So but that means in the general structure we're actually quite lucky, so we we have we load into memory for the whole series of meetings just the utterances and rankings for the utterances and some information probably that says, well, the I guess that goes with the utterance, who's speaking. I don't think that's a good idea because isn't it like that we expect th there to change over i b with the different topic segments more? Well but also about the displays, I mean the displays in the in the text body, in the in the latest draft that we had sort of we came up with the idea that it isn't displaying utterance for utterance, but it's also displaying uh a summarised version in you know, like below the below the graph, the part. Also like for this part, maybe if we go over it with named entity in the end, if I mean w if one of the people doing DIL has some named entity code to spare, and just like at least for the for sort of for finding topics, titles for for segments, just take a named entity which has a really high, what's it called, I'm just thinking for this whole thing of like a different level, sort of cutting out different different pieces, whether we do that through a query where we say give us everything that's ab above this It's Think they say um quite a lot of numbers and before that, uh um there's this number. But that we can decide about, but a general sort of It's the same thing if like whether you play and it moves forward or whether you jump to a position through search, it's essentially for all the window handling even if you go on an per utterance level, there's still some merit on within utterances cutting out stuff which clearly isn't relevant at all, and that maybe also for the audio we'd have to do. if it i let's say I mean if if their query language is strange and if it would return b ten million results and it can't handle it, then we can just write our individual components in the way that they know which what the threshold is. Yeah, now I'd say if for the prototype if we just like wherever possible p chunk in the stuff that we have um pre-annotated and stuff, and for the stuff that we don't have pre-annotated write like a stupid baseline, then we should probably be able to basically that means we focus on on the interface first sort of, so that we we take the the ready-made parts and just see how we get them work together in the interface the way we want the only thing that would happen basically if I double-click let's say from the whole meeting series on a single meeting, is that the zoom level changes. It more and more appears to me that if we if we scrap the notion of the meeting as an individual thing and sort of ju see meetings as as topic segments and have sort of like hierarchical topic segmentation instead, then it's b like a more coherent framework. Sort of c I was just thinking you know like if if the overhead for having the same amount of data coming from two d files instead of from one file is massive then it would probably be for us easy to just like off-line put the the weight into into the file that has the segments, uh yeah, segments slash utterances already. Well you can probably also you can probably pre-filter like with regular expressions even just say if it consists of only dig digits, then skip it, or even if it consists any special characters, then skip it because it's probably something with a dot in between, which is usually not something you wanna have Because it's it's less stuff to store probably for Dave in the in the audio playing. I think it's 'cause we had to specify it ourselves that it's not as um like focus the specification of most um work we have to do. and and the interface, so the interface is mainly while it's running just working on data that's just loaded from a file, I guess. It's um I think it's just the dictionary in the first place. Yeah, but the thing is um It's gonna need some th th thought of how we Actually maybe it doesn't actually matter. 'Cause before we have that, it's gonna be very difficult for anyone to really see how much the work that they're doing is making sense because you just well I guess you can see something from the data that you have in your individual 'Cause if we're gonna allow disjoint segments for example, then how are we gonna know what's gonna be in context at any given time? Um but that's still sort of that's good. That's what I'm guessing that's, you know, um Who's who's sort of doing the the the central coordination of of of the browser application now? But I'm I'm still confused 'cause I thought like that's just what Jonathan said we do c that we can't do, like load a massive document of that size. Like if if if the same thing is in different files, would it then maybe like, you know, if if utterances are split over three or ten or w hundred different files, is then a chance maybe that it doesn't try to load them all into memory at the same time, but just So why does it fail then in the first place? the building the browser comes first, and then only comes the creating new sophisticated data chunks, because that's sort of the whole thing about having a prototype system which is more or less working on on chunk data. I think we are running queries, it's not just about um what we load and what we don't load, but we're l running queries in the sense that we dynamically select by by weights I have always thought it's like more that oh, whatever, I'm a can't think of it at the moment. So so um I will probably send um just one file of the first meeting um to all those who need it so that you can have a look whether that's what you want. So if you find that the player accepts some n input from memory, and if it's easy to do, then I guess that's I think he's talking about sort of the ones that Yeah, if you if you enter the big room, in the right-hand corner, I think. So at at every level everyone has to be careful to really just take even at the highest level, just take stuff from one series and not merge stuff from different series together because they would probably be just majorly messy. And I mean if you if you know how to do it for individual words, then you can just in the worst case, if you can't find anything else, just sort of make the mean of the words over the utterance. it's not gonna be so big that we can't load in a information density for every utterance. It's it's quite strange. Yeah, sort of like off-line create a virtual meeting, which which basically treats the meeting series as if it was a meeting, and treats the individual meetings within the series as if they were segments, and treats the individual segments within meetings as if they were um utterances. Who actually like for this whole discussion I mean, who of us is doing stuff that is happening on-line and who of us is doing stuff that's happening off-line? If maybe if we realise that there's massive differences in in gain or in something, you can probably just make some simple simple normalization, but that really depends on how much time we have and and how much is necessary. Yo, Forrest Hill, whatever one's easier to discuss stuff, I don't know. So I'm just wondering if there's ways to abandon the whole concept of of meetings and sort of but just not really treating separate meetings as too much of a separate entity. Oh, so that's what f Rainbow does, because that's what L_S_A_ builds on. At the moment it's it's just lines of Mm-hmm. What's what's nine megabyte? Yeah, I mean there's one series that has just one meeting. and and a topic's basically they are just on the I_D_, probably with a start time or something, and and the utterances referenced to those topics I guess. I mean I don't see why that's really a big problem. And even though probably if there's a lot of over-head in having two different files, we can probably merge the weights into it off-line. Um so that's where that's stored. Like it's it's the sa There's a there's a serialise command , it's it's by hashes. So, it's a Yeah, but that's Um but that that's enough data for the skimming and the the searching, so what the searching does is the searching leaves the whole framework, goes to the S_Q_L_ database and gets like basically in the end gets just a time marker for where that is, like that utterance that we are concerned with. As long as like the algorithms are designed um with it in mind, I don't think it's a very big problem. I mean if you think if it's r roughly a million words and nine characters per word sounds realisti So Well what I think I might try and build is basically a class that you just feed it a linked list of um different wave-forms, and it will just string them all together with maybe, I don't know, tenth of a second silence in between each one or something like that. It would probably find another number task quite easily seeing that it's a constrained vocabulary with a high co-occurrence of the same nine words. so like if even if we make one single text file which has the whole corpus, sort of our corpus, that would still be from one series only. No no, not demonstrate, but like didn't you say that uh didn't we sort of agree that it would be useful to have a demonstrator of it, like some primitive thing working next week. I don't think it's too slow to do on-line, to be honest. So alternatively, if we realise we can't do the whole thing in one go, we can probably just process some sort of meta-data Yeah, you just like whatever you want to look at, you just jam together into an X_M_L_ file and that's your meeting, even though bits of it may come from all over the place or whatever. It would be useful to know how everyone's gonna store their things though. I could just use it with the frequency, I think, until the information density thing's finished. , it's probably just those people have to work together a lot and very closely and just make sure that they're always f I don't think it's very difficult though. I think before we can like answer that specific question how we c deal with that, it's probably good for us to look at what the audio player is capable of doing.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgwWyl17fJwn"
      },
      "source": [
        "# соберем все в одну функцию \n",
        "def summary_luhn(filename):\n",
        "    test = open(filename, \"r\", errors = 'ignore')\n",
        "    transcript_sum = test.read()\n",
        "    stopwords = list(STOP_WORDS)\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(transcript_sum)\n",
        "    tokens = [token.text for token in doc]\n",
        "    word_freq = {}\n",
        "    for word in doc:\n",
        "        if word.text.lower() not in stopwords_english:\n",
        "            if word.text.lower() not in punctuation:\n",
        "                if word.text not in word_freq.keys():\n",
        "                    word_freq[word.text] = 1\n",
        "                else:\n",
        "                    word_freq[word.text] += 1\n",
        "    max_freq = max(word_freq.values())\n",
        "    for word in word_freq.keys():\n",
        "        word_freq[word] = word_freq[word]/max_freq\n",
        "    sentence_tokens = [sent for sent in doc.sents]\n",
        "    sentence_scores = {}\n",
        "    for sent in sentence_tokens:\n",
        "        for word in sent:\n",
        "            if word.text.lower() in word_freq.keys():\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent] = word_freq[word.text.lower()]\n",
        "                else:\n",
        "                    sentence_scores[sent] += word_freq[word.text.lower()]\n",
        "    select_length = int(len(sentence_tokens) * 0.05)\n",
        "    summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
        "    summary_luhn =[]\n",
        "    for i in summary:\n",
        "        summary_luhn.append(str(i))\n",
        "    final_luhn = ' '.join(summary_luhn)\n",
        "    return final_luhn \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZciyOjbfJwn"
      },
      "source": [
        "final_luhn = summary_luhn('ami-transcripts/ES2002a.transcript.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJNU6sucfJwo",
        "outputId": "395c9c9d-2cb6-4e5f-e1a4-cfad855541fb"
      },
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(final_luhn, transcript_sum)\n",
        "print('Luhn scores: {}'.format(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Luhn scores: [{'rouge-1': {'f': 0.041492771514736286, 'p': 0.7813211845102506, 'r': 0.021312290294519698}, 'rouge-2': {'f': 0.016211964882469444, 'p': 0.3059360730593607, 'r': 0.008326601628037035}, 'rouge-l': {'f': 0.14112149348982883, 'p': 0.6894977168949772, 'r': 0.07860489328474753}}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL9SYtSGfJwo"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import numpy as np \n",
        "def preprocess_text_simple(text):\n",
        "    # 0>\n",
        "    text = text.lower()\n",
        "    # 1\n",
        "    tokens = word_tokenize(text)\n",
        "    # 2 и 3 \n",
        "    tokens = [token for token in tokens if token not in stopwords_english\n",
        "              and token.isalpha()\n",
        "              and token.strip() not in punctuation\n",
        "              #and token.strip() not in numbers\n",
        "             ]\n",
        "    # 4\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNkdwBA8fJwo"
      },
      "source": [
        "scores1f = []\n",
        "scores1p = []\n",
        "scores1r = []\n",
        "scores2f = []\n",
        "scores2p = []\n",
        "scores2r = []\n",
        "scoresllf = []\n",
        "scoresllp = []\n",
        "scoresllr = []\n",
        "bleu1 = []\n",
        "bleu2 = []\n",
        "bleu3 = []\n",
        "bleu_together = []\n",
        "for i in range(len(ext_sum_trans)):\n",
        "    fin_luhn = summary_luhn(ext_sum_trans[i])\n",
        "    test_ = open(ext_summaries_files[i], \"r\", errors = 'ignore')\n",
        "    transcript_ = test_.read()\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(fin_luhn, transcript_)\n",
        "    scores1f.append(scores[0]['rouge-1']['f'])\n",
        "    scores1p.append(scores[0]['rouge-1']['p'])\n",
        "    scores1r.append(scores[0]['rouge-1']['r'])\n",
        "    scores2f.append(scores[0]['rouge-2']['f'])\n",
        "    scores2p.append(scores[0]['rouge-2']['p'])\n",
        "    scores2r.append(scores[0]['rouge-2']['r'])\n",
        "    scoresllf.append(scores[0]['rouge-l']['f'])\n",
        "    scoresllp.append(scores[0]['rouge-l']['p'])\n",
        "    scoresllr.append(scores[0]['rouge-l']['r'])  \n",
        "    # bleu \n",
        "    reference = preprocess_text_simple(transcript_)\n",
        "    candidate = preprocess_text_simple(fin_luhn)\n",
        "    score_together = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    bleu_together.append(score_together)\n",
        "    bleu_1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "    bleu1.append(bleu_1)\n",
        "    bleu_2 = sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))\n",
        "    bleu2.append(bleu_2)\n",
        "    bleu_3 = sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))\n",
        "    bleu3.append(bleu_3) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auDglGM7fJwp",
        "outputId": "2b105a98-b33e-4586-8377-770152bd7a5a"
      },
      "source": [
        "print('bleu-1 : {}, bleu: {}'.format(np.mean(np.array(bleu1)), np.mean(np.array(bleu_together))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bleu-1 : 0.008351922362028185, bleu: 2.6142116916912516e-157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwpelqwBfJwp",
        "outputId": "320bf96b-e03b-45fa-c764-d441f3fdd54c"
      },
      "source": [
        "print('Rouge-1 : r: {}, p: {}, f:{}'.format(np.mean(np.array(scores1f)), np.mean(np.array(scores1p)), np.mean(np.array(scores1r))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rouge-1 : r: 0.4646528647845984, p: 0.7892145451881657, f:0.34220256504333546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSJ6ZiMPfJwp",
        "outputId": "c8595702-1687-46e0-f24b-c1b673b4b5f0"
      },
      "source": [
        "print('Rouge-2 : r: {}, p: {}, f:{}'.format(np.mean(np.array(scores2f)), np.mean(np.array(scores2p)), np.mean(np.array(scores2r))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rouge-2 : r: 0.27887126341179275, p: 0.488309751473352, f:0.202651536160877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVllcs6NfJwq",
        "outputId": "54b50e90-1eac-4db2-d6c5-407c0e57f1d1"
      },
      "source": [
        "print('Rouge-l : r: {}, p: {}, f:{}'.format(np.mean(np.array(scoresllf)), np.mean(np.array(scoresllp)), np.mean(np.array(scoresllr))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rouge-l : r: 0.4813876925896682, p: 0.655863855618521, f:0.3878440594376704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pKrZ5i6anVq"
      },
      "source": [
        "**Возможные улучшения**: \n",
        "1. Заметим, что саммари, которое мы получили, не связано и не описывает основную суть фильма. В нем содержится много лишних слов, таких как stuff, goddamn и другое. Эти слова не несут в себе смысла, отражающего содеражние фильма, вероятно, чтобы улучшить данный метод,надо удалять слова, которые встречаются в нашем тексте чаще всего, чтобы их вес не мешал нам составлять саммари. \n",
        "2. Так как у нас ведется диалог между людьми, то в саммари должна происходить некая замена прямой речи на косвенную речь, но тогда нужно подумать, как быть с тем, что у нас нет информации о том, кто что говорит. \n",
        "3. Чтобы избежать \"я\" нужно не брать их во внимания, наша модель должна запоминать основные факты слова и в идеале дополнять предложения, но это уже abstractive модели. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBUUUDxNanVq"
      },
      "source": [
        "### TextRank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsNwvwJJanVq"
      },
      "source": [
        "1. Сплитим текст о предложениям  \n",
        "2. Считаем похожесть предложений между собой  \n",
        "3. Строим граф предложений с взвешенными ребрами  \n",
        "4. С помощью PageRank ищем наиболее важные предложенмия "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDSMIoMKanVq"
      },
      "source": [
        "Как работает алгоритм PageRank в нашем случае?  \n",
        "У нас есть similarity матрица, к ней нужно применить PageRank. \n",
        "Принято считать, что на нулевой (0) итерации PageRank каждой страницы одинаковый и равен 1 / N. Далее мы применяем ниже написанную формулу, получаем следующий вектор, который показывает веса каждой вершины нашего графа, затем повторяем эту операцию до тех пор, пока мы не достигнем состояния convergence. На следующих итерациях используется вес всех входящих ссылок, который представляет собой вес с предыдущей итерации делённый на количество исходящих ссылок (в формуле – L).\n",
        "PR (A) = (1 - d) / N + d * (PR(B) / L(B) + PR(C) / L(C) + ...)\n",
        "N – общее количество вершин;\n",
        "d – коэффициент затухания (обычно используется значение 0,85);\n",
        "L – количество исходящих ребер."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpNb1U8YanVr"
      },
      "source": [
        "http://dnevniknauki.ru/images/publications/2020/5/technics/Gritsenko.pdf  \n",
        "https://arxiv.org/pdf/1602.03606.pdf  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSv4JJcWanVr"
      },
      "source": [
        "TextRank это аналогия PageRank, но для предложений.  \n",
        "Предложение - node графа. \n",
        "Почти такой же алгоритм, как и LexRank, был изобретен в то же время, но есть маленькие отличия, например, LexRank используется больше для multi-document classification, а TextRank наоборот, для single-document classification + различаются в similarity matrix.  \n",
        "Он заключается в следующем:  \n",
        "По тексту строится взвешенный неориентированный граф, вершины в котором обозначают предложения текста. Весом ребра между двумя\n",
        "вершинами является степень схожести двух предложений, соответствующих вершинам. Она вычисляется, как количество совпадающих слов\n",
        "в предложениях, нормированное суммарной длиной этих предложений.  \n",
        "Исходя из весов ребер, каждой вершине присваивается вес по следующей формуле:  \n",
        "$𝑊({𝑉_i}) = (1 − 𝑑) + 𝑑 * {\\sum\\limits_{V_j \\in Inc(V_i)}\\frac{w_{ij}}{\\sum\\limits_{V_k \\in Inc(V_i)}w_{jk}} * W({V_j})}$  \n",
        "где 𝑉𝑖, 𝑉𝑗 — вершины графа  \n",
        "𝐼𝑛𝑐(𝑉𝑖) — множество вершин, смежных с вершиной 𝑉𝑖  \n",
        "𝑤𝑖𝑗 — вес ребра между вершинами 𝑉𝑖, 𝑉𝑗  \n",
        "𝑑 — коэфициент затухания, который в данном алгоритме равен 0.85  \n",
        "Итерационный процесс завершается, как только веса вершин перестают меняться более, чем на 0.0001.\n",
        "После вычисления весов вершин они упорядочиваются по убыванию значения веса и в реферат включаются предложения, соответствующие первым 𝑛 вершинам, где 𝑛 — желаемое количество предложений в кратком описании.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqkHWG9AanVr"
      },
      "source": [
        "# косинусовая сходимость \n",
        "# на вход подается списки слов двух предложений\n",
        "def read(filename):\n",
        "    f = open(filename, \"r\", errors = 'ignore') \n",
        "    a = f.read()\n",
        "    text =[]\n",
        "    split_regex = re.compile(r'[.|!|?|…]')\n",
        "    sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(a)])\n",
        "    for s in sentences:\n",
        "        text.append(s)\n",
        "    res = list(sent_to_words(text))\n",
        "    return res\n",
        "\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords):\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "     \n",
        "    # вектор для первого предложения \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # вектор для второго предложения \n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return (1 - cosine_distance(vector1, vector2))\n",
        "\n",
        "\n",
        "# создаем матрицу косинусовой сходимости \n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "    return similarity_matrix\n",
        "\n",
        "\n",
        "def generate_summary_textrank(file_name, top_n):\n",
        "    summarize_text = []\n",
        "    sentences = read(file_name)\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stopwords_english)\n",
        "\n",
        "    # используем pagerank, чтобы определить веса предложений \n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank_numpy(sentence_similarity_graph)\n",
        "\n",
        "    # отсортируем предложения и выберем веса с большим весом \n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "\n",
        "    for i in range(top_n - 1):\n",
        "        if len(ranked_sentence[i][1]) > 2 and len(list((Counter(ranked_sentence[i][1]) - Counter(ranked_sentence[i + 1][1])).elements())) > 3:\n",
        "              summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "    return summarize_text\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UftBoQeXanVs",
        "outputId": "844f9480-4d51-416c-c293-6f2cb5e4ea11"
      },
      "source": [
        "a = generate_summary_textrank('ami-transcripts/ES2002a.transcript.txt', top_n=100)\n",
        "print(\"Summarize Text TextRank: \\n\", \". \".join(a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/alina/opt/anaconda3/lib/python3.7/site-packages/nltk/cluster/util.py:131: RuntimeWarning: invalid value encountered in true_divide\n",
            "  return 1 - (numpy.dot(u, v) / (sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Summarize Text TextRank: \n",
            " um dunno ive never bought remote control so dont know how how good remote control that would get you. right so do you think that should be like main design aim of our remote control you know do your your satellite and your regular telly and your v_c_r_ and everything. like how much does you know remote control cost. right okay well thats thats the end of the meeting then. um guess thats up to us mean you probably want some kind of unique selling point of it so um you know yeah. and um you know when think about what they are now its better but actually its still kind of dunno like massive junky thing on the table. um so first of all just to kind of make sure that we all know each other im laura and im the project manager. oh thats very good of you. um well anyway dont know its just the first animal can think off the top of my head. or another way is maybe people who have t_v_ sets are really fed up with their remote control and they really want better one or something. well twenty five euro mean thats um thats about like eighteen pounds or something isnt it. god still dont know what im gonna write about. yeah interesting thing about discussing um production of remote control for me is that as you point out just dont think of remote controls as somethin something people consciously assess in their purchasing habits. um so im wondering right away is selling twenty five euros is that sort of the thi is this gonna to be like the premium product kinda thing or uh huh. like you might put in there oh want to watch such and such and look oh thats good idea. right away im making some kind of assumptions about what what information were given here thinking kay trendy probably means something other than just basic something other than just standard. thats just really good id yep. well for remote control do you think that will be suppose its depends on how complicated our remote control is. well um are we at ma right now on the assumption that our television remote control may have features which go beyond the television. yeah thats thats it. do you know what mean. know um my parents went out and bought um remote controls because um they got fed up of having four or five different remote controls for each things the house. um for user interface technical functions guess thats you know like what weve been talking about what itll actually do. but dont know mean do you think the fact that its going to be sold internationally will have bearing on how we design it at all. so this one here right. im gonna have to think on the spot now. maybe we could think about how could be more you know streamlined. um so were designing new remote control and um oh have to record whos here actually. like so sort of like how do you mean one one way of looking at it would be well the people producing television sets maybe they have to buy remote controls. um so thats kind of our our brief as it were. think that thats the main factor. thats about it didnt get anything else. dont know what mine is. before we wrap up just to make sure were all on the same page here um do we we were given sort of an example of coffee machine or something right. um so anything else anybody wants to add about what they dont like about remote controls theyve used what they would really like to be part of this new one at all. um hes very friendly and cheery and always pleased to see you and very kind of affectionate and um uh and hes quite quite wee as well so you know he can doesnt take up too much space. okay so right so in function one of the priorities might be to combine as many uses think so. right well um so just to wrap up the next meetings gonna be in thirty minutes. there mean is that something wed want to include do you think. um so inbetween now and then um as the industrial designer youre gonna be working on you know the actual working design of it so you know what youre doing there. so like wonder if we might add something new to the to the remote control market such as the lighting in your house or um yeah yeah. and uh and thats the end of the meeting. im thinking the price might might appeal to certain market in one region whereas in another itll be different so just chara just characteristic of the just or just like basic product podi positioning the twenty five euro remote control might be big hit in london might not be such big hit in greece who knows something like that yeah. um yeah so des uh design new remote control. so they all work actually function together but have different remote controls for each of them. so uh you get to draw your favourite animal and sum up your favourite characteristics of it. right um where did you find this. imagine thats good question. well right away im wondering if theres um th th uh like with d_v_d_ players if there are zones. um so according to the brief um were gonna be selling this remote control for twenty five euro um and were aiming to make fifty million euro. think one factor would be production cost. um and uh marketing executive youll be just thinking about what it actually what you know what requirements it has to has to fulfil and youll all get instructions emailed to you guess. my favourite animal is like beagle. but ill just draw different kind of dog. mean its usually quite small or when you want it right it slipped behind the couch or its kicked under the table. so its sort of ironic that that then theyre in there um you know the sound and everything its just one system. finding them is really pain you know. my favourite animal is my own dog at home. well my favourite animal would be monkey. well guess thats up to our industrial designer. cause it could it could it could be that it could be that functionally that doesnt make it any better but that just the appeal of of not having you know these days theres pe things in peoples homes are becoming more and more like chic you know. remember when the first remote control my my family had was on cable. uh right well basically um high priority for any animal for me is that they be willing to take lot of physical affection from their family. so who would like to go first. did you get the same thing. um and what do like about him um thats just to suggest that his tail wags. and im andrew and im uh our marketing expert. um and um this is just what were gonna be doing over the next twenty five minutes. actually had cable between it and the t_v_ and big like buttons that sort of like like on blender or something. so thats about um about ten to twelve by my watch. maybe like touch screen or something. boy let me tell you. and you keep losing them. you keep losing them. let me just scoot on ahead here. what did you get. so got that little message lot sooner than thought would so mm hmm. um im not really sure what what you guys have already received um in your emails\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPLtp_KJanVs"
      },
      "source": [
        "Применив алгоритм TextRank, можно заметить, что наше краткое содержание не отражает главную суть текста и включает в себя предложения из одного-двух слов, которые почти одни и те же (forrest gump). Давайте попробуем поставить ограничение на то, что предложение должно содержать больше 4 - 5 слов и эти слова должны быть не похожи на лругие слова в нашем саммари, поэтому в конце добавим условия на то, что длина предложения > 2 и слова одного предложения отличаются от слов второго предложения на 3 и больше. \n",
        "Возможные варианты модификации данного алгоритма: \n",
        "1. Мы столкнулись с проблемой, что предложения в саммари одинаковые, поэтому возможно нужно добавить оглраничение на добавление предложения в саммари. Давайте попробуем использовать Maximum marginal relevance из пункта 8 (описание возможных алгоритмов)    \n",
        "MMR = $argmax_{s_i \\in \\mathcal{R-S}} (\\lambda * Sim_1(s_i, Q) - (1-\\lambda) * \\max\\limits_{s_j \\in S}Sim_2(s_i, s_j))$  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "occBgw1xanVs"
      },
      "source": [
        "textrank_hypothesis = ''\n",
        "for i in a:\n",
        "    textrank_hypothesis += i "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcI3wahmanVt",
        "outputId": "1586a82a-c064-45a3-9254-975f932ed4fb"
      },
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(textrank_hypothesis, transcript_sum)\n",
        "print('TextRank scores: {}'.format(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TextRank scores: [{'rouge-1': {'f': 0.4478741955069878, 'p': 0.2957692307692308, 'r': 0.9220623501199041}, 'rouge-2': {'f': 0.3927738890977979, 'p': 0.2593305117352828, 'r': 0.8091236494597839}, 'rouge-l': {'f': 0.5626134259831654, 'p': 0.3979460847240051, 'r': 0.9597523219814241}}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By3NbloPfJwt"
      },
      "source": [
        "На всем датасете. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6LcOb3afJwt"
      },
      "source": [
        "scores1f = []\n",
        "scores1p = []\n",
        "scores1r = []\n",
        "scores2f = []\n",
        "scores2p = []\n",
        "scores2r = []\n",
        "scoresllf = []\n",
        "scoresllp = []\n",
        "scoresllr = []\n",
        "bleu1 = []\n",
        "bleu2 = []\n",
        "bleu3 = []\n",
        "bleu_together = []\n",
        "for i in range(len(ext_sum_trans)):\n",
        "    fin_textrank = generate_summary_textrank(ext_sum_trans[i], top_n = 20)\n",
        "    textrank_hypothesis = ''\n",
        "    for j in fin_textrank :\n",
        "        textrank_hypothesis += j \n",
        "    test_ = open(ext_summaries_files[i], \"r\", errors = 'ignore')\n",
        "    transcript_ = test_.read()\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(textrank_hypothesis, transcript_)\n",
        "    scores1f.append(scores[0]['rouge-1']['f'])\n",
        "    scores1p.append(scores[0]['rouge-1']['p'])\n",
        "    scores1r.append(scores[0]['rouge-1']['r'])\n",
        "    scores2f.append(scores[0]['rouge-2']['f'])\n",
        "    scores2p.append(scores[0]['rouge-2']['p'])\n",
        "    scores2r.append(scores[0]['rouge-2']['r'])\n",
        "    scoresllf.append(scores[0]['rouge-l']['f'])\n",
        "    scoresllp.append(scores[0]['rouge-l']['p'])\n",
        "    scoresllr.append(scores[0]['rouge-l']['r'])  \n",
        "    # bleu \n",
        "    reference = preprocess_text_simple(transcript_)\n",
        "    candidate = preprocess_text_simple(textrank_hypothesis)\n",
        "    score_together = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    bleu_together.append(score_together)\n",
        "    bleu_1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "    bleu1.append(bleu_1)\n",
        "    bleu_2 = sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))\n",
        "    bleu2.append(bleu_2)\n",
        "    bleu_3 = sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))\n",
        "    bleu3.append(bleu_3) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJsctmpefJwt",
        "outputId": "888f18e2-74a6-4200-d864-664f149ca778"
      },
      "source": [
        "print('bleu-1 : {}, bleu: {}'.format(np.mean(np.array(bleu1)), np.mean(np.array(bleu_together))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bleu-1 : 0.0, bleu: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgeKpI1AfJwu",
        "outputId": "65538875-671f-4883-931e-3a672c691e32"
      },
      "source": [
        "print('Rouge-1 : r: {}, p: {}, f:{}'.format(np.mean(np.array(scores1f)), np.mean(np.array(scores1p)), np.mean(np.array(scores1r))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rouge-1 : r: 0.2922234898592246, p: 0.791813890992965, f:0.19408800561762413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sTTxha_fJwu",
        "outputId": "60b2e664-c5d2-4c0c-896f-663c05a7bb16"
      },
      "source": [
        "print('Rouge-2 : r: {}, p: {}, f:{}'.format(np.mean(np.array(scores2f)), np.mean(np.array(scores2p)), np.mean(np.array(scores2r))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rouge-2 : r: 0.15336679582270266, p: 0.41746304213901675, f:0.10161198072330613\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9T4VpbyfJwu",
        "outputId": "d489cb71-5b69-49e8-b829-d9a73d7bda0d"
      },
      "source": [
        "print('Rouge-l : r: {}, p: {}, f:{}'.format(np.mean(np.array(scoresllf)), np.mean(np.array(scoresllp)), np.mean(np.array(scoresllr))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rouge-l : r: 0.3206825199555802, p: 0.6486002368646, f:0.2240761168138765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLloGUgsanVt"
      },
      "source": [
        "### ClusterRank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbZ9gN_danVt"
      },
      "source": [
        "https://github.com/AdiChat/senpai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNB1JW_0anVu"
      },
      "source": [
        "def read(filename):\n",
        "    f = open(filename, \"r\", errors = 'ignore') \n",
        "    a = f.read()\n",
        "    text =[]\n",
        "    split_regex = re.compile(r'[.|!|?|…]')\n",
        "    sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(a)])\n",
        "    for s in sentences:\n",
        "        text.append(s)\n",
        "    res = list(sent_to_words(text))\n",
        "    return res\n",
        "\n",
        "# Расстояние левенштайна \n",
        "def lDistance(firstString, secondString):\n",
        "    if len(firstString) > len(secondString):\n",
        "        firstString, secondString = secondString, firstString\n",
        "    distances = range(len(firstString) + 1)\n",
        "    for index2, char2 in enumerate(secondString):\n",
        "        newDistances = [index2 + 1]\n",
        "        for index1, char1 in enumerate(firstString):\n",
        "            if char1 == char2:\n",
        "                newDistances.append(distances[index1])\n",
        "            else:\n",
        "               newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1])))\n",
        "        distances = newDistances\n",
        "    return distances[-1]\n",
        "\n",
        "def build_lev_matrix(sentences, stop_words):\n",
        "    lev_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            lev_matrix[idx1][idx2] = lDistance(sentences[idx1], sentences[idx2])\n",
        "    return lev_matrix\n",
        "\n",
        "\n",
        "def generate_clusterrank_summary(file_name, top_n):\n",
        "    summarize_text = []\n",
        "    sentences = read(file_name)\n",
        "    sentence_lev_martix = build_lev_matrix(sentences, stopwords_english)\n",
        "\n",
        "    # используем pagerank, чтобы определить веса предложений \n",
        "    sentence_lev_graph = nx.from_numpy_array(sentence_lev_martix)\n",
        "    scores = nx.pagerank(sentence_lev_graph)\n",
        "\n",
        "    # отсортируем предложения и выберем веса с большим весом \n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    #print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "\n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "    return summarize_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykbiD0LoanVu",
        "outputId": "0c518f61-97fa-4c00-b7ab-aa709afb6853"
      },
      "source": [
        "k = generate_clusterrank_summary('ami-transcripts/ES2002a.transcript.txt', 10)\n",
        "print(\"Summarize Text: \\n\", \". \".join(k))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summarize Text: \n",
            " im thinking the price might might appeal to certain market in one region whereas in another itll be different so just chara just characteristic of the just or just like basic product podi positioning the twenty five euro remote control might be big hit in london might not be such big hit in greece who knows something like that yeah. cause it could it could it could be that it could be that functionally that doesnt make it any better but that just the appeal of of not having you know these days theres pe things in peoples homes are becoming more and more like chic you know. um hes very friendly and cheery and always pleased to see you and very kind of affectionate and um uh and hes quite quite wee as well so you know he can doesnt take up too much space. yeah interesting thing about discussing um production of remote control for me is that as you point out just dont think of remote controls as somethin something people consciously assess in their purchasing habits. um and uh marketing executive youll be just thinking about what it actually what you know what requirements it has to has to fulfil and youll all get instructions emailed to you guess. um so anything else anybody wants to add about what they dont like about remote controls theyve used what they would really like to be part of this new one at all. before we wrap up just to make sure were all on the same page here um do we we were given sort of an example of coffee machine or something right. like so sort of like how do you mean one one way of looking at it would be well the people producing television sets maybe they have to buy remote controls. right so do you think that should be like main design aim of our remote control you know do your your satellite and your regular telly and your v_c_r_ and everything. um so im wondering right away is selling twenty five euros is that sort of the thi is this gonna to be like the premium product kinda thing or uh huh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmBFZbnVanVu",
        "outputId": "4505635b-833f-429b-bcf6-784101b19341"
      },
      "source": [
        "k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['im thinking the price might might appeal to certain market in one region whereas in another itll be different so just chara just characteristic of the just or just like basic product podi positioning the twenty five euro remote control might be big hit in london might not be such big hit in greece who knows something like that yeah',\n",
              " 'cause it could it could it could be that it could be that functionally that doesnt make it any better but that just the appeal of of not having you know these days theres pe things in peoples homes are becoming more and more like chic you know',\n",
              " 'um hes very friendly and cheery and always pleased to see you and very kind of affectionate and um uh and hes quite quite wee as well so you know he can doesnt take up too much space',\n",
              " 'yeah interesting thing about discussing um production of remote control for me is that as you point out just dont think of remote controls as somethin something people consciously assess in their purchasing habits',\n",
              " 'um and uh marketing executive youll be just thinking about what it actually what you know what requirements it has to has to fulfil and youll all get instructions emailed to you guess',\n",
              " 'um so anything else anybody wants to add about what they dont like about remote controls theyve used what they would really like to be part of this new one at all',\n",
              " 'before we wrap up just to make sure were all on the same page here um do we we were given sort of an example of coffee machine or something right',\n",
              " 'like so sort of like how do you mean one one way of looking at it would be well the people producing television sets maybe they have to buy remote controls',\n",
              " 'right so do you think that should be like main design aim of our remote control you know do your your satellite and your regular telly and your v_c_r_ and everything',\n",
              " 'um so im wondering right away is selling twenty five euros is that sort of the thi is this gonna to be like the premium product kinda thing or uh huh']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV1XQIVYanVv"
      },
      "source": [
        "clusterrank_hypothesis = ''\n",
        "for i in k:\n",
        "    clusterrank_hypothesis += i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_kb7JK2anVv",
        "outputId": "b4464925-13d6-437e-fb32-448e73794e7f"
      },
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(clusterrank_hypothesis, transcript_sum)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'f': 0.4539363441966954,\n",
              "   'p': 0.7527777777777778,\n",
              "   'r': 0.3249400479616307},\n",
              "  'rouge-2': {'f': 0.22651006290472558,\n",
              "   'p': 0.37604456824512533,\n",
              "   'r': 0.16206482593037214},\n",
              "  'rouge-l': {'f': 0.42043221540151543,\n",
              "   'p': 0.5752688172043011,\n",
              "   'r': 0.33126934984520123}}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLvGoVNEanVv"
      },
      "source": [
        "На всем датасете. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNZNMeGUfJww"
      },
      "source": [
        "scores1f = []\n",
        "scores1p = []\n",
        "scores1r = []\n",
        "scores2f = []\n",
        "scores2p = []\n",
        "scores2r = []\n",
        "scoresllf = []\n",
        "scoresllp = []\n",
        "scoresllr = []\n",
        "bleu1 = []\n",
        "bleu_together = []\n",
        "for i in range(len(ext_sum_trans)):\n",
        "    fin_clusterrank = generate_clusterrank_summary(ext_sum_trans[i], top_n = 18)\n",
        "    clusterrank_hypothesis = ''\n",
        "    for j in fin_clusterrank :\n",
        "        clusterrank_hypothesis += j \n",
        "    test_ = open(ext_summaries_files[i], \"r\", errors = 'ignore')\n",
        "    transcript_ = test_.read()\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(clusterrank_hypothesis, transcript_)\n",
        "    scores1f.append(scores[0]['rouge-1']['f'])\n",
        "    scores1p.append(scores[0]['rouge-1']['p'])\n",
        "    scores1r.append(scores[0]['rouge-1']['r'])\n",
        "    scores2f.append(scores[0]['rouge-2']['f'])\n",
        "    scores2p.append(scores[0]['rouge-2']['p'])\n",
        "    scores2r.append(scores[0]['rouge-2']['r'])\n",
        "    scoresllf.append(scores[0]['rouge-l']['f'])\n",
        "    scoresllp.append(scores[0]['rouge-l']['p'])\n",
        "    scoresllr.append(scores[0]['rouge-l']['r'])  \n",
        "    # bleu \n",
        "    reference = preprocess_text_simple(transcript_)\n",
        "    candidate = preprocess_text_simple(clusterrank_hypothesis)\n",
        "    score_together = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    bleu_together.append(score_together)\n",
        "    bleu_1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "    bleu1.append(bleu_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys2SO7DLfJww"
      },
      "source": [
        "print('bleu-1 : {}, bleu: {}'.format(np.mean(np.array(bleu1)), np.mean(np.array(bleu_together))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWtahhCtfJwx"
      },
      "source": [
        "print('Rouge-1 : r: {}, p: {}, f:{}'.format(np.mean(np.array(scores1f)), np.mean(np.array(scores1p)), np.mean(np.array(scores1r))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWbYZ-AJfJwx"
      },
      "source": [
        "print('Rouge-2 : r: {}, p: {}, f:{}'.format(np.mean(np.array(scores2f)), np.mean(np.array(scores2p)), np.mean(np.array(scores2r))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyh3xLTXfJwx"
      },
      "source": [
        "print('Rouge-l : r: {}, p: {}, f:{}'.format(np.mean(np.array(scoresllf)), np.mean(np.array(scoresllp)), np.mean(np.array(scoresllr))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrw50ZHdanVv"
      },
      "source": [
        "### LSA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYXxqMxZanVw"
      },
      "source": [
        "Источники, где можно посмотреть  про этот метод:  \n",
        "https://github.com/llazzaro/lsa_python/blob/master/lsa/lsa.py  \n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python  \n",
        "http://www.kiv.zcu.cz/~jstein/publikace/isim2004.pdf  \n",
        "https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RORkFDr_anVx"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcnGEBILanVx"
      },
      "source": [
        "def calc_sigma(matrix):\n",
        "    (m, n) = matrix.shape   # узнаем размерность матрицы train\n",
        "    sigma = np.zeros((m, n))  # задаем матрицу из нулей для сигмы такой же размерности, как A\n",
        " \n",
        "    matrix_transposed = matrix.transpose()    # транспонируем A\n",
        "    matrix_c = 1/n * matrix.dot(matrix_transposed)   # считаем ковариационную матрицу C\n",
        " \n",
        "    wa, u = np.linalg.eig(matrix_c)   # wa - собственные значения, u - соответствующие собственные векторы\n",
        "    wa.sort()   # сортируем собственные значения\n",
        "    wa = wa[::-1]  # в порядке убывания\n",
        " \n",
        "    sigmas = np.sqrt(wa)   # вычисляем корни от всех собственных чисел\n",
        " \n",
        "    i = 0\n",
        "    if m < n:   # выставляем собственные значения по главной диагонали\n",
        "        while i < m:\n",
        "            sigma[i, i] = sigmas[i]\n",
        "            i += 1\n",
        "    if n < m:\n",
        "        while i < n:\n",
        "            sigma[i, i] = sigmas[i]\n",
        "            i += 1\n",
        " \n",
        "    s = []  # формальность: для удобства меняем тип array на список\n",
        "    for vector in sigma:\n",
        "        s.append([vector[i] for i in range(len(vector))])\n",
        " \n",
        "    return s\n",
        " \n",
        " \n",
        "# Подсчет W_transposed - каждый столбик - собственный вектор матрицы C = A^(T) * A (отсортированы по убыванию)\n",
        " \n",
        "def calc_w(matrix):\n",
        "    va_sorted = []  # отсортируем собственные векторы по убыванию собственных значений\n",
        " \n",
        "    matrix_transposed = matrix.transpose()  # транспонируем A\n",
        "    matrix_c = matrix_transposed.dot(matrix)  # считаем матрицу С\n",
        "    print(\"Матрица C: \")\n",
        "    print(matrix_c)\n",
        "    print()\n",
        " \n",
        "    eigenvalues_c, eigenvectors_c = np.linalg.eig(matrix_c)  # собственные значения и собственные векторы матрицы C (правые)\n",
        "    wa_index_value = []  # создадим массив с элементами вида (индекс собственного значения, собственное значение)\n",
        " \n",
        "    counter = 0\n",
        "    while counter < len(eigenvalues_c):  # заполним такой массив\n",
        "        (m, n) = (counter, eigenvalues_c[counter])\n",
        "        wa_index_value.append((m, n))\n",
        "        counter += 1\n",
        "    wa_index_value_sorted = sorted(wa_index_value, key=lambda index_value: index_value[1])  # сортируем элементы по величине собственного значения\n",
        "    wa_index_value_sorted = wa_index_value_sorted[::-1]  # в порядке убывания\n",
        " \n",
        "    counter = 0  # заполняем поэлементно матрицу сортированных собственных значений\n",
        "    while counter < len(wa_index_value_sorted):  # проходим по очереди по всем () в wa_index_value_sorted\n",
        "        va_sorted.append(eigenvectors_c[:, wa_index_value_sorted[counter][0]])  # берем из списка собств. векторов тот, который с таким же индексом, как индекс в ()\n",
        "        counter += 1\n",
        " \n",
        "    eigenvectors_c_sorted = []  # формальность: для удобства меняем тип array на список\n",
        "    for vector in va_sorted:\n",
        "        eigenvectors_c_sorted.append([vector[i] for i in range(len(va_sorted))])\n",
        " \n",
        "    w = np.array(eigenvectors_c_sorted).transpose()  # транспонируем матрицу W\n",
        " \n",
        "    return w\n",
        " \n",
        " \n",
        "#   Подсчет U\n",
        " \n",
        "def calc_u(matrix):\n",
        "    matrix_transposed = matrix.transpose()  # транспонируем A\n",
        "    matrix_c2 = matrix.dot(matrix_transposed)  # считаем матрицу c2\n",
        "    va_sorted = []  # отсортируем впоследствии собственные векторы по убыванию собственных значений и внесем сюда\n",
        " \n",
        "    eigenvalues_c2, eigenvectors_c2 = np.linalg.eig(matrix_c2)   # левые собственные числа и векторы\n",
        "    wa_index_value = []  # создадим массив с элементами вида (индекс собственного значения, собственное значение)\n",
        " \n",
        "    counter = 0\n",
        "    while counter < len(eigenvalues_c2):  # заполним такой массив\n",
        "        (m, n) = (counter, eigenvalues_c2[counter])\n",
        "        wa_index_value.append((m, n))\n",
        "        counter += 1\n",
        "    wa_index_value_sorted = sorted(wa_index_value, key=lambda index_value: index_value[1])  # сортируем элементы по величине собственного значения\n",
        "    wa_index_value_sorted = wa_index_value_sorted[::-1]  # в порядке убывания\n",
        " \n",
        "    counter = 0  # заполняем поэлементно матрицу сортированных собственных значений\n",
        "    while counter < len(wa_index_value_sorted):  # проходим по очереди по всем () в wa_index_value_sorted\n",
        "        va_sorted.append(eigenvectors_c2[:, wa_index_value_sorted[counter][0]])  # берем из списка собств. векторов тот, который с таким же индексом, как индекс в ()\n",
        "        counter += 1\n",
        " \n",
        "    u = []  # формальность: для удобства меняем тип array на список\n",
        "    for vector in va_sorted:\n",
        "        u.append([vector[i] for i in range(len(va_sorted))])\n",
        "    u = np.array(u)\n",
        " \n",
        "    return u"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfi8nFXcfJwy"
      },
      "source": [
        "# как вариант можно делать через truncatedsvd \n",
        "'''\n",
        "from sklearn.decomposition import TruncatedSVD \n",
        "components = 30\n",
        "lsa = TruncatedSVD(n_components=components) \n",
        "lsa.fit(dtm)\n",
        "lsa_dtm = lsa.transform(dtm)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmYwvr2-anVx"
      },
      "source": [
        "# входные данные (текст , разбитый на предложения; количество предложений, которое мы хотим видеть в summary)\n",
        "def summary_LSA(transcript, k):\n",
        "    a2 = transcript.replace(\"\\n\", \"\")\n",
        "    a = a2.replace(\"\\ \", \" \")\n",
        "    text =[]\n",
        "    split_regex = re.compile(r'[.|!|?|…]')\n",
        "    sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(a)])\n",
        "    for s in sentences:\n",
        "        text.append(s)\n",
        "    vectorizer = TfidfVectorizer(stop_words = stopwords_english)\n",
        "    X = vectorizer.fit_transform(text)\n",
        "    y = X.toarray()\n",
        "    a = np.matrix(y).T\n",
        "    U_numpy, s_numpy, W_numpy = np.linalg.svd(a) \n",
        "    W_numpy_2 = np.square(W_numpy)\n",
        "    s_numpy_2 = np.square(s_numpy)\n",
        "    weights = np.sqrt(np.dot(W_numpy_2, s_numpy_2))\n",
        "    weights = np.array(weights)[0]\n",
        "    dict_weights = {k: v for k, v in enumerate(list(weights))}\n",
        "    indexes_weights = sorted(dict_weights, key=dict_weights.get, reverse = True)\n",
        "    indexes = indexes_weights[:k]\n",
        "    summary = []\n",
        "    for i in indexes:\n",
        "        summary.append(text[i])\n",
        "    return summary\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mwN4Mt7anVy",
        "outputId": "866dbeb0-e02b-4893-ca6a-9641ca888308"
      },
      "source": [
        "summary = summary_LSA(transcript, 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/alina/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['yeah'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ManQNUMKanVy",
        "outputId": "fd69b644-43a6-496e-98b2-cbbff55d1c6d"
      },
      "source": [
        "print(\"Summarize Text LSA: \\n\", \". \".join(summary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summarize Text LSA: \n",
            " So cost like production cost is twelve fifty, but selling price is is that wholesale or retail. Think it will. I imagine it probably is our sale actually because it's probably up to the the um the retailer to uh sell it for whatever price they want. Ah. Yeah, yeah. Okay, uh we now need to discuss the project finance. Um and what do I like about him, um That's just to suggest that his tail wags. Um, can we just go over that again. Is he aware that th it's his own cha tail he's chasing. Uh um. Um so according to the brief um we're gonna be selling this remote control for twenty five Euro, um and we're aiming to make fifty million Euro. I don't know what mine is. All together. And uh we don't want it to cost any more than uh twelve fifty Euros, so fifty percent of the selling price. Yeah. Well like um, maybe what we could use is a sort of like a example of a successful other piece technology is palm palm pilots. Do we have any other background information on like how that compares to other other Yeah. Okay. That would be useful, though, wouldn't it, if you knew like what your money would get you now. I see a dog in there. Sure. Yeah, so it's th the functional design stage is next, I guess. I imagine That's a good question. Is that right. Um I dunno. Okay. They're gone from being just like little sort of scribble boards to cameras, M_P_ three players, telephones, everything, agenda. Blue beagle. Mm-hmm. Um. No. Yeah. Well for a remote control, do you think that will be I suppose it's depends on how complicated our remote control is. Aye, I see what you mean, yeah. But I I don't know, I mean do you think the fact that it's going to be sold internationally will have a bearing on how we design it at all. And you all arrived on time. Uh, so bas at twel Alright, yeah. Um so we're gonna be selling this on an international scale. Um so that's kind of our our brief, as it were. Superb sketch, by the way. Okay so Right, so in function one of the priorities might be to combine as many uses I think so. Ah well. I will go. We're a bit behind. So that's David, Andrew and Craig, isn't it. Yeah. Five minutes to end of meeting. What, just like in terms of like the wealth of the country. Right. Yeah\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xCx3FiJanVy",
        "outputId": "95418472-a03d-4837-b1a9-1388f8fa3b9f"
      },
      "source": [
        "lsa_hypothesis = ''\n",
        "for i in summary:\n",
        "    lsa_hypothesis += i\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(lsa_hypothesis, transcript_sum)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'f': 0.4440789430592647,\n",
              "   'p': 0.7068062827225131,\n",
              "   'r': 0.3237410071942446},\n",
              "  'rouge-2': {'f': 0.2487644108496292,\n",
              "   'p': 0.3963254593175853,\n",
              "   'r': 0.18127250900360145},\n",
              "  'rouge-l': {'f': 0.3724394737907334,\n",
              "   'p': 0.4672897196261682,\n",
              "   'r': 0.30959752321981426}}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG46158UfJwz"
      },
      "source": [
        "На всем датасете. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okDwRrnUfJw0"
      },
      "source": [
        "scores1f = []\n",
        "scores1p = []\n",
        "scores1r = []\n",
        "scores2f = []\n",
        "scores2p = []\n",
        "scores2r = []\n",
        "scoresllf = []\n",
        "scoresllp = []\n",
        "scoresllr = []\n",
        "bleu1 = []\n",
        "bleu2 = []\n",
        "bleu3 = []\n",
        "bleu_together = []\n",
        "for i in range(len(ext_sum_trans)):\n",
        "    fin_LSA = summary_LSA(ext_sum_trans[i], top_n = 20)\n",
        "    LSA_hypothesis = ''\n",
        "    for j in fin_LSA :\n",
        "        LSA_hypothesis += j \n",
        "    test_ = open(ext_summaries_files[i], \"r\", errors = 'ignore')\n",
        "    transcript_ = test_.read()\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(LSA_hypothesis, transcript_)\n",
        "    scores1f.append(scores[0]['rouge-1']['f'])\n",
        "    scores1p.append(scores[0]['rouge-1']['p'])\n",
        "    scores1r.append(scores[0]['rouge-1']['r'])\n",
        "    scores2f.append(scores[0]['rouge-2']['f'])\n",
        "    scores2p.append(scores[0]['rouge-2']['p'])\n",
        "    scores2r.append(scores[0]['rouge-2']['r'])\n",
        "    scoresllf.append(scores[0]['rouge-l']['f'])\n",
        "    scoresllp.append(scores[0]['rouge-l']['p'])\n",
        "    scoresllr.append(scores[0]['rouge-l']['r'])  \n",
        "    # bleu \n",
        "    reference = preprocess_text_simple(transcript_)\n",
        "    candidate = preprocess_text_simple(LSA_hypothesis)\n",
        "    score_together = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    bleu_together.append(score_together)\n",
        "    bleu_1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "    bleu1.append(bleu_1)\n",
        "    bleu_2 = sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))\n",
        "    bleu2.append(bleu_2)\n",
        "    bleu_3 = sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))\n",
        "    bleu3.append(bleu_3) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrltVDIofJw0"
      },
      "source": [
        "print('bleu-1 : {}, bleu: {}'.format(np.mean(np.array(bleu1)), np.mean(np.array(bleu_together))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3CiZJwNfJw0"
      },
      "source": [
        "print('Rouge-1 : r: {}, p: {}, f:{}'.format(np.mean(np.array(scores1f)), np.mean(np.array(scores1p)), np.mean(np.array(scores1r))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3JZ4mLxfJw0"
      },
      "source": [
        "print('Rouge-2 : r: {}, p: {}, f:{}'.format(np.mean(np.array(scores2f)), np.mean(np.array(scores2p)), np.mean(np.array(scores2r))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE4PMLlCfJw0"
      },
      "source": [
        "print('Rouge-l : r: {}, p: {}, f:{}'.format(np.mean(np.array(scoresllf)), np.mean(np.array(scoresllp)), np.mean(np.array(scoresllr))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxM6MCGUanVz"
      },
      "source": [
        "### KLsum "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f140J_e9anVz"
      },
      "source": [
        "Вероятностный алгоритм, который используя KL дивергенцию как меру расстояния между вероятностными распределениями, ищет подмножество предложений текста, распределение слов в которых ближе всего к распределению слов в исходном тексте.KL Sum вводит следующий критерий для выбора предложений в реферат S для предложения D:  \n",
        "KL(P|Q) = Σ P(w)log P(w)/Q(w)  \n",
        "Это метод, который добавляет предложения, пока коэффицент Kl divergence уменьшается. Цель этого алгорттма найти множество предложений, чья длина слов меньше L и распределение слов в саммари совпадает с распределением в тексте.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSBHHg86fJw1"
      },
      "source": [
        "Расписать подробнее про расстояние Кульбака - Лейбера. Расстояние Кульбака - Лейбреа - неотрицательнозначный функционал, являющийся несимметричной мерой удалённости друг от друга двух вероятностных распределений, определённых на общем пространстве элементарных событий.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR1ZOwjZanV0"
      },
      "source": [
        "# принимает список частот слов в саммари и список частот слов в тексте\n",
        "def sent_to_words_1(sentences):\n",
        "    for sent in sentences:\n",
        "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
        "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
        "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
        "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
        "        yield(sent) \n",
        "\n",
        "def delete_sentecnes_with_stopwords(a):\n",
        "    for i in a:\n",
        "        length_sent = len(i)\n",
        "        for j in i:\n",
        "            if j in stopwords_english:\n",
        "                length_sent -= 1\n",
        "        if length_sent < 3:\n",
        "            a.remove(i)\n",
        "    return a\n",
        "                \n",
        "    \n",
        "def kl_divergence(summary_density, text_density):\n",
        "        sum_val = 0\n",
        "        # для каждого слова из саммари смотрим, есть ли слово из саммари в тексте \n",
        "        for w in summary_density:\n",
        "            # frequency \n",
        "            frequency = text_density.get(w)\n",
        "            # если частота этого слова существует, то мы считаем частоту в тексте и в саммари этого слова и используем формулу kl\n",
        "            if frequency: \n",
        "                sum_val += frequency * math.log(frequency / summary_density[w])\n",
        "        return sum_val\n",
        "\n",
        "\n",
        "def get_all_words(snw):\n",
        "    all_words = []\n",
        "    for i in snw:\n",
        "        for j in i:\n",
        "            if j not in stopwords_english:\n",
        "                all_words.append(j)\n",
        "    return all_words\n",
        "\n",
        "def calculate_freq(array):\n",
        "    word_freq = {}\n",
        "    for w in array:\n",
        "        word_freq[w] = word_freq.get(w, 0) + 1\n",
        "    return word_freq  \n",
        "\n",
        "def get_sentences_list(filename):\n",
        "    f = open(filename, \"r\", errors = 'ignore') \n",
        "    a = f.read()\n",
        "    text =[]\n",
        "    split_regex = re.compile(r'[.|!|?|…]')\n",
        "    sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(a)])\n",
        "    for s in sentences:\n",
        "        if len(s) > 7:\n",
        "            text.append(s)\n",
        "    return text   \n",
        "\n",
        "def find_index_of_best_sentence(kls):\n",
        "    return kls.index(min(kls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh3Bem_jfJw2"
      },
      "source": [
        "import math \n",
        "import re\n",
        "from gensim import models\n",
        "def generate_klsum_summary(transcript, num_sent):\n",
        "    sentences_list = get_sentences_list(transcript)\n",
        "    sentences_as_words_= list(sent_to_words(sentences_list))\n",
        "    sentences_as_words = delete_sentecnes_with_stopwords(sentences_as_words_)\n",
        "    all_w = get_all_words(sentences_as_words)\n",
        "    word_freq = calculate_freq(all_w)\n",
        "    summary = []\n",
        "    while num_sent > 0:\n",
        "        # будет хранить значения kls, чтобы из них выбрать наиментгее\n",
        "        kls = []\n",
        "        # превращает краткое содержание в список слов \n",
        "        summary_as_word_list = get_all_words(sent_to_words(summary))\n",
        "\n",
        "        for s in sentences_as_words:\n",
        "            # если мы добавим предложение, то какое будет распределение\n",
        "            # соединим список слов в предложении и то, что в саммари \n",
        "            new_joint = s + summary_as_word_list\n",
        "            joint_freq = calculate_freq(new_joint)\n",
        "\n",
        "            # считаем kls между распределением саммари и слова в тексте \n",
        "            kls.append(kl_divergence(joint_freq, word_freq))\n",
        "            new_joint = summary_as_word_list\n",
        "\n",
        "        indexToRemove = find_index_of_best_sentence(kls)\n",
        "        best_sentence = sentences_list.pop(indexToRemove)\n",
        "        del sentences_as_words[indexToRemove]\n",
        "        summary.append(best_sentence)\n",
        "        num_sent -= 1 \n",
        "    summary_fin = ''\n",
        "    for i in summary:\n",
        "        summary_fin += '. ' + i\n",
        "    return summary_fin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF_L5ex5fJw2",
        "outputId": "083b4741-deb3-494a-bc9b-d0f3b68e646a"
      },
      "source": [
        "generate_klsum_summary('ami-transcripts/ES2002a.transcript.txt', 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\". I coulda told you a whole lot more about beagles. Is that right. Um I'm not really sure what what you guys have already received um in your emails. Actually had a cable between it and the T_V_ and big like buttons that sort of like, like on a blender or something. Yeah, interesting thing about discussing um production of a remote control for me is that l as you point out, I just don't think of remote controls as somethin something people consciously assess in their purchasing habits. Uh, so bas at twel Alright, yeah. Don't feel like you're in a rush, anyway. Like how much does, you know, a remote control cost. I was gonna choose a dog as well. Okay, yeah. Yeah, sure. He only does it after he's had his dinner and um he'll just all of a sudden just get up and start chasing his tail 'round the living room. Yeah, yeah. I I don't know. Um and so there are three different stages to the design. Think it will. Right away I'm making some kind of assumptions about what what information we're given here, thinking, 'kay trendy probably means something other than just basic, something other than just standard. Is he aware that th it's his own cha tail he's chasing. It's just like getting shoelaces with shoes or something. But each one's got its own little part\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lOQ2jR_anV0"
      },
      "source": [
        "'''\n",
        "# существует библиотека для этого метода \n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.kl import KLSummarizer\n",
        "document = a\n",
        "parser=PlaintextParser.from_string(document,Tokenizer(\"english\"))\n",
        "summarizer = KLSummarizer()\n",
        "summary = summarizer(parser.document,50)\n",
        "summ_klsumm = []\n",
        "for sentence in summary:\n",
        "    summ_klsumm.append(sentence)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkthlGUqfJw2"
      },
      "source": [
        "На полном датасете: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gsVQePEfJw3",
        "outputId": "a9de06ea-340a-4deb-e555-6160ee484a52"
      },
      "source": [
        "scores1f = []\n",
        "scores1p = []\n",
        "scores1r = []\n",
        "scores2f = []\n",
        "scores2p = []\n",
        "scores2r = []\n",
        "scoresllf = []\n",
        "scoresllp = []\n",
        "scoresllr = []\n",
        "bleu1 = []\n",
        "bleu2 = []\n",
        "bleu3 = []\n",
        "bleu_together = []\n",
        "for i in range(len(ext_sum_trans)):\n",
        "    fin_klsum = generate_klsum_summary(ext_sum_trans[i], 30)\n",
        "    klsum_hypothesis = ''\n",
        "    for j in fin_klsum :\n",
        "        klsum_hypothesis += j \n",
        "    test_ = open(ext_summaries_files[i], \"r\", errors = 'ignore')\n",
        "    transcript_ = test_.read()\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(klsum_hypothesis, transcript_)\n",
        "    scores1f.append(scores[0]['rouge-1']['f'])\n",
        "    scores1p.append(scores[0]['rouge-1']['p'])\n",
        "    scores1r.append(scores[0]['rouge-1']['r'])\n",
        "    scores2f.append(scores[0]['rouge-2']['f'])\n",
        "    scores2p.append(scores[0]['rouge-2']['p'])\n",
        "    scores2r.append(scores[0]['rouge-2']['r'])\n",
        "    scoresllf.append(scores[0]['rouge-l']['f'])\n",
        "    scoresllp.append(scores[0]['rouge-l']['p'])\n",
        "    scoresllr.append(scores[0]['rouge-l']['r'])  \n",
        "    # bleu \n",
        "    reference = preprocess_text_simple(transcript_)\n",
        "    candidate = preprocess_text_simple(klsum_hypothesis)\n",
        "    score_together = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    bleu_together.append(score_together)\n",
        "    bleu_1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "    bleu1.append(bleu_1)\n",
        "    bleu_2 = sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))\n",
        "    bleu2.append(bleu_2)\n",
        "    bleu_3 = sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))\n",
        "    bleu3.append(bleu_3) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/Users/alina/opt/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/Users/alina/opt/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/Users/alina/opt/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-OXnyqianV1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "namNjickanV1"
      },
      "source": [
        "### Mead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9heiKoAanV1"
      },
      "source": [
        "Для начала определяем наши предложения, как точки в векторном пространстве. Первое, что мы хотим это применить к ним кластеризацию, то есть разделить их на топики. Затем мы ищем центроиды кластеров. И выбираем предложения, которые наиболее близки к центроидам, то есть фактически применяем k-means. Но тут также может возникнуть, что у нас появятся предложения, одинаковые по смыслу, но близкие к центроиду, поэтому нужно будет затем посмотреть на similarity matrix (их виды указаны ниже) и выбрать предложения, у которых коэффицент схожести низкий.  \n",
        "Имеем предложения в векторном пространстве,делаем клестеризацию по топикам. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc7Te49wanV2"
      },
      "source": [
        "Итак, определим точки в векторном пространстве. Как мы это сделаем? Мы создадим множество слов нашего документа, и они будут нашими столбцами, а наши рядами будут сами предложения, получается, у нас будет матрица из n строк, где n - число предложений и из m столбцов, где m - количество уникальных слов в документе.  \n",
        "Итак, теперь нам нужно применить KMeans, но центроид в алгоритме Kmeans это центроид масс, то есть это среднее значение векторов, описывающих это класс. А что такое среднее значение слов предложений? С семантической точки зрения брать среднее значение бессмысленно. Поэтому нужно придумать другой метод, по сути скорее всего в нашей матрице данных у нас будут в основном бинарные данные за исключением редких случаев. Может, тогда есть смысл считать расстояние Хемминга и сделать каким-то образом кластеризацию с другой метрикой. Но, к счастью, есть такой способ Kmedoids (https://en.wikipedia.org/wiki/K-medoids), k-means требует в качестве метрики евклидово расстояние, а этот метод минимизирует сумма парных схожестей между 2 объектами.  \n",
        "Медоид (в кластерном анализе) — объект, принадлежащий набору данных или кластеру, различие (например, по координатам) которого с другими объектами в наборе данных или кластере минимально. Медоиды близки по смыслу центроидам, но в отличие от них, являются объектом, принадлежащим кластеру, и как правило используются в тех случаях, когда невозможно вычислить средние координаты или центр масс кластера. И это то, что нам нужно!  \n",
        "Количество кластеров - колчиество предложений, которые мы хотим изъять в краткое содержание.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6rEAgAdanV2"
      },
      "source": [
        "# функция, которая объединяет все выше сделанные действия \n",
        "def text_to_sentences(filename):\n",
        "    f = open(filename, \"r\", errors = 'ignore') \n",
        "    a = f.read()\n",
        "    text =[]\n",
        "    split_regex = re.compile(r'[.|!|?|…]')\n",
        "    sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(a)])\n",
        "    for s in sentences:\n",
        "        text.append(s)\n",
        "    return text\n",
        "    \n",
        "def preprocess(filename):\n",
        "    f = open(filename, \"r\", errors = 'ignore') \n",
        "    a = f.read()\n",
        "    text =[]\n",
        "    split_regex = re.compile(r'[.|!|?|…]')\n",
        "    sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(a)])\n",
        "    for s in sentences:\n",
        "        text.append(s)\n",
        "    res = list(sent_to_words(text))\n",
        "    res_ = []\n",
        "    s_ =[]\n",
        "    for s in res:\n",
        "        for word in s:\n",
        "            if word not in stopwords_english:\n",
        "                s_.append(word)\n",
        "        res_.append(s_)\n",
        "        s_ = []\n",
        "    res_fin = []\n",
        "    s_fin = []\n",
        "    for s in res_:\n",
        "        for word in s:\n",
        "            s_fin.append(lemmatizer.lemmatize(word))\n",
        "        res_fin.append(s_fin)\n",
        "        s_fin = []\n",
        "    return res_fin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igE-I5W8anV3"
      },
      "source": [
        "from sklearn_extra.cluster import KMedoids\n",
        "def summary_mead(file):\n",
        "    text = text_to_sentences(file)\n",
        "    words = []\n",
        "    f = preprocess(file)\n",
        "    new_f = []\n",
        "    for i in f:\n",
        "        if len(i) > 6:\n",
        "            new_f.append(i)\n",
        "            for j in i:\n",
        "                words.append(j)\n",
        "    all_words = list(set(words))\n",
        "    matrix = []\n",
        "    vector = [0] * len(all_words)\n",
        "    for sent in new_f:\n",
        "        for w in sent:\n",
        "            if w in all_words:\n",
        "                vector[all_words.index(w)] += 1\n",
        "        matrix.append(vector)\n",
        "        vector = [0] * len(all_words)\n",
        "    kmedoids = KMedoids(n_clusters=46, random_state=42).fit(matrix)\n",
        "    kmedoids_centers = kmedoids.cluster_centers_\n",
        "    kmedoids_centers = list(kmedoids_centers)\n",
        "    for i in range(len(kmedoids_centers)):\n",
        "        kmedoids_centers[i] = list(kmedoids_centers[i])\n",
        "    indexes_sum = []\n",
        "    for i in kmedoids_centers:\n",
        "        indexes_sum.append(matrix.index(i))\n",
        "    final_summary = []\n",
        "    for i in indexes_sum:\n",
        "        final_summary.append(' '.join(new_f[i]))\n",
        "    full_summary = '. '.join(final_summary)\n",
        "    return full_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihjnesEeanV3",
        "outputId": "e75e51ee-09b3-44df-d495-32824a4c27cc"
      },
      "source": [
        "mead_hypothesis = summary_mead('/Users/alina/Downloads/AMICorpusXML-master/data/ami-transcripts/ES2002a.transcript.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summary: designing new remote control oh record who actually. im really sure guy already received email. anyway dont know first animal think top head. probably little got lot attention forever conditioned. hi im david im supposed industrial designer. guess thats u mean probably want kind unique selling point know. frequency something character different keypad style symbol. got little message lot sooner thought would. user interface technical function guess thats know talking itll actually. right wrap next meeting gonna thirty minute. get one whistle make really high pitched noise beep. id wel gonna wrap pretty quickly next couple minute. dont know mean think fact going sold internationally bearing design. right function one priority might combine many us think. god still dont know im gonna write. take long havent got awful lot discus. wonder might add something new remote control market lighting house. get draw favourite animal sum favourite characteristic. work actually function together different remote control. sort ironic theyre know sound everything one system. actually cable t_v_ big button sort blender something. theyre small cute furry planet ape becomes real im gonna. dont want cost twelve fifty euro fifty percent selling price. mean usually quite small want right slipped behind couch kicked table. right assumption television remote control may feature go beyond television. first kind make sure know im laura im project manager. know think better actually still kind dunno massive junky thing table. right basically high priority animal willing take lot physical affection family. cost production cost twelve fifty selling price wholesale retail. right think main design aim remote control know satellite regular telly v_c_r_ everything. would useful though wouldnt knew money would get. wrap make sure page given sort example coffee machine something right. he dinner hell sudden get start chasing tail round living room. remote control think suppose depends complicated remote control. twenty five euro mean thats thats eighteen pound something isnt. imagine probably sale actually probably retailer sell whatever price want. right away im wondering there th th d_v_d_ player zone. anything else anybody want add dont remote control theyve used would really part new one. make sense maybe design point view cause complicated character european language need button. dunno never bought remote control dont know good remote control would get. maybe could use sort example successful piece technology palm palm pilot. theyre gone little sort scribble board camera m_p_ three player telephone everything agenda. might put oh want watch look oh thats good idea. personally home combined audio video television set d_v_d_ player c_d_ player. according brief gonna selling remote control twenty five euro aiming make fifty million euro. marketing executive youll thinking actually know requirement fulfil youll get instruction emailed guess\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mm3dSHyZfJw5"
      },
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(mead_hypothesis, transcript_sum)\n",
        "print('TextRank scores: {}'.format(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X80XLENgfJw5"
      },
      "source": [
        "scores1f = []\n",
        "scores1p = []\n",
        "scores1r = []\n",
        "scores2f = []\n",
        "scores2p = []\n",
        "scores2r = []\n",
        "scoresllf = []\n",
        "scoresllp = []\n",
        "scoresllr = []\n",
        "bleu1 = []\n",
        "bleu2 = []\n",
        "bleu3 = []\n",
        "bleu_together = []\n",
        "for i in range(len(ext_sum_trans)):\n",
        "    fin_klsum = generate_klsum_summary(ext_sum_trans[i], 30)\n",
        "    klsum_hypothesis = ''\n",
        "    for j in fin_klsum :\n",
        "        klsum_hypothesis += j \n",
        "    test_ = open(ext_summaries_files[i], \"r\", errors = 'ignore')\n",
        "    transcript_ = test_.read()\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(klsum_hypothesis, transcript_)\n",
        "    scores1f.append(scores[0]['rouge-1']['f'])\n",
        "    scores1p.append(scores[0]['rouge-1']['p'])\n",
        "    scores1r.append(scores[0]['rouge-1']['r'])\n",
        "    scores2f.append(scores[0]['rouge-2']['f'])\n",
        "    scores2p.append(scores[0]['rouge-2']['p'])\n",
        "    scores2r.append(scores[0]['rouge-2']['r'])\n",
        "    scoresllf.append(scores[0]['rouge-l']['f'])\n",
        "    scoresllp.append(scores[0]['rouge-l']['p'])\n",
        "    scoresllr.append(scores[0]['rouge-l']['r'])  \n",
        "    # bleu \n",
        "    reference = preprocess_text_simple(transcript_)\n",
        "    candidate = preprocess_text_simple(klsum_hypothesis)\n",
        "    score_together = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "    bleu_together.append(score_together)\n",
        "    bleu_1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
        "    bleu1.append(bleu_1)\n",
        "    bleu_2 = sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))\n",
        "    bleu2.append(bleu_2)\n",
        "    bleu_3 = sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))\n",
        "    bleu3.append(bleu_3) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahE81epdanV3"
      },
      "source": [
        "## Abstractive methods "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMni3VUvfJw6"
      },
      "source": [
        "Рассмотрим методы, которые напоминают нам о том, как именно человек подсознательно делает краткое саммари текста. Эти методы основаны на архитектуре трансформеров. Их преимущество заключается в том, что они не используют RNN сегменты и больше уделяют вниманию токенам с помощью механизма self-attention. Нижеприведенные алгоритмы осуществлены с помощью предобученного трансформера. До трансформеров (трансформеры появились в 2017 году) для задачи саммаризации использовались RNN, но они испытывали утрату в долговременной памяти. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKB1m1qtfJw6"
      },
      "source": [
        "Как работают трансформемы? (главная фишка состоит в том, что убираются связи rnn во времени) \n",
        "RNN принимает слова последовательно, мы подаем слово и ждем, пока rnn закончет его обрабатывать, а потом давать второе. (нарисовать схемку transformer) Все, что нужно:  \n",
        "1. ReLu \n",
        "2. Batch normalization \n",
        "3. attention   \n",
        "4. embedding layers  (первые слови, когда подоем сетки слова, переведенные в числа) \n",
        "5. residual connections  \n",
        "6. positional encoding. \n",
        "7. softmax layer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bNBSt-TanV3"
      },
      "source": [
        "### Bidirectional Encoder Representations from Transformers (BERT) - 2018 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clJ1zcUufJw6"
      },
      "source": [
        "Основные особенности: \n",
        "1. В отличие от GPT, BERT смотрит на текст не только слева на право, но и справа налево.  \n",
        "2. Немного меняет обработку входных данных, использует в качестве tokenizer wordpiece  \n",
        "3. Использует при обучение Masked language model (маскирует часть входных данных, затем учит модель предсказывать недостающие токены) + Next Sentence Prediction + finetuning "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlkJk59TanV4"
      },
      "source": [
        "# !pip install bert-extractive-summarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG8m27vuanV4"
      },
      "source": [
        "# !pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIPboZ8fanV4"
      },
      "source": [
        "from summarizer import TransformerSummarizer\n",
        "from summarizer import Summarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPQm_aOJf2IX"
      },
      "source": [
        "test = open('/content/ES2002a.transcript.txt', \"r\", errors = 'ignore')\n",
        "body = test.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd1EiMnngrJL"
      },
      "source": [
        "test_sum = open('/content/ES2002a.abssumm.txt', \"r\", errors = 'ignore')\n",
        "transcript_sum = test_sum.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299,
          "referenced_widgets": [
            "7ababbb917e8427991b9af81dffa1680",
            "bdb37c7571614b5bb90d90e2bd9947e0",
            "2b2cd25d7bf34c898b4fd3888e52bc67",
            "b459594541d64a2e9c5b2f4754b2ee5a",
            "528a2d12f68a49ae87821a550dac15be",
            "d5f84b72ed9146578f58b82b2b9e2074",
            "8ef9e282a0b64c08a7606a8547530648",
            "8cd4fc093a7e41fabc37ade69146ac99",
            "2c02ef3d1c2f44a3b490362ae8c391ee",
            "3bddfeecbe1e447199e05a0228812f0e",
            "c5ccf3da42d44959b9d8e6327d1e76e5",
            "819b5786799e4498b02fd818bbcddf02",
            "0e3d8659993a4f1ca0cb2e617e8d8ee4",
            "6e96e60e741447ac8b80cd970e2bbf49",
            "8cc9a57634164569a9f057489db8d598",
            "4cb1889baf36418d977ab79d30e8d7e1",
            "683f37d80a1c40a49b52e204a35c7bb4",
            "7ba9e2f760024470a1b756d5119ccfa9",
            "7f9fdf30c287421fb900b6be8956da70",
            "18b2e64f536b4c4d9cf05bfc47e50fd2",
            "f0f30aa0ff12487aa69018892e631847",
            "2375bb2c124143dc8fd2c98d95675bb0",
            "b212133eb4e442b6b78ad0c449ba880a",
            "c6310450d4f54b75a91c8eddfe0f9401",
            "a1cc9ed33536458691eea4033079ddf6",
            "abad6dbbd46141c39b2343990274fe41",
            "660c11570197475bbedcc6526b540a22",
            "e6fa4e39e3b941fca09b4484af78adc3",
            "130c71fa4d5845139dc7a357a6479f4f",
            "ff0cea90dc074ef5a6a3c9f387e4d47c",
            "17ec69dbcb8a409a85641e28517b28fa",
            "cb143baeecb24303ab29162eb67b511d",
            "71bbc4ce1b674615b57103107ffd6f2a",
            "915641fb8e70495baf3bd55f1db3d16c",
            "6304db960a9848d3b8eb9fb3f805e78c",
            "c93bd3e43a2549afb42028de34d83337",
            "f234129661a149d2806bb46365834b6c",
            "1a5cee1b46a54cacb389c2197fa6cd35",
            "0bde55504e09459587c6ebe1b94d5156",
            "8a222052ed73406a8758540677f19f86"
          ]
        },
        "id": "ZvyJmvHNanV4",
        "outputId": "6dfc4b4c-2b88-4b3d-9c14-c68e915a8894"
      },
      "source": [
        "bert_model = Summarizer()\n",
        "bert_summary = ''.join(bert_model(body, min_length=60))\n",
        "print(bert_summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ababbb917e8427991b9af81dffa1680",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=571.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c02ef3d1c2f44a3b490362ae8c391ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1344997306.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "683f37d80a1c40a49b52e204a35c7bb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1cc9ed33536458691eea4033079ddf6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71bbc4ce1b674615b57103107ffd6f2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Then they're small cute and furry, and uh when planet of the apes becomes real, I'm gonna be up there with them. Uh, right, well basically um high priority for any animal for me is that they be willing to take a lot of physical affection from their family. Probably when he was little he got lots of attention for doing it and has forever been conditioned. ' Like so sort of like how do you I I mean one one way of looking at it would be, well the people producing television sets, maybe they have to buy remote controls. Or another way is maybe people who have T_V_ sets are really fed up with their remote control and they really want a better one or something. Okay so Right, so in function one of the priorities might be to combine as many uses I think so. Well like um, maybe what we could use is a sort of like a example of a successful other piece technology is palm palm pilots. So, like, I wonder if we might add something new to the to the remote control market, such as the lighting in your house, or um Yeah, yeah. So it's sort of ironic that that then they're in there um you know, the sound and everything it's just one system. Maybe we could think about how, could be more, you know, streamlined. Or are we keeping sort of like a a design commitment to television features? Um, I dunno, I've never bought a remote control, so I don't know how how good a remote control that would get you. Um and uh marketing executive, you'll be just thinking about what it actually what, you know, what requirements it has to has to fulfil and you'll all get instructions emailed to you, I guess. Um I guess that's up to us, I mean you probably want some kind of unique selling point of it, so um, you know Yeah. Um, well anyway, I don't know, it's just the first animal I can think off the top of my head.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "258cUJRtgfua",
        "outputId": "073d2b3f-5e6e-46c8-fa95-072e3ff016cc"
      },
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(bert_summary, transcript_sum)\n",
        "print('BERT scores: {}'.format(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT scores: [{'rouge-1': {'f': 0.12195121704830464, 'p': 0.07122507122507123, 'r': 0.423728813559322}, 'rouge-2': {'f': 0.009803919129662275, 'p': 0.005714285714285714, 'r': 0.034482758620689655}, 'rouge-l': {'f': 0.10572686963845608, 'p': 0.0625, 'r': 0.34285714285714286}}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evh5HNdLh3gD",
        "outputId": "c10cb10f-337d-4818-e695-c531642e9e02"
      },
      "source": [
        "score_together = sentence_bleu(transcript_sum, bert_summary, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "bleu_1 = sentence_bleu(transcript_sum, bert_summary, weights=(1, 0, 0, 0))\n",
        "bleu_2 = sentence_bleu(transcript_sum, bert_summary, weights=(0, 1, 0, 0))\n",
        "bleu_3 = sentence_bleu(transcript_sum, bert_summary, weights=(0, 0, 1, 0))\n",
        "print('BLEU - 1: {}, BLEU-2: {}, BLEU-3: {}, BLEU: {}'.format(bleu_1, bleu_2, bleu_3, score_together))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU - 1: 0.01495016611295681, BLEU-2: 1.0, BLEU-3: 1.0, BLEU: 0.3496725209261994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRy305iXanV5"
      },
      "source": [
        "### GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cVoKJD0fJw8"
      },
      "source": [
        "decoder + multi-headed masked self-attention\n",
        "(в отличие от RNN предсказывание токенов идет параллельно) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCCFfEMufJw8"
      },
      "source": [
        "Добавить описание!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "570b7eb2ff844d50889c8ebf986e29c5",
            "9bc031fb5e104ba6910ac0778ab4f559",
            "36b8b2dc6d534848bec66aab34fcfe97",
            "9a8b3857c8a14c64a44d0f306d3ff76e",
            "5b09d55a8d5f4831b84b6744d5ce70be",
            "1f0de94287cf4de183d4e7eb368f9f90",
            "edfc6581d197425ab5ab4c7028a72514",
            "2b78a7189e514ed791ee79ae8a3c403d",
            "45669e37fcc94111af15efbd5ef3cd78",
            "dbdf8970481a43c4ac32934502f821b1",
            "43d11d5661644332b5c618fca9584e95",
            "0ba8687590f249b9b7e355e3ed2ff534",
            "8309409102d44bf8b75d2ae600ddd27d",
            "2d951bc69b34436ea24f4da805b62082",
            "9b1d856e3f0a4572a09e3077436f130c",
            "6f05655b617d4aa385640aafdcb309ea",
            "6a314ffae4334c6d81beebbeaca9c1d4",
            "79d7397c69914203adae864bce8f0a24",
            "391b1a112a8f414985099727332d27de",
            "5e77d3ed98a04a1d91c12d7e3949d2c4",
            "46a76cd135974dee834531a7e13a3ca2",
            "f5f3228a279b45c68b1f2383e2103c9f",
            "b054df56acec413cbc1687979f47a29a",
            "5b0ad052d02d4257befdd74d0cd36fbc",
            "2eee9f698271469f83f18602b70fc17c",
            "808cf20c6a0f4430a666d1093a76376d",
            "adb2e0b0e81243a39a79bda182935bca",
            "4c64746478984036b10a2d825ce03220",
            "f4317ec5dcf64865b4227d5bab52f9fb",
            "4655a13f3d194da4aaa5dfedc387fa9f",
            "b6064d0d2d8a4465b2e04f1d1c865dd4",
            "7c3cbfac64c54a19aca7861bf4f95653",
            "98ac83e820c044118e0be4ae56abd88a",
            "ea5332e277934b8cbfffdba493a2ca2a",
            "83927b5d0add4de183e2eb38cb9f0d51",
            "76bfe9baa49a47b1b6e04a5bfa97c5f5",
            "b9310ff12804418d9b6e013ecf10b951",
            "b6efb7444ef34e62b68dfed308d1a1f8",
            "4737848bf4124c82b76405ef7a7bfda5",
            "bb8813b9cf4a4056b90bb8871e6c41c2"
          ]
        },
        "id": "iBrVQ0bPanV5",
        "outputId": "0ade4a3e-a90e-4c5a-b733-5307d66c7ab7"
      },
      "source": [
        "GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-medium\")\n",
        "full = ''.join(GPT2_model(body, min_length=60))\n",
        "print(full)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "570b7eb2ff844d50889c8ebf986e29c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=718.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45669e37fcc94111af15efbd5ef3cd78",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1520013706.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'h.12.attn.masked_bias', 'h.13.attn.masked_bias', 'h.14.attn.masked_bias', 'h.15.attn.masked_bias', 'h.16.attn.masked_bias', 'h.17.attn.masked_bias', 'h.18.attn.masked_bias', 'h.19.attn.masked_bias', 'h.20.attn.masked_bias', 'h.21.attn.masked_bias', 'h.22.attn.masked_bias', 'h.23.attn.masked_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a314ffae4334c6d81beebbeaca9c1d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2eee9f698271469f83f18602b70fc17c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98ac83e820c044118e0be4ae56abd88a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Then they're small cute and furry, and uh when planet of the apes becomes real, I'm gonna be up there with them. So cost like production cost is twelve fifty, but selling price is is that wholesale or retail? Um f frequencies or something um as well as uh characters, um different uh keypad styles and s symbols. Like, p personally for me, at home I've I've combined the um the audio video of my television set and my D_V_D_ player and my C_D_ player. And um, you know, when I think about what they are now, it's better, but actually it's still kind of, I dunno, like a massive junky thing on the table. Well, um are we at ma right now on the assumption that our television remote control may have features which go beyond the television? So uh you get to draw your favourite animal and sum up your favourite characteristics of it. Um and what do I like about him, um That's just to suggest that his tail wags. He only does it after he's had his dinner and um he'll just all of a sudden just get up and start chasing his tail 'round the living room. And uh we don't want it to cost any more than uh twelve fifty Euros, so fifty percent of the selling price. Well for a remote control, do you think that will be I suppose it's depends on how complicated our remote control is. There I mean is that something we'd want to include, do you think? Yeah, so it's th the functional design stage is next, I guess. So I got that little message a lot sooner than I thought I would, so Mm-hmm. Um I guess that's up to us, I mean you probably want some kind of unique selling point of it, so um, you know Yeah.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9VBguXvitmm",
        "outputId": "df9141b6-77ec-4e9c-c893-3cc66f2860ac"
      },
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(full, transcript_sum)\n",
        "print('GPT-2 scores: {}'.format(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPT-2 scores: [{'rouge-1': {'f': 0.10781670891551215, 'p': 0.0641025641025641, 'r': 0.3389830508474576}, 'rouge-2': {'f': 0.010840105751574153, 'p': 0.006430868167202572, 'r': 0.034482758620689655}, 'rouge-l': {'f': 0.0921658959119965, 'p': 0.054945054945054944, 'r': 0.2857142857142857}}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmlJo492ivqM",
        "outputId": "f63b82ff-7462-4a18-f852-6879ad573183"
      },
      "source": [
        "score_together = sentence_bleu(transcript_sum, full, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "bleu_1 = sentence_bleu(transcript_sum, full, weights=(1, 0, 0, 0))\n",
        "bleu_2 = sentence_bleu(transcript_sum, full, weights=(0, 1, 0, 0))\n",
        "bleu_3 = sentence_bleu(transcript_sum, full, weights=(0, 0, 1, 0))\n",
        "print('BLEU - 1: {}, BLEU-2: {}, BLEU-3: {}, BLEU: {}'.format(bleu_1, bleu_2, bleu_3, score_together))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU - 1: 0.016896120150187734, BLEU-2: 1.0, BLEU-3: 1.0, BLEU: 0.3605344319814106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhmoBjAoanV5"
      },
      "source": [
        "### XLNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xtOg3C0fJw8"
      },
      "source": [
        "Добавить описание!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250,
          "referenced_widgets": [
            "d45559ceb3e74e49b9442c1eb01797ba",
            "52363879d9794b8e9ba9b95a6b3468b4",
            "ad8f2b32b7fa4a1aa8c9bce4dea6c134",
            "340cbc5e876e440c8a528162c5cbc5cf",
            "9eabba7b047748268582f5651311e34f",
            "b9c7ec4003cd4956be6658e6c56fb0b2",
            "28a3520817c54b92a733ec991a5042ca",
            "a8a38317f5124b38a7c33fef115a6aac",
            "70b0a3ba98c54b8ab10d3aab0a9f16e2",
            "dbdf02a8aab04bf990bebc19b5cfa6f1",
            "9dc676a99e024d6ca3a6f77992c412b9",
            "27df0bcbd6014447b02de5ea2b04786a",
            "e8d4307ab0744acaaca28e830e6fd161",
            "94e7bddd43fa4257aa19e43a15039fa6",
            "2d029b45f5b24f75b1555687fe5cdb50",
            "2eb5696871d04ae28b3332d08dc9b2fa",
            "4ca4aa332f6d400f8d080337aad1315d",
            "e32f7a7aa0f84f40b169c7997ee04938",
            "7320043017a346728a426312d4d7e072",
            "ee0bcbd64e40479e86d6ef9e1bfd6d7b",
            "fd52456be2fc4c8c89745a21ea610162",
            "b7825592757f4962aecb98759405bf3e",
            "700ec00122f14dc09b27894191b07415",
            "f7072b3cd7a247e381c605edbe93b3c5",
            "97eeb9791d7a407a9cd19f7a79ad5047",
            "c2bce7d1bc7b41beb7de980c6d74e173",
            "a074ffc49417443db38c320b76adc347",
            "de6e46b42ac34111a71739559eb80816",
            "bdb3fdf6b8a946bda53beb2888dda625",
            "72b294e6db804e39b7209be50d8481c2",
            "2f0caf30d46341c6b2fda47901ed9966",
            "3c11856d85524e7dbb015ab8ca089873"
          ]
        },
        "id": "1V_3iIQoanV6",
        "outputId": "32b692f3-07f1-4497-e700-df059707663c"
      },
      "source": [
        "model = TransformerSummarizer(transformer_type=\"XLNet\",transformer_model_key=\"xlnet-base-cased\")\n",
        "full = ''.join(model(body, min_length=60))\n",
        "print(full)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d45559ceb3e74e49b9442c1eb01797ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70b0a3ba98c54b8ab10d3aab0a9f16e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=467042463.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ca4aa332f6d400f8d080337aad1315d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97eeb9791d7a407a9cd19f7a79ad5047",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1382015.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Then they're small cute and furry, and uh when planet of the apes becomes real, I'm gonna be up there with them. I know um My parents went out and bought um remote controls because um they got fed up of having four or five different remote controls for each things the house. And then a and then al the other thing international is on top of the price. Um so I'm wondering right away, is selling twenty five Euros, is that sort of the thi is this gonna to be like the premium product kinda thing or Uh-huh. Maybe we could think about how, could be more, you know, streamlined. 'Cause it could b it could it could be that f it could be that functionally that doesn't make it any better, but that just the appeal of of not having You know, these days there's a r pe things in people's homes are becoming more and more like chic, you know. Or are we keeping sort of like a a design commitment to television features? Um so first of all, just to kind of make sure that we all know each other, I'm Laura and I'm the project manager. Um so we're designing a new remote control and um Oh I have to record who's here actually. Um, as you can see it's supposed to be original, trendy and user friendly. Well twenty five Euro, I mean that's um that's about like eighteen pounds or something, isn't it? Um so anything else anybody wants to add about what they don't like about remote controls they've used, what they would really like to be part of this new one at all? Th Okay, well just very quickly 'cause this we're supposed to finish now. Um I guess that's up to us, I mean you probably want some kind of unique selling point of it, so um, you know Yeah. I mean it's usually quite small, or when you want it right, it slipped behind the couch or it's kicked under the table.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5JndIB-jAN-",
        "outputId": "474ab223-ba64-41d6-aca0-d0bb2fce1d25"
      },
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(full, transcript_sum)\n",
        "print('XLNet scores: {}'.format(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XLNet scores: [{'rouge-1': {'f': 0.11442785819200026, 'p': 0.06705539358600583, 'r': 0.3898305084745763}, 'rouge-2': {'f': 0.019999997520500308, 'p': 0.011695906432748537, 'r': 0.06896551724137931}, 'rouge-l': {'f': 0.09523809266692909, 'p': 0.05612244897959184, 'r': 0.3142857142857143}}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOB4lRdJjCC-",
        "outputId": "19b20473-234c-4854-fcb7-15367178298e"
      },
      "source": [
        "score_together = sentence_bleu(transcript_sum, full, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "bleu_1 = sentence_bleu(transcript_sum, full, weights=(1, 0, 0, 0))\n",
        "bleu_2 = sentence_bleu(transcript_sum, full, weights=(0, 1, 0, 0))\n",
        "bleu_3 = sentence_bleu(transcript_sum, full, weights=(0, 0, 1, 0))\n",
        "print('BLEU - 1: {}, BLEU-2: {}, BLEU-3: {}, BLEU: {}'.format(bleu_1, bleu_2, bleu_3, score_together))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU - 1: 0.014705882352941173, BLEU-2: 1.0, BLEU-3: 1.0, BLEU: 0.34823528327578535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkRXLfNLanV8"
      },
      "source": [
        "# Speech Recognition "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aHyK-xDfJw-"
      },
      "source": [
        "Современные модели распознование речи основываются на срытых марковских цепях. Предполагается, что звуковой сигнал может быть представлен стационарным процессом. В стандартной модели звуковой сигнал разбивается по временной шкале на 10 милисекунд. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9SmWfFafJw-"
      },
      "source": [
        "- apiai\n",
        "- assemblyai\n",
        "- google-cloud-speech\n",
        "- pocketsphinx\n",
        "- SpeechRecognition\n",
        "- watson-developer-cloud\n",
        "- wit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuEm259NtkQv"
      },
      "source": [
        "test_sr = open('/content/EN2001a.transcript.txt', \"r\", errors = 'ignore')\n",
        "transcript_sr= test_sr.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-Vd9na4cyQX"
      },
      "source": [
        "## speech_recognition модуль "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9WpWMEEbp6Z"
      },
      "source": [
        "# !pip3 install --upgrade speechrecognition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8yppp1Tc6-D"
      },
      "source": [
        "# !pip install ibm_watson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXd8oVeNanV8",
        "outputId": "767de2ea-2218-46e3-8033-fa71ce9cd25b"
      },
      "source": [
        "import speech_recognition as sr\n",
        "file = sr.AudioFile('/content/EN2001a.Array1-01.wav')\n",
        "r = sr.Recognizer()\n",
        "with file as source:\n",
        "    r.adjust_for_ambient_noise(source) # убираем шумы \n",
        "    audio = r.record(source, duration = 130)\n",
        "print(r.recognize_google(audio))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zoom\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geKLQ8KYuCDr"
      },
      "source": [
        "Недостаток speech_recognition является то, что он плохо справляется с шумами. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2MapiZyfJxD"
      },
      "source": [
        "## IBM Watson "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPDiX3dfxBCC",
        "outputId": "3b97a552-62c6-44a6-e20b-18f537f1d0d6"
      },
      "source": [
        "!pip install ibm_watson"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ibm_watson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/88/395d7d52df29f321ae1150cf9b5a71cef8611570230502597c427bc1e9d9/ibm-watson-5.1.0.tar.gz (382kB)\n",
            "\u001b[K     |████████████████████████████████| 389kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from ibm_watson) (2.23.0)\n",
            "Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from ibm_watson) (2.8.1)\n",
            "Collecting websocket-client==0.48.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/a1/72ef9aa26cfe1a75cee09fc1957e4723add9de098c15719416a1ee89386b/websocket_client-0.48.0-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 10.9MB/s \n",
            "\u001b[?25hCollecting ibm_cloud_sdk_core>=3.3.6\n",
            "  Downloading https://files.pythonhosted.org/packages/dd/24/ba1f1ac7e6dad5efe88362fe9fcf548f7104daf3ed4ec2333b2ae35a7a21/ibm-cloud-sdk-core-3.9.0.tar.gz\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2020.12.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python_dateutil>=2.5.3->ibm_watson) (1.15.0)\n",
            "Collecting PyJWT<3.0.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/3f/32/d5d3cab27fee7f6b22d7cd7507547ae45d52e26030fa77d1f83d0526c6e5/PyJWT-2.1.0-py3-none-any.whl\n",
            "Building wheels for collected packages: ibm-watson, ibm-cloud-sdk-core\n",
            "  Building wheel for ibm-watson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-watson: filename=ibm_watson-5.1.0-cp37-none-any.whl size=375439 sha256=c843b3a0e0feb275def01e8ddf5040b772ce7a6e36be52c370c4e42858c510d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/6d/cf/1d91261b96363da78bf9b02699fd2262e6b5dad179500690c1\n",
            "  Building wheel for ibm-cloud-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cloud-sdk-core: filename=ibm_cloud_sdk_core-3.9.0-cp37-none-any.whl size=59612 sha256=209dea69415b071207c9aefbaa571d4cbdf6f9fd1e89cc700522aa727f2f7976\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/23/f2/ae9db79b4234ed0fc74bf00bc97bcd16440ab2764c5f443167\n",
            "Successfully built ibm-watson ibm-cloud-sdk-core\n",
            "Installing collected packages: websocket-client, PyJWT, ibm-cloud-sdk-core, ibm-watson\n",
            "Successfully installed PyJWT-2.1.0 ibm-cloud-sdk-core-3.9.0 ibm-watson-5.1.0 websocket-client-0.48.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Wp1vElfJxD"
      },
      "source": [
        "from ibm_watson import SpeechToTextV1\n",
        "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0g4H-F9fJxE"
      },
      "source": [
        "apikey = 'wx9rd4_OMeYZpzOHBcfhDNXW5X6ECrfaU0u54fhwiY0M'\n",
        "url = 'https://api.eu-gb.speech-to-text.watson.cloud.ibm.com/instances/5019271f-5147-4778-835d-8c1fac38fee6'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4nsdVvAfJxE"
      },
      "source": [
        "authenticator = IAMAuthenticator(apikey)\n",
        "stt = SpeechToTextV1(authenticator = authenticator)\n",
        "stt.set_service_url(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAl_SF6NfJxE",
        "outputId": "cec4900d-b792-4f8c-8b2a-c7bb397f5333"
      },
      "source": [
        "import subprocess \n",
        "import os\n",
        "# производим chunking (деление аудиозаписи на несколько по 360 секунд)\n",
        "command = 'ffmpeg -i EN2001a.Array1-01.wav -vn -ar 44100 -ac 2 -b:a 192k audio.mp3'\n",
        "subprocess.call(command, shell=True)\n",
        "command = 'ffmpeg -i audio.mp3 -f segment -segment_time 360 -c copy %03d.mp3'\n",
        "subprocess.call(command, shell=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNK6eTy3fJxE"
      },
      "source": [
        "files = []\n",
        "for filename in os.listdir('.'):\n",
        "    if filename.endswith(\".mp3\") and filename !='audio.mp3':\n",
        "        files.append(filename)\n",
        "files.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "29TBKGDZfJxF"
      },
      "source": [
        "results = []\n",
        "for filename in files:\n",
        "    with open(filename, 'rb') as f:\n",
        "        res = stt.recognize(audio=f, content_type='audio/mp3', model='en-AU_NarrowbandModel', continuous=True, \\\n",
        "                           inactivity_timeout=360).get_result()\n",
        "        results.append(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PDNWZxsOfJxF"
      },
      "source": [
        "text = []\n",
        "for file in results:\n",
        "    for result in file['results']:\n",
        "        text.append(result['alternatives'][0]['transcript'].rstrip() + '.\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9CPcHXbvFwO"
      },
      "source": [
        "final_s = ' '.join(text)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "jtJ5oRq68_pc",
        "outputId": "3fe5052c-32fd-44b0-8491-9ad8d5da789e"
      },
      "source": [
        "final_sr = final_s.replace(\"\\n\", \"\")\n",
        "final_sr"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"the regular passengers you always get the most. some. Asians back do you have to. me why is wrong. tough situation where somebody has to make a billion population on. modifying that this accident. other vehicles. orders refunded differently the actual browser. and. probably. transaction was the exact same place is with it is a business statement the. Maurice weather is meeting in the reading serious for a second. place the left. of course. everything. St. one six hundred. a different. one version twenty offline. what really displayed. currently find reasons. G.. just confirm your. we're currently. all you do is you say if you're looking to see. I'm not losing everything but maybe it was. although some. just. or if either some response. your audio to send copy bother way of getting an extra just. remove. but when you talk about. in the instructor. isn't displaying aftermarket business. store the importance of previously working just. the company then gets just. me just storeys on a. free items. if you were going on with it. they need to be calculated at one level %HESITATION because of. anything that has the. your order. I think. I don't think it was very hard either. just build. so just like the sound of the organ media way. please pray for that position for that information. what's in common. but you just need it linked list. forms it'll just struggle to get. the second signs in between. what happens with. we also really want to be able to search please. a different phone. searchable. that's. you're standing on hold and. basically sped up the big thing into a different. yep. if it's warm I'm using increased. there's a serious. second with you immediately if they were. if you are. thank you for the new plans come. but generally I reflected on both cars. the problem because it's too big for you can do just. just process. a hundred. just look at the meeting. they. we should build. stormy letter for the second meetings as well. nothing giving. she doesn't. whether these depositions employment conditions they were. yep yep yep I thought what if if something yep even that previously the weekly pop up. manages the manager looked everything that's the only lotus giving them your user needs most of Texas they're both vision plans at any time for example. the current level students are office needs and starting position is where they're. if. research centre for all that we don't have a specific. bye. which basically make. presupposition emails I misrepresent yes my use of the store. just. from. and %HESITATION weeks. type two. send R. on a word limit them somehow. for some weapons and will get the data. although apostles but sometimes. indicated by. the other friends is reference to the topic please get. the topic stolen. number of whatever. the other end of the. do every every. one of the few weeks for them. with one place to the other person of the week for the. the other end so why do we need to have to. three now. at less having two different. having the same month. the media centre our office is definitely use that. we looted front part of the deal. the business. we do have to acquiring received give us everything that's about business week. they can still get. what is the best possible. probably the store. cool information density as well so like. motion. dollars will be recalculating the same thing. packages. message your maps of the fact that there's a problem. some. just most payment. should be wasted. I mean that you just. response of that but if you want a more general conclusion is the left. which is right one of. related to the corporate to listen. useful to know. the segment. what is working on charges of recognition then test everything. want to see how much. something really simple like just displaying a whole meeting. just being able to scroll through. please. well. I wouldn't have. yep. tension of business. yes. just doing addition. what is just a news. doesn't use questions. I think it's just. sure insurance. change with. just frequencies membership. interested in considering only the fish give me a reason. any special characters. be working there but one of these. has a list of stock works. interesting boring. we could fill. one of the papers I read. several company additionally. yes. the useful. how biggest without. contemptuous. directly off. he would be. each. she. total for the year agreement. I was presented. I would like to look at the frequency of roads and may. text. you could just. you could use just the same. just used to. probably just completed on my. different. or something from one chance. tables that Cheryl. from text them the wrong. I guess the from US. insure that prefix notified. a document in the sense of car computer. change. segments with. you'll need how many. things. what we consider a documents contend on complex's. considering treating document. sorted. you. change or were. contacted all the more impressive. yeah there's not it's not the issue. the offices. when you requested as long as when you're working with offices. if you need something that's. if you were looking at send this one. as long as your average respect. all right so that would be the serious as a whole definitely sort of meetings meeting that may help. damaged. to get a few. as long as you build up with respect. rather than posted its own structure. is this structurally than almost identity. to me that be completed within agreed market I mean. the stock value options instead when you go. confusion over you have things on this. for such a safe more than. Wednesday Wednesday we only give you. just. two things together. reasonable amount with this everything. we. the big directory session transfer. because. to get to. for choosing favours service from. there was. do you. yeah so far extracted. duration but it's from the roads phones or exercise. musical. the highest level of motion. might be funny to see what. but it is of the phone your phone previous identification letters. something has a. the highest level two six hundred once you written almost. one thing is exposure. times. transaction. true on which terms. I just want to find a specific person must plans with everything looking into. changes in Ashland. just streaming provision. let me just. I would need something that marks the end of your. thank you said you. your submission. your text reputed. currently. no. give give me different five dollars they give me convenience. the specialist. I wanted to order. contacted my mother. previously continue button. although. because. what I got from. a driver was going strong. probably. terms of. just one fails first nineteen. you can only look. it just saves you policy. using. building a structure again. check back. merry. just. recent refuse. seems like this. I would have thought. I think. things influence. already. imagine. anywhere I think I think they might be. provisionally review. this is the software. specification of most. once you stop.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ17sB8VuRI7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96fa2285-56c0-4632-993a-91623ffe26bf"
      },
      "source": [
        "print('WER: {}'.format(wer(final_sr, transcript_sr)))\n",
        "print('CER: {}'.format(cer(final_sr, transcript_sr)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WER: 1308.43776106934\n",
            "CER: 1080.2458448753462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IeTs_K6fJxF"
      },
      "source": [
        "## Метрики для распознавания речи "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJM7dNgifJxF"
      },
      "source": [
        "1. WER = Word error rate = $\\frac{S + D + I}{N}$, где I - количество вставок слов, D - количество удалений слов, S - количество замен слов, N - всего слов \n",
        "2. CER = character error rate =  $\\frac{S + D + I}{N}$, аналогичная ситуация, только с символами "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37-0O-vhfJxF"
      },
      "source": [
        "#!pip install Levenshtein"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd0F5FjzfJxG"
      },
      "source": [
        "import Levenshtein as Lev\n",
        "def wer(s1, s2): \n",
        "    b = set(s1.split() + s2.split()) \n",
        "    word2char = dict(zip(b, range(len(b))))\n",
        "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
        "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
        "    wer_lev = Lev.distance(''.join(w1), ''.join(w2)) \n",
        "    wer_inst = float(wer_lev)/len(s1.split()) * 100\n",
        "    return wer_inst\n",
        "def cer(s1, s2):\n",
        "    s1, s2, = s1.replace(' ', ''), s2.replace(' ', '')\n",
        "    cer_inst = float(Lev.distance(s1, s2)) / len(s1) * 100 \n",
        "    return cer_inst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ5sbUx3anWA"
      },
      "source": [
        "## Список литературы\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulwqsm01anWA"
      },
      "source": [
        "[1]  H.P.LUHN \"The Automatic Creation of Literature Abstracts\"      https://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf  \n",
        "[2]  Dipanjan Das, Andre F.T. Martins \"A Survey on Automatic Text Summarization\"    \n",
        "https://www.cs.cmu.edu/~nasmith/LS2/das-martins.07.pdf  \n",
        "[3]  Edmundson H. P. (1969), \"New methods in automatic extracting, Journal of the\n",
        "ACM\", vol. 16 Issue 2, pp. 264–285.  \n",
        "[4]  DeJong G. F. (1982), An Overview of the FRUMP System, in Strategies for Natural Language Processing, Lawrence Erlbaum, Hillsdale, New Jersey, pp. 149–176.  \n",
        "[5]  \"Variations of the Similarity Function of TextRank for Automated Summarization\"\n",
        "Federico Barrios, Federico L´opez, Luis Argerich, Rosita Wachenchauzer. Facultad de Ingenier´ıa, Universidad de Buenos Aires, Ciudad Aut´onoma de Buenos Aires, Argentina. Universidad Nacional de Tres de Febrero, Caseros, Argentina.  \n",
        "[6] K.NandhiniS.R.Balasundaram \"Improving readability through extractive summarization for learners with reading difficulties\"\n",
        "[7] Dineshnath .G \"Abstractive Text Summarization of Research Articles Based on Word Associations\"  \n",
        "[8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin \"Attention is all you need\"  \n",
        "[9] Rahul Katragadda, Prasad Pingali, Vasudeva Varma \"Sentence Position revisited: A robust light-weight Update Summarization ‘baseline’ Algorithm\"  \n",
        "[10] Josef Steinberger, Karel Ježek \"Using Latent Semantic Analysis in Text Summarization and Summary Evaluation\"      \n",
        "[11] Sandeep Sripadaa, Jagadeesh Jagarlamudi \"Summarization Approaches Based on Document Probability Distributions\"  \n",
        "[12] Новосёлова Анастасия Максимовна \"Исследование методов автоматического реферирования текстов\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbRggo92fJxG"
      },
      "source": [
        "## Идеи для чего-то нового"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKJpsJVafJxH"
      },
      "source": [
        "? сделать логистическую регрессию с большим числом признаков, например, со сколькими предложениями имеется связь (по similarity matrix), вес ребра из LexRank algo, в какой кластер предложение определилось по алгоритму k-means, score предложения с помощью меры td-idf  \n",
        "? может, брать в каждой модели среднее от всех резульаттов embeddings  \n",
        "? попробовать привязать LDA (применять LDA, выделять основные топики, выводить слова, которые определяют эти темы и на основе этих слов строить предложения)\n",
        "? Есть такой алгоритм, как  ***Skip-thought vector***  \n",
        "Будем использовать фреймворк кодера/декодера для генерации векторов признаков.  \n",
        "Кодер: Кодировщик обычно представляет собой GRU-RNN, который генерирует векторное представление фиксированной длины h(i) для каждого предложения S(i) на входе. Кодированное представление h(i) получается путем передачи конечного скрытого состояния ячейки GRU.  \n",
        "Декодер: Сеть декодера принимает это векторное представление h(i) в качестве входных данных и пытается сгенерировать два предложения — S(i-1) и S(i+1), которые могут возникнуть до и после входного предложения соответственно. Отдельные декодеры реализуются для генерации предыдущего и следующего предложений, оба являются GRU-RNN. Векторное представление h(i) действует как начальное скрытое состояние для GRU сетей декодера.  \n",
        "Подобно тому, как вложения Word2Vec обучаются предсказанию окружающих слов, skip-thought vector обучается предсказанию окружающих предложений. По мере обучения этой модели изученное представление (скрытый слой) теперь будет размещать похожие предложения ближе друг к другу, что обеспечивает более семантически связную кластеризацию.\n",
        "Можно будем применить extractive method, а затем для обеспечения баланса в предложений и отсуствия разрывных предложений, можно использовать skip-thought vector.\n",
        "? Использовать вместо Word2Vec embedding Милокова"
      ]
    }
  ]
}